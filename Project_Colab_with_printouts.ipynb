{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGKCXbWIVjT6"
   },
   "source": [
    "# Simulating Complex Physics with Graph Networks: step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1E_PEIxaHr8"
   },
   "source": [
    "## Overview\n",
    "\n",
    "• By Peng Chen, Shiyu Li, Haochen Shi as part of Stanford CS224W course project. \n",
    "\n",
    "• This tutorial provides a step-by-step guide for how to build a Graph Network to simulate complex physics.\n",
    "\n",
    "**Before we get started:**\n",
    "- This Colab includes a concise PyG implementation of paper ***Learning to Simulate Complex Physics with Graph Networks*.\n",
    "- We adapted our code from open-source tensorflow implementation by DeepMind.\n",
    "    - Link to pdf of this paper: https://arxiv.org/abs/2002.09405\n",
    "    - Link to Deepmind's implementation: https://github.com/deepmind/deepmind-research/tree/master/learning_to_simulate\n",
    "    - Link to video site by DeepMind: https://sites.google.com/view/learning-to-simulate\n",
    "- Run **sequentially run all cells in each section**, so intermediate variables / packages will carry over to next cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wgi77SUya5RT"
   },
   "source": [
    "## Device\n",
    "\n",
    "We recommend using a GPU for this Colab. Click `Runtime` then `Change runtime type`. Then set `hardware accelerator` to **GPU**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPBthm7fHqNC"
   },
   "source": [
    "## Setup\n",
    "\n",
    "installation of PyG on Colab can be a little bit tricky. Before we get started, let's check which version of PyTorch you are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGMo0gTp-GLe",
    "outputId": "1a45454d-a1a0-4ca7-8471-0a0919907b7f"
   },
   "outputs": [],
   "source": [
    "# Dataset Source #1:\n",
    "# https://drive.google.com/file/d/1ZmiKpsQVLFxPOIff-LfFkZwe5ZYG1FEb/view?usp=drive_link\n",
    "\n",
    "# Dataset Source #2:\n",
    "# https://drive.google.com/drive/mobile/folders/11uuYl0peqPg2DQno64YPYMODPu8fjDXU?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGMo0gTp-GLe",
    "outputId": "1a45454d-a1a0-4ca7-8471-0a0919907b7f"
   },
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGMo0gTp-GLe",
    "outputId": "1a45454d-a1a0-4ca7-8471-0a0919907b7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch has version 1.12.1+cu102 with cuda 10.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "print(f\"PyTorch has version {torch.__version__} with cuda {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgnNePtAcNJu"
   },
   "source": [
    "• Download necessary packages for PyG. \n",
    "\n",
    "• ensure your version of torch matches output from cell above. \n",
    "\n",
    "• In case of any issues, more information may be found on [PyG's installation page](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fS00k2JAKAzR",
    "outputId": "6d88f1c4-b8b5-4f1f-80c9-f0deb7e8ce87"
   },
   "source": [
    "!pip3 install torch==1.12.1+cu102 torchvision==0.13.1+cu102 torchaudio==0.12.1 torchtext --extra-index-url https://download.pytorch.org/whl/cu102\n",
    "\n",
    "!pip install https://data.pyg.org/whl/torch-1.12.0%2Bcu102/torch_cluster-1.6.0%2Bpt112cu102-cp37-cp37m-linux_x86_64.whl\n",
    "\n",
    "!pip install https://data.pyg.org/whl/torch-1.12.0%2Bcu102/torch_scatter-2.1.0%2Bpt112cu102-cp37-cp37m-linux_x86_64.whl\n",
    "\n",
    "!pip install https://data.pyg.org/whl/torch-1.12.0%2Bcu102/torch_sparse-0.6.16%2Bpt112cu102-cp37-cp37m-linux_x86_64.whl\n",
    "\n",
    "!pip install torch-geometric\n",
    "\n",
    "!pip install matplotlib\n",
    "\n",
    "!pip install networkx\n",
    "\n",
    "\n",
    "# Dataset Preparation\n",
    "!cd /home/admin1/Desktop/gnndataset/datasets/WaterDrop/\n",
    "\n",
    "# metadata.json\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1o6cKxgbnfUUFPTX1JngBzB928w2bUIwk' -O metadata.json\n",
    "\n",
    "# test_offset.json\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vr4JiVliKCQNWVV4kziyusxNVUvQuAYL' -O test_offset.json\n",
    "\n",
    "# test_particle_type.dat\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Z_r9ivdKqKZzVJG80gb2uY6JDVRd0wAt' -O test_particle_type.dat\n",
    "\n",
    "# test_position.dat\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1wCeBz1pZ5hxmlqWw4eylajg6pzFgQjIJ' -O test_position.dat\n",
    "\n",
    "# train_offset.json\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=160wnp9PEc1HuzsBi7kO0ryMu3tnon2tI' -O train_offset.json\n",
    "\n",
    "# train_particle_type.dat\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1LVtGLld7assF4sPk0mF2Bz2F7FBaxU0O' -O train_particle_type.dat\n",
    "\n",
    "# train_position.dat\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1YCXcir_fmJZLvXkbPjchsrr8VuuWugH0' -O train_position.dat\n",
    "\n",
    "# valid_offset.json\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1tiDP5uHMJQDTNxyRNSb6sEZCWAADPu8a' -O valid_offset.json\n",
    "\n",
    "# valid_particle_type.dat\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1fXIw9RWM0xzfK2sGn1H0DaAOxzm59ZEd' -O valid_particle_type.dat\n",
    "\n",
    "# valid_position.dat\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1U9QuV3Ra0E1tDD1HgXYCYyn4SeLKXQGs' -O valid_position.dat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z4HbK-OPUK8"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "• Dataset WaterDropSmall includes 100 videos of dropping water to ground rendered in a particle-based physics simulator. \n",
    "\n",
    "• It is a cropped version of WaterDrop dataset by Deepmind. \n",
    "\n",
    "• will download this dataset from Google Cloud stoarge to folder `temp/datasets` in file system. \n",
    "\n",
    "• may inspect downloaded files on **Files** menu on left of this Colab.\n",
    "\n",
    "`metadata.json` file in dataset includes following information:\n",
    "1. sequence length of each video data point\n",
    "2. dimensionality, 2d or 3d\n",
    "3. box bounds - specify bounding box for scene\n",
    "4. default connectivity radius - defines size of each particle's neighborhood\n",
    "5. statistics for normalization e.g. velocity mean and standard deviation and acceleration of particles\n",
    "\n",
    "\n",
    "Each data point in dataset includes following information:\n",
    "1. Particle type, such as water\n",
    "2. particle positions at each frame in video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxevKO1yPb4d",
    "outputId": "d4f3df44-fc86-4822-bb36-ca383dfc235d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch has version 2.3.0+cpu with cuda None\n",
      "##################\n",
      "## DATASET_NAME ##\n",
      "##################\n",
      "WaterDrop\n",
      "\n",
      "################\n",
      "## OUTPUT_DIR ##\n",
      "################\n",
      "/home/admin1/Desktop/gnndataset/datasets/WaterDrop\n",
      "\n",
      "####################\n",
      "## META_DATA_PATH ##\n",
      "####################\n",
      "/home/admin1/Desktop/gnndataset/datasets/WaterDrop/metadata.json\n",
      "\n",
      "###############\n",
      "## DATA_PATH ##\n",
      "###############\n",
      "/home/admin1/Desktop/gnndataset/datasets/WaterDrop/test_offset.json\n",
      "\n",
      "###############\n",
      "## DATA_PATH ##\n",
      "###############\n",
      "/home/admin1/Desktop/gnndataset/datasets/WaterDrop/test_particle_type.dat\n",
      "\n",
      "###############\n",
      "## DATA_PATH ##\n",
      "###############\n",
      "/home/admin1/Desktop/gnndataset/datasets/WaterDrop/test_position.dat\n",
      "\n",
      "###############\n",
      "## DATA_PATH ##\n",
      "###############\n",
      "/home/admin1/Desktop/gnndataset/datasets/WaterDrop/train_offset.json\n",
      "\n",
      "###############\n",
      "## DATA_PATH ##\n",
      "###############\n",
      "/home/admin1/Desktop/gnndataset/datasets/WaterDrop/train_particle_type.dat\n",
      "\n",
      "###############\n",
      "## DATA_PATH ##\n",
      "###############\n",
      "/home/admin1/Desktop/gnndataset/datasets/WaterDrop/train_position.dat\n",
      "\n",
      "###############\n",
      "## DATA_PATH ##\n",
      "###############\n",
      "/home/admin1/Desktop/gnndataset/datasets/WaterDrop/valid_offset.json\n",
      "\n",
      "###############\n",
      "## DATA_PATH ##\n",
      "###############\n",
      "/home/admin1/Desktop/gnndataset/datasets/WaterDrop/valid_particle_type.dat\n",
      "\n",
      "###############\n",
      "## DATA_PATH ##\n",
      "###############\n",
      "/home/admin1/Desktop/gnndataset/datasets/WaterDrop/valid_position.dat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "print(f\"PyTorch has version {torch.__version__} with cuda {torch.version.cuda}\")\n",
    "\n",
    "DATASET_NAME = \"WaterDrop\"\n",
    "OUTPUT_DIR = os.path.join(\"/home/admin1/Desktop/gnndataset/datasets/WaterDrop\")\n",
    "\n",
    "print(\"##################\")\n",
    "print(\"## DATASET_NAME ##\")\n",
    "print(\"##################\")\n",
    "print(DATASET_NAME)\n",
    "print(\"\")\n",
    "\n",
    "print(\"################\")\n",
    "print(\"## OUTPUT_DIR ##\")\n",
    "print(\"################\")\n",
    "print(OUTPUT_DIR)\n",
    "print(\"\")\n",
    "\n",
    "# BASE_URL = f\"https://storage.googleapis.com/cs224w_course_project_dataset/{DATASET_NAME}\"\n",
    "\n",
    "# !mkdir -p \"$OUTPUT_DIR\"\n",
    "\n",
    "META_DATA_PATH = f\"{OUTPUT_DIR}/metadata.json\"\n",
    "print(\"####################\")\n",
    "print(\"## META_DATA_PATH ##\")\n",
    "print(\"####################\")\n",
    "print(META_DATA_PATH)\n",
    "print(\"\")\n",
    "\n",
    "# CLOUD_PATH = f\"{BASE_URL}/metadata.json\"\n",
    "# !wget -O \"$META_DATA_PATH\" \"$CLOUD_PATH\"\n",
    "\n",
    "for split in [\"test\", \"train\", \"valid\"]:\n",
    "  for suffix in [\"offset.json\", \"particle_type.dat\", \"position.dat\"]:\n",
    "      DATA_PATH = f\"{OUTPUT_DIR}/{split}_{suffix}\"\n",
    "      print(\"###############\")\n",
    "      print(\"## DATA_PATH ##\")\n",
    "      print(\"###############\")\n",
    "      print(DATA_PATH)\n",
    "      print(\"\")\n",
    "\n",
    "#     CLOUD_PATH = f\"{BASE_URL}/{split}_{suffix}\"\n",
    "\n",
    "#     !wget -O \"$DATA_PATH\" \"$CLOUD_PATH\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NW0_YsEPG68T"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "• Cannot apply raw data in dataset to train GNN model directly, so must perform below steps to convert raw data into graphs with descriptive node features and edge features:\n",
    "1. Apply noise to trajectory to have more diverse training examples\n",
    "1. Construct graph based on distance between particles\n",
    "1. Extract node-level features: particle velocities and their distance to boundary\n",
    "1. Extract edge-level features: displacement and distance between particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aCy3zaaOGrrS"
   },
   "outputs": [],
   "source": [
    "!export LD_LIBRARY_PATH=/home/admin1/anaconda3/envs/GNN/lib:$LD_LIBRARY_PATH\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aCy3zaaOGrrS"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch_geometric as pyg\n",
    "\n",
    "def generate_noise(position_seq, noise_std):\n",
    "    \"\"\"Generate noise for a trajectory\"\"\"\n",
    "    # Calculate the velocity by subtracting each position from the next one\n",
    "    # Find the changes in position (how much each position moves to the next one)\n",
    "    velocity_seq = position_seq[:, 1:] - position_seq[:, :-1]\n",
    "    print(\"####################################\")\n",
    "    print(\"## velocity_seq ## generate_noise ##\")\n",
    "    print(\"####################################\")\n",
    "    print(velocity_seq)\n",
    "    print(\"\")\n",
    "\n",
    "    # Determine the number of time steps from the velocity sequence\n",
    "    # Find out how many movements (time steps) there are\n",
    "    time_steps = velocity_seq.size(1)\n",
    "    print(\"##################################\")\n",
    "    print(\"## time_steps ## generate_noise ##\")\n",
    "    print(\"##################################\")\n",
    "    print(time_steps)\n",
    "    print(\"\")\n",
    "\n",
    "    # Generate random noise for the velocity, scaled by noise_std and time steps\n",
    "    # Create random movements (noise) for the velocity, adjusted by noise level and time steps\n",
    "    velocity_noise = torch.randn_like(velocity_seq) * (noise_std / time_steps ** 0.5)\n",
    "    print(\"######################################\")\n",
    "    print(\"## velocity_noise ## generate_noise ##\")\n",
    "    print(\"######################################\")\n",
    "    print(velocity_noise)\n",
    "    print(\"\")\n",
    "\n",
    "    # Cumulatively sum the velocity noise over time to get smooth noise\n",
    "    # Add up the random velocity movements over time to make them smoother\n",
    "    velocity_noise = velocity_noise.cumsum(dim=1)\n",
    "    print(\"######################################\")\n",
    "    print(\"## velocity_noise ## generate_noise ##\")\n",
    "    print(\"######################################\")\n",
    "    print(velocity_noise)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Cumulatively sum the velocity noise again to get position noise\n",
    "    # Add up the smooth velocity movements again to get position changes\n",
    "    position_noise = velocity_noise.cumsum(dim=1)\n",
    "    print(\"######################################\")\n",
    "    print(\"## position_noise ## generate_noise ##\")\n",
    "    print(\"######################################\")\n",
    "    print(position_noise)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Ensure the position noise starts with zero noise\n",
    "    # Make sure the noise starts at zero (no noise at the beginning)\n",
    "    position_noise = torch.cat((torch.zeros_like(position_noise)[:, 0:1], position_noise), dim=1)\n",
    "    print(\"######################################\")\n",
    "    print(\"## position_noise ## generate_noise ##\")\n",
    "    print(\"######################################\")\n",
    "    print(position_noise)\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "    return position_noise\n",
    "\n",
    "\n",
    "def preprocess(particle_type, position_seq, target_position, metadata, noise_std):\n",
    "    \"\"\"Preprocess a trajectory and construct graph\"\"\"\n",
    "    # apply noise to trajectory\n",
    "    # Add random fluctuations to the path    \n",
    "    position_noise = generate_noise(position_seq, noise_std)\n",
    "    print(\"##################################\")\n",
    "    print(\"## position_noise ## preprocess ##\")\n",
    "    print(\"##################################\")\n",
    "    print(position_noise)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Update the position sequence with the added noise\n",
    "    # Make the path look more jittery    \n",
    "    position_seq = position_seq + position_noise\n",
    "    print(\"################################\")\n",
    "    print(\"## position_seq ## preprocess ##\")\n",
    "    print(\"################################\")\n",
    "    print(position_seq)\n",
    "    print(\"\")\n",
    "    \n",
    "\n",
    "    # Calculate the most recent position of each particle\n",
    "    # Find the latest spot of each particle\n",
    "    recent_position = position_seq[:, -1]\n",
    "    print(\"###################################\")\n",
    "    print(\"## recent_position ## preprocess ##\")\n",
    "    print(\"###################################\")\n",
    "    print(recent_position)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Calculate the velocity sequence from the position sequence\n",
    "    # Find out how fast each position is changing    \n",
    "    velocity_seq = position_seq[:, 1:] - position_seq[:, :-1]\n",
    "    print(\"################################\")\n",
    "    print(\"## velocity_seq ## preprocess ##\")\n",
    "    print(\"################################\")\n",
    "    print(velocity_seq)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Determine the number of particles\n",
    "    # Find out how many particles there are    \n",
    "    n_particle = recent_position.size(0)\n",
    "    print(\"##############################\")\n",
    "    print(\"## n_particle ## preprocess ##\")\n",
    "    print(\"##############################\")\n",
    "    print(n_particle)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Construct a graph based on distances between particles\n",
    "    # Create a network of connections based on how close particles are    \n",
    "    edge_index = pyg.nn.radius_graph(recent_position, metadata[\"default_connectivity_radius\"], loop=True, max_num_neighbors=n_particle)\n",
    "    print(\"##############################\")\n",
    "    print(\"## edge_index ## preprocess ##\")\n",
    "    print(\"##############################\")\n",
    "    print(edge_index)\n",
    "    print(\"\")\n",
    "\n",
    "    # node-level features: velocity, distance to boundary\n",
    "    \n",
    "    # Normalize the velocity sequence using metadata\n",
    "    # Standardize the speed information for each particle    \n",
    "    normal_velocity_seq = (velocity_seq - torch.tensor(metadata[\"vel_mean\"])) / torch.sqrt(torch.tensor(metadata[\"vel_std\"]) ** 2 + noise_std ** 2)\n",
    "    print(\"#######################################\")\n",
    "    print(\"## normal_velocity_seq ## preprocess ##\")\n",
    "    print(\"#######################################\")\n",
    "    print(normal_velocity_seq)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Define the boundary based on metadata\n",
    "    # Get the edges of the area where particles can be    \n",
    "    boundary = torch.tensor(metadata[\"bounds\"])\n",
    "    print(\"############################\")\n",
    "    print(\"## boundary ## preprocess ##\")\n",
    "    print(\"############################\")\n",
    "    print(boundary)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Calculate the distance from each particle to the lower boundary\n",
    "    # Find out how close each particle is to the lower edge    \n",
    "    distance_to_lower_boundary = recent_position - boundary[:, 0]\n",
    "    print(\"##############################################\")\n",
    "    print(\"## distance_to_lower_boundary ## preprocess ##\")\n",
    "    print(\"##############################################\")\n",
    "    print(\"Shape:\", distance_to_lower_boundary.shape)\n",
    "    print(\"Length:\", len(distance_to_lower_boundary))\n",
    "    print(\"Type:\", type(distance_to_lower_boundary))\n",
    "    print(distance_to_lower_boundary)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Calculate the distance from each particle to the upper boundary\n",
    "    # Find out how close each particle is to the upper edge    \n",
    "    distance_to_upper_boundary = boundary[:, 1] - recent_position\n",
    "    print(\"##############################################\")\n",
    "    print(\"## distance_to_upper_boundary ## preprocess ##\")\n",
    "    print(\"##############################################\")\n",
    "    print(\"Shape:\", distance_to_upper_boundary.shape)\n",
    "    print(\"Length:\", len(distance_to_upper_boundary))\n",
    "    print(\"Type:\", type(distance_to_upper_boundary))\n",
    "    print(distance_to_upper_boundary)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Combine the distances to the lower and upper boundaries\n",
    "    # Merge the distances to both edges    \n",
    "    distance_to_boundary = torch.cat((distance_to_lower_boundary, distance_to_upper_boundary), dim=-1)\n",
    "    print(\"########################################\")\n",
    "    print(\"## distance_to_boundary ## preprocess ##\")\n",
    "    print(\"########################################\")\n",
    "    print(\"Shape:\", distance_to_boundary.shape)\n",
    "    print(\"Length:\", len(distance_to_boundary))\n",
    "    print(\"Type:\", type(distance_to_boundary))\n",
    "    print(distance_to_boundary)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Clip the distance values to be within a specified range\n",
    "    # Make sure the distance values are within a certain range    \n",
    "    distance_to_boundary = torch.clip(distance_to_boundary / metadata[\"default_connectivity_radius\"], -1.0, 1.0)\n",
    "    print(\"########################################\")\n",
    "    print(\"## distance_to_boundary ## preprocess ##\")\n",
    "    print(\"########################################\")\n",
    "    print(\"Shape:\", distance_to_boundary.shape)\n",
    "    print(\"Length:\", len(distance_to_boundary))\n",
    "    print(\"Type:\", type(distance_to_boundary))\n",
    "    print(distance_to_boundary)\n",
    "    print(\"\")\n",
    "    \n",
    "    # edge-level features: displacement, distance\n",
    "\n",
    "    # Calculate the displacement for each edge\n",
    "    # Find out the shift between connected particles    \n",
    "    dim = recent_position.size(-1)\n",
    "    print(\"#######################\")\n",
    "    print(\"## dim ## preprocess ##\")\n",
    "    print(\"#######################\")\n",
    "    # print(\"Shape:\", dim.shape)\n",
    "    # print(\"Length:\", len(dim))\n",
    "    print(\"Type:\", type(dim))\n",
    "    print(dim)\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "    edge_displacement = (torch.gather(recent_position, dim=0, index=edge_index[0].unsqueeze(-1).expand(-1, dim)) -\n",
    "                   torch.gather(recent_position, dim=0, index=edge_index[1].unsqueeze(-1).expand(-1, dim)))\n",
    "    print(\"#####################################\")\n",
    "    print(\"## edge_displacement ## preprocess ##\")\n",
    "    print(\"#####################################\")\n",
    "    print(\"Shape:\", edge_displacement.shape)\n",
    "    print(\"Length:\", len(edge_displacement))\n",
    "    print(\"Type:\", type(edge_displacement))\n",
    "    print(edge_displacement)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Normalize the displacement by the default connectivity radius\n",
    "    # Adjust the shift based on a standard distance    \n",
    "    edge_displacement /= metadata[\"default_connectivity_radius\"]\n",
    "    print(\"#####################################\")\n",
    "    print(\"## edge_displacement ## preprocess ##\")\n",
    "    print(\"#####################################\")\n",
    "    print(\"Shape:\", edge_displacement.shape)\n",
    "    print(\"Length:\", len(edge_displacement))\n",
    "    print(\"Type:\", type(edge_displacement))\n",
    "    print(edge_displacement)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Calculate the distance for each edge\n",
    "    # Measure how far apart connected particles are    \n",
    "    edge_distance = torch.norm(edge_displacement, dim=-1, keepdim=True)\n",
    "    print(\"#################################\")\n",
    "    print(\"## edge_distance ## preprocess ##\")\n",
    "    print(\"#################################\")\n",
    "    print(\"Shape:\", edge_distance.shape)\n",
    "    print(\"Length:\", len(edge_distance))\n",
    "    print(\"Type:\", type(edge_distance))\n",
    "    print(edge_distance)\n",
    "    print(\"\")\n",
    "\n",
    "    \n",
    "    # ground truth for training\n",
    "\n",
    "    # Check if the target position is provided for training\n",
    "    # If there is a target position, use it for training    \n",
    "    if target_position is not None:\n",
    "        # Calculate the last velocity of each particle\n",
    "        # Find the most recent speed of each particle        \n",
    "        last_velocity = velocity_seq[:, -1]\n",
    "        print(\"#################################\")\n",
    "        print(\"## last_velocity ## preprocess ##\")\n",
    "        print(\"#################################\")\n",
    "        print(\"Shape:\", last_velocity.shape)\n",
    "        print(\"Length:\", len(last_velocity))\n",
    "        print(\"Type:\", type(last_velocity))\n",
    "        print(last_velocity)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Calculate the next velocity towards the target position\n",
    "        # Find the speed needed to reach the target        \n",
    "        next_velocity = target_position + position_noise[:, -1] - recent_position\n",
    "        print(\"#################################\")\n",
    "        print(\"## next_velocity ## preprocess ##\")\n",
    "        print(\"#################################\")\n",
    "        print(\"Shape:\", next_velocity.shape)\n",
    "        print(\"Length:\", len(next_velocity))\n",
    "        print(\"Type:\", type(next_velocity))\n",
    "        print(next_velocity)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Calculate the acceleration required to reach the target\n",
    "        # Find out the change in speed needed        \n",
    "        acceleration = next_velocity - last_velocity\n",
    "        print(\"################################\")\n",
    "        print(\"## acceleration ## preprocess ##\")\n",
    "        print(\"################################\")\n",
    "        print(\"Shape:\", acceleration.shape)\n",
    "        print(\"Length:\", len(acceleration))\n",
    "        print(\"Type:\", type(acceleration))\n",
    "        print(acceleration)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Normalize the acceleration using metadata\n",
    "        # Standardize the change in speed        \n",
    "        acceleration = (acceleration - torch.tensor(metadata[\"acc_mean\"])) / torch.sqrt(torch.tensor(metadata[\"acc_std\"]) ** 2 + noise_std ** 2)\n",
    "        print(\"################################\")\n",
    "        print(\"## acceleration ## preprocess ##\")\n",
    "        print(\"################################\")\n",
    "        print(\"Shape:\", acceleration.shape)\n",
    "        print(\"Length:\", len(acceleration))\n",
    "        print(\"Type:\", type(acceleration))\n",
    "        print(acceleration)\n",
    "        print(\"\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        # Set acceleration to None if target position is not provided\n",
    "        # No acceleration calculation needed if no target        \n",
    "        acceleration = None\n",
    "\n",
    "    # return graph with features\n",
    "\n",
    "    # Construct and return the graph with node and edge features\n",
    "    # Give back the network of particles with all the calculated information    \n",
    "    graph = pyg.data.Data(\n",
    "        x=particle_type,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=torch.cat((edge_displacement, edge_distance), dim=-1),\n",
    "        y=acceleration,\n",
    "        pos=torch.cat((velocity_seq.reshape(velocity_seq.size(0), -1), distance_to_boundary), dim=-1)\n",
    "    )\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqfx4NcguDEY"
   },
   "source": [
    "### One Step Dataset\n",
    "\n",
    "• Each datapoint in this dataset contains trajectories sliced to short time windows. \n",
    "\n",
    "• We use this dataset in training phase because history of particles' states are necessary for model to make predictions. \n",
    "\n",
    "• But in meantime, since long-horizon prediction is inaccurate and time-consuming, sliced trajectories to short time windows to improve perfomance of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "b2HrUjPnsF_4"
   },
   "outputs": [],
   "source": [
    "class OneStepDataset(pyg.data.Dataset):\n",
    "    def __init__(self, data_path, split, window_length=7, noise_std=0.0, return_pos=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(\"#################################\")\n",
    "        print(\"## data_path ## OneStepDataset ##\")\n",
    "        print(\"#################################\")\n",
    "        print(data_path)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "        # Load dataset metadata from disk\n",
    "        # Read the general information about the dataset from a file        \n",
    "        # load dataset from disk\n",
    "        with open(os.path.join(data_path, \"metadata.json\")) as f:\n",
    "            self.metadata = json.load(f)\n",
    "            print(\"#####################################\")\n",
    "            print(\"## self.metadata ## OneStepDataset ##\")\n",
    "            print(\"#####################################\")\n",
    "            # print(\"Shape:\", self.metadata.shape)\n",
    "            print(\"Length:\", len(self.metadata))\n",
    "            print(\"Type:\", type(self.metadata))\n",
    "            print(self.metadata)\n",
    "            print(\"\")\n",
    "            \n",
    "        # Load dataset offset information from disk\n",
    "        # Read the information about where data segments start and end from a file            \n",
    "        with open(os.path.join(data_path, f\"{split}_offset.json\")) as f:\n",
    "            self.offset = json.load(f)\n",
    "            print(\"###################################\")\n",
    "            print(\"## self.offset ## OneStepDataset ## 1\")\n",
    "            print(\"###################################\")\n",
    "            # print(\"Shape:\", self.offset.shape)\n",
    "            print(\"Length:\", len(self.offset))\n",
    "            print(\"Type:\", type(self.offset))\n",
    "            print(self.offset)\n",
    "            print(\"\")\n",
    "            \n",
    "        # Convert offset keys from strings to integers\n",
    "        # Change the starting points data keys to numbers            \n",
    "        self.offset = {int(k): v for k, v in self.offset.items()}\n",
    "        print(\"###################################\")\n",
    "        print(\"## self.offset ## OneStepDataset ## 2\")\n",
    "        print(\"###################################\")\n",
    "        #print(\"Shape:\", self.offset.shape)\n",
    "        print(\"Length:\", len(self.offset))\n",
    "        print(\"Type:\", type(self.offset))\n",
    "        print(self.offset)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Store the window length for slicing data\n",
    "        # Keep the size of data chunks to be used        \n",
    "        self.window_length = window_length\n",
    "        print(\"#####################################\")\n",
    "        print(\"## window_length ## OneStepDataset ##\")\n",
    "        print(\"#####################################\")\n",
    "        # print(\"Shape:\", window_length.shape)\n",
    "        # print(\"Length:\", len(window_length))\n",
    "        print(\"Type:\", type(window_length))\n",
    "        print(window_length)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Store the noise standard deviation\n",
    "        # Keep the level of randomness to be added        \n",
    "        self.noise_std = noise_std\n",
    "        print(\"#################################\")\n",
    "        print(\"## noise_std ## OneStepDataset ##\")\n",
    "        print(\"#################################\")\n",
    "        # print(\"Shape:\", noise_std.shape)\n",
    "        # print(\"Length:\", len(noise_std))\n",
    "        print(\"Type:\", type(noise_std))\n",
    "        print(noise_std)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Store the flag for returning positions\n",
    "        # Decide if the positions should be given back        \n",
    "        self.return_pos = return_pos\n",
    "        print(\"##################################\")\n",
    "        print(\"## return_pos ## OneStepDataset ##\")\n",
    "        print(\"##################################\")\n",
    "        # print(\"Shape:\", return_pos.shape)\n",
    "        # print(\"Length:\", len(return_pos))\n",
    "        print(\"Type:\", type(return_pos))\n",
    "        print(return_pos)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Load particle types from memory-mapped file\n",
    "        # Read the type of each particle directly from the file\n",
    "        self.particle_type = np.memmap(os.path.join(data_path, f\"{split}_particle_type.dat\"), dtype=np.int64, mode=\"r\")\n",
    "        print(\"##########################################\")\n",
    "        print(\"## self.particle_type ## OneStepDataset ##\")\n",
    "        print(\"##########################################\")\n",
    "        print(\"Shape:\", self.particle_type.shape)\n",
    "        print(\"Length:\", len(self.particle_type))\n",
    "        print(\"Type:\", type(self.particle_type))\n",
    "        print(self.particle_type)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Load particle positions from memory-mapped file\n",
    "        # Read the positions of particles directly from the file        \n",
    "        self.position = np.memmap(os.path.join(data_path, f\"{split}_position.dat\"), dtype=np.float32, mode=\"r\")\n",
    "        print(\"#####################################\")\n",
    "        print(\"## self.position ## OneStepDataset ##\")\n",
    "        print(\"#####################################\")\n",
    "        print(\"Shape:\", self.position.shape)\n",
    "        print(\"Length:\", len(self.position))\n",
    "        print(\"Type:\", type(self.position))\n",
    "        print(self.position)\n",
    "        print(\"\")\n",
    "\n",
    "        # Determine the dimensionality of the position data\n",
    "        # Find out the number of position coordinates (e.g., 2D or 3D)        \n",
    "        for traj in self.offset.values():\n",
    "            self.dim = traj[\"position\"][\"shape\"][2]\n",
    "            print(\"################################\")\n",
    "            print(\"## self.dim ## OneStepDataset ##\")\n",
    "            print(\"################################\")\n",
    "            # print(\"Shape:\", self.dim.shape)\n",
    "            # print(\"Length:\", len(self.dim))\n",
    "            print(\"Type:\", type(self.dim))\n",
    "            print(self.dim)\n",
    "            print(\"\")\n",
    "            \n",
    "            break\n",
    "\n",
    "        # Cut particle trajectories according to time slices\n",
    "        # Divide the particle paths into smaller segments over time        \n",
    "        self.windows = []\n",
    "        for traj in self.offset.values():\n",
    "            # Get the size of the particle trajectory\n",
    "            # Find out how many particles are in the trajectory            \n",
    "            size = traj[\"position\"][\"shape\"][1]\n",
    "            print(\"############################\")\n",
    "            print(\"## size ## OneStepDataset ##\")\n",
    "            print(\"############################\")\n",
    "            # print(\"Shape:\", size.shape)\n",
    "            # print(\"Length:\", len(size))\n",
    "            print(\"Type:\", type(size))\n",
    "            print(size)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Calculate the number of windows for the trajectory\n",
    "            # Determine how many chunks can be made from the trajectory            \n",
    "            length = traj[\"position\"][\"shape\"][0] - window_length + 1\n",
    "            print(\"############\")\n",
    "            print(\"## length ## OneStepDataset ##\")\n",
    "            print(\"############\")\n",
    "            # print(\"Shape:\", length.shape)\n",
    "            # print(\"Length:\", len(length))\n",
    "            print(\"Type:\", type(length))\n",
    "            print(length)\n",
    "            print(\"\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            for i in range(length):\n",
    "                # Create a description for each window\n",
    "                # Define each data chunk with its size and location\n",
    "                desc = {\n",
    "                    \"size\": size,\n",
    "                    \"type\": traj[\"particle_type\"][\"offset\"],\n",
    "                    \"pos\": traj[\"position\"][\"offset\"] + i * size * self.dim,\n",
    "                }\n",
    "\n",
    "                # Append the window description to the list\n",
    "                # Add the data chunk details to the list                \n",
    "                self.windows.append(desc)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.windows)\n",
    "\n",
    "    def get(self, idx):\n",
    "        # load corresponding data for this time slice\n",
    "        # Fetch the data chunk for the given index        \n",
    "        window = self.windows[idx]\n",
    "        print(\"###################\")\n",
    "        print(\"## window ## get ##\")\n",
    "        print(\"###################\")\n",
    "        # print(\"Shape:\", window.shape)\n",
    "        print(\"Length:\", len(window))\n",
    "        print(\"Type:\", type(window))\n",
    "        print(window)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Get the size of the particle trajectory\n",
    "        # Find out how many particles are in this chunk        \n",
    "        size = window[\"size\"]\n",
    "        print(\"#################\")\n",
    "        print(\"## size ## get ##\")\n",
    "        print(\"#################\")\n",
    "        # print(\"Shape:\", size.shape)\n",
    "        # print(\"Length:\", len(size))\n",
    "        print(\"Type:\", type(size))\n",
    "        print(size)\n",
    "        print(\"\")\n",
    "        \n",
    "    # Extract the particle types for this time slice\n",
    "    # Get the types of particles in this chunk        \n",
    "        particle_type = self.particle_type[window[\"type\"]: window[\"type\"] + size].copy()\n",
    "        print(\"##########################\")\n",
    "        print(\"## particle_type ## get ##\")\n",
    "        print(\"##########################\")\n",
    "        print(\"Shape:\", particle_type.shape)\n",
    "        print(\"Length:\", len(particle_type))\n",
    "        print(\"Type:\", type(particle_type))\n",
    "        print(particle_type)\n",
    "        print(\"\")\n",
    "        \n",
    "    # Convert the particle types to a PyTorch tensor\n",
    "    # Change the particle types into a format PyTorch can use        \n",
    "        particle_type = torch.from_numpy(particle_type)\n",
    "        print(\"##########################\")\n",
    "        print(\"## particle_type ## get ##\")\n",
    "        print(\"##########################\")\n",
    "        print(\"Shape:\", particle_type.shape)\n",
    "        print(\"Length:\", len(particle_type))\n",
    "        print(\"Type:\", type(particle_type))\n",
    "        print(particle_type)\n",
    "        print(\"\")\n",
    "        \n",
    "    # Extract the position sequence for this time slice\n",
    "    # Get the positions of particles in this chunk        \n",
    "        position_seq = self.position[window[\"pos\"]: window[\"pos\"] + self.window_length * size * self.dim].copy()\n",
    "        print(\"#########################\")\n",
    "        print(\"## position_seq ## get ##\")\n",
    "        print(\"#########################\")\n",
    "        print(\"Shape:\", position_seq.shape)\n",
    "        print(\"Length:\", len(position_seq))\n",
    "        print(\"Type:\", type(position_seq))\n",
    "        print(position_seq)\n",
    "        print(\"\")        \n",
    "\n",
    "    # Resize the position sequence to the correct shape\n",
    "    # Reshape the position data to fit the window length and particle count        \n",
    "        position_seq.resize(self.window_length, size, self.dim)\n",
    "        print(\"#########################\")\n",
    "        print(\"## position_seq ## get ##\")\n",
    "        print(\"#########################\")\n",
    "        print(\"Shape:\", position_seq.shape)\n",
    "        print(\"Length:\", len(position_seq))\n",
    "        print(\"Type:\", type(position_seq))\n",
    "        print(position_seq)\n",
    "        print(\"\")        \n",
    "\n",
    "    # Transpose the position sequence for correct orientation\n",
    "    # Flip the data around so it matches the expected format        \n",
    "        position_seq = position_seq.transpose(1, 0, 2)\n",
    "        print(\"#########################\")\n",
    "        print(\"## position_seq ## get ##\")\n",
    "        print(\"#########################\")\n",
    "        print(\"Shape:\", position_seq.shape)\n",
    "        print(\"Length:\", len(position_seq))\n",
    "        print(\"Type:\", type(position_seq))\n",
    "        print(position_seq)\n",
    "        print(\"\")        \n",
    "\n",
    "    # Get the target position (last position in the sequence)\n",
    "    # Find the final position of each particle in the chunk        \n",
    "        target_position = position_seq[:, -1]\n",
    "        print(\"############################\")\n",
    "        print(\"## target_position ## get ##\")\n",
    "        print(\"############################\")\n",
    "        print(\"Shape:\", target_position.shape)\n",
    "        print(\"Length:\", len(target_position))\n",
    "        print(\"Type:\", type(target_position))\n",
    "        print(target_position)\n",
    "        print(\"\")\n",
    "        \n",
    "    # Remove the last position from the position sequence\n",
    "    # Take out the final positions from the sequence        \n",
    "        position_seq = position_seq[:, :-1]\n",
    "        print(\"#########################\")\n",
    "        print(\"## position_seq ## get ##\")\n",
    "        print(\"#########################\")\n",
    "        print(\"Shape:\", position_seq.shape)\n",
    "        print(\"Length:\", len(position_seq))\n",
    "        print(\"Type:\", type(position_seq))\n",
    "        print(position_seq)\n",
    "        print(\"\")        \n",
    "\n",
    "    # Convert the target positions to a PyTorch tensor\n",
    "    # Change the final positions into a format PyTorch can use        \n",
    "        target_position = torch.from_numpy(target_position)\n",
    "        print(\"############################\")\n",
    "        print(\"## target_position ## get ##\")\n",
    "        print(\"############################\")\n",
    "        print(\"Shape:\", target_position.shape)\n",
    "        print(\"Length:\", len(target_position))\n",
    "        print(\"Type:\", type(target_position))\n",
    "        print(target_position)\n",
    "        print(\"\")\n",
    "        \n",
    "    # Convert the position sequence to a PyTorch tensor\n",
    "    # Change the position sequence into a format PyTorch can use        \n",
    "        position_seq = torch.from_numpy(position_seq)\n",
    "        print(\"#########################\")\n",
    "        print(\"## position_seq ## get ##\")\n",
    "        print(\"#########################\")\n",
    "        print(\"Shape:\", position_seq.shape)\n",
    "        print(\"Length:\", len(position_seq))\n",
    "        print(\"Type:\", type(position_seq))\n",
    "        print(position_seq)\n",
    "        print(\"\")\n",
    "\n",
    "        # construct graph\n",
    "        with torch.no_grad():\n",
    "            graph = preprocess(particle_type, position_seq, target_position, self.metadata, self.noise_std)\n",
    "        if self.return_pos:\n",
    "          return graph, position_seq[:, -1]\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqhPcZeKthYq"
   },
   "source": [
    "### Rollout Dataset\n",
    "\n",
    "• Each datapoint in this dataset contains trajectories of particles over 1000 time frames. \n",
    "\n",
    "• This dataset used in evaluation phase to measure model's ability to make long-horizon predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Kuk2Z-I8sFv7"
   },
   "outputs": [],
   "source": [
    "class RolloutDataset(pyg.data.Dataset):\n",
    "    def __init__(self, data_path, split, window_length=7):\n",
    "        super().__init__()\n",
    "\n",
    "        # load data from disk\n",
    "\n",
    "        # Load metadata from disk\n",
    "        # Read the general information about the dataset from a file        \n",
    "        with open(os.path.join(data_path, \"metadata.json\")) as f:\n",
    "            self.metadata = json.load(f)\n",
    "            print(\"############################################\")\n",
    "            print(\"## self.metadata ## RolloutDataset\\_init_ ##\")\n",
    "            print(\"############################################\")\n",
    "            # print(\"Shape:\", self.metadata.shape)\n",
    "            print(\"Length:\", len(self.metadata))\n",
    "            print(\"Type:\", type(self.metadata))\n",
    "            print(self.metadata)\n",
    "            print(\"\")\n",
    "            \n",
    "        # Load dataset offset information from disk\n",
    "        # Read the information about where data segments start and end from a file            \n",
    "        with open(os.path.join(data_path, f\"{split}_offset.json\")) as f:\n",
    "            self.offset = json.load(f)\n",
    "            print(\"##########################################\")\n",
    "            print(\"## self.offset ## RolloutDataset\\_init_ ##\")\n",
    "            print(\"##########################################\")\n",
    "            # print(\"Shape:\", self.offset.shape)\n",
    "            print(\"Length:\", len(self.offset))\n",
    "            print(\"Type:\", type(self.offset))\n",
    "            print(self.offset)\n",
    "            print(\"\")\n",
    "            \n",
    "        # Convert offset keys from strings to integers\n",
    "        # Change the starting points data keys to numbers            \n",
    "        self.offset = {int(k): v for k, v in self.offset.items()}\n",
    "        print(\"##########################################\")\n",
    "        print(\"## self.offset ## RolloutDataset\\_init_ ##\")\n",
    "        print(\"##########################################\")\n",
    "        # print(\"Shape:\", self.offset.shape)\n",
    "        print(\"Length:\", len(self.offset))\n",
    "        print(\"Type:\", type(self.offset))\n",
    "        print(self.offset)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Store the window length for slicing data\n",
    "        # Keep the size of data chunks to be used        \n",
    "        self.window_length = window_length\n",
    "        print(\"############################################\")\n",
    "        print(\"## window_length ## RolloutDataset\\_init_ ##\")\n",
    "        print(\"############################################\")\n",
    "        # print(\"Shape:\", window_length.shape)\n",
    "        # print(\"Length:\", len(window_length))\n",
    "        print(\"Type:\", type(window_length))\n",
    "        print(window_length)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Load particle types from memory-mapped file\n",
    "        # Read the type of each particle directly from the file\n",
    "        self.particle_type = np.memmap(os.path.join(data_path, f\"{split}_particle_type.dat\"), dtype=np.int64, mode=\"r\")\n",
    "        print(\"#################################################\")\n",
    "        print(\"## self.particle_type ## RolloutDataset\\_init_ ##\")\n",
    "        print(\"#################################################\")\n",
    "        print(\"Shape:\", self.particle_type.shape)\n",
    "        print(\"Length:\", len(self.particle_type))\n",
    "        print(\"Type:\", type(self.particle_type))\n",
    "        print(self.particle_type)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Load particle positions from memory-mapped file\n",
    "        # Read the positions of particles directly from the file        \n",
    "        self.position = np.memmap(os.path.join(data_path, f\"{split}_position.dat\"), dtype=np.float32, mode=\"r\")\n",
    "        print(\"############################################\")\n",
    "        print(\"## self.position ## RolloutDataset\\_init_ ##\")\n",
    "        print(\"############################################\")\n",
    "        print(\"Shape:\", self.position.shape)\n",
    "        print(\"Length:\", len(self.position))\n",
    "        print(\"Type:\", type(self.position))\n",
    "        print(self.position)\n",
    "        print(\"\")\n",
    "\n",
    "        # Determine the dimensionality of the position data\n",
    "        # Find out the number of position coordinates (e.g., 2D or 3D)\n",
    "        for traj in self.offset.values():\n",
    "            self.dim = traj[\"position\"][\"shape\"][2]\n",
    "            break\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.offset)\n",
    "\n",
    "    def get(self, idx):\n",
    "    # Get trajectory information for the given index\n",
    "    # Fetch the data segment information for the specified index\n",
    "        traj = self.offset[idx]\n",
    "        print(\"################################\")\n",
    "        print(\"## traj ## RolloutDataset\\get ##\")\n",
    "        print(\"################################\")\n",
    "        print(\"Shape:\", traj.shape)\n",
    "        print(\"Length:\", len(traj))\n",
    "        print(\"Type:\", type(traj))\n",
    "        print(traj)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Get the number of particles in this trajectory\n",
    "        # Find out how many particles are in this segment        \n",
    "        size = traj[\"position\"][\"shape\"][1]\n",
    "        print(\"################################\")\n",
    "        print(\"## size ## RolloutDataset\\get ##\")\n",
    "        print(\"################################\")\n",
    "        # print(\"Shape:\", size.shape)\n",
    "        # print(\"Length:\", len(size))\n",
    "        print(\"Type:\", type(size))\n",
    "        print(size)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Get the number of time steps in this trajectory\n",
    "        # Find out how many time points are in this segment        \n",
    "        time_step = traj[\"position\"][\"shape\"][0]\n",
    "        print(\"#####################################\")\n",
    "        print(\"## time_step ## RolloutDataset\\get ##\")\n",
    "        print(\"#####################################\")\n",
    "        print(\"Shape:\", time_step.shape)\n",
    "        print(\"Length:\", len(time_step))\n",
    "        print(\"Type:\", type(time_step))\n",
    "        print(time_step)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Extract the particle types for this trajectory\n",
    "        # Get the types of particles in this segment        \n",
    "        particle_type = self.particle_type[traj[\"particle_type\"][\"offset\"]: traj[\"particle_type\"][\"offset\"] + size].copy()\n",
    "        print(\"#########################################\")\n",
    "        print(\"## particle_type ## RolloutDataset\\get ##\")\n",
    "        print(\"#########################################\")\n",
    "        print(\"Shape:\", particle_type.shape)\n",
    "        print(\"Length:\", len(particle_type))\n",
    "        print(\"Type:\", type(particle_type))\n",
    "        print(particle_type)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Convert the particle types to a PyTorch tensor\n",
    "        # Change the particle types into a format PyTorch can use\n",
    "        particle_type = torch.from_numpy(particle_type)\n",
    "        print(\"#########################################\")\n",
    "        print(\"## particle_type ## RolloutDataset\\get ##\")\n",
    "        print(\"#########################################\")\n",
    "        print(\"Shape:\", particle_type.shape)\n",
    "        print(\"Length:\", len(particle_type))\n",
    "        print(\"Type:\", type(particle_type))\n",
    "        print(particle_type)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Extract the positions for this trajectory\n",
    "        # Get the positions of particles in this segment        \n",
    "        position = self.position[traj[\"position\"][\"offset\"]: traj[\"position\"][\"offset\"] + time_step * size * self.dim].copy()\n",
    "        print(\"##############\")\n",
    "        print(\"## position ## RolloutDataset\\get ##\")\n",
    "        print(\"##############\")\n",
    "        print(\"Shape:\", position.shape)\n",
    "        print(\"Length:\", len(position))\n",
    "        print(\"Type:\", type(position))\n",
    "        print(position)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Resize the position array to match the trajectory shape\n",
    "        # Reshape the position data to fit the segment structure        \n",
    "        position.resize(traj[\"position\"][\"shape\"])\n",
    "        print(\"####################################\")\n",
    "        print(\"## position ## RolloutDataset\\get ##\")\n",
    "        print(\"####################################\")\n",
    "        print(\"Shape:\", position.shape)\n",
    "        print(\"Length:\", len(position))\n",
    "        print(\"Type:\", type(position))\n",
    "        print(position)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Convert the position data to a PyTorch tensor\n",
    "        # Change the position data into a format PyTorch can use        \n",
    "        position = torch.from_numpy(position)\n",
    "        print(\"####################################\")\n",
    "        print(\"## position ## RolloutDataset\\get ##\")\n",
    "        print(\"####################################\")\n",
    "        print(\"Shape:\", position.shape)\n",
    "        print(\"Length:\", len(position))\n",
    "        print(\"Type:\", type(position))\n",
    "        print(position)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Create a dictionary with particle type and position\n",
    "        # Bundle the particle types and positions into a package        \n",
    "        data = {\"particle_type\": particle_type, \"position\": position}\n",
    "        print(\"################################\")\n",
    "        print(\"## data ## RolloutDataset\\get ##\")\n",
    "        print(\"################################\")\n",
    "        print(\"Shape:\", data.shape)\n",
    "        print(\"Length:\", len(data))\n",
    "        print(\"Type:\", type(data))\n",
    "        print(data)\n",
    "        print(\"\")\n",
    "        \n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVjrldn4kD-P"
   },
   "source": [
    "### Visualize a graph in dataset\n",
    "\n",
    "• Each data point in dataset is a `pyg.data.Data` object which describes a graph. \n",
    "\n",
    "• explain contents of first data point, visualize graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 656
    },
    "id": "_4MJuOhjkTjx",
    "outputId": "6ef28c82-9055-43d5-ec66-19b70e8a3ddf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OneStepDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m dataset_sample \u001b[38;5;241m=\u001b[39m \u001b[43mOneStepDataset\u001b[49m(OUTPUT_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m####################\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m## dataset_sample ##\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OneStepDataset' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Comment for above\n",
    "# Enables inline plotting in Jupyter notebooks\n",
    "# This ensures that plots appear within the Jupyter notebook.\n",
    "\n",
    "# Import matplotlib for plotting\n",
    "# This is used for creating and displaying plots.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import NetworkX for graph operations\n",
    "# This library is used for manipulating and visualizing graphs.\n",
    "import networkx as nx\n",
    "\n",
    "# Load a sample from the dataset\n",
    "# This line loads a sample from the validation set of the dataset, including positions.\n",
    "dataset_sample = OneStepDataset(OUTPUT_DIR, \"valid\", return_pos=True)\n",
    "print(\"####################\")\n",
    "print(\"## dataset_sample ##\")\n",
    "print(\"####################\")\n",
    "# print(\"Shape:\", dataset_sample.shape)\n",
    "print(\"Length:\", len(dataset_sample))\n",
    "print(\"Type:\", type(dataset_sample))\n",
    "print(dataset_sample)\n",
    "print(\"\")\n",
    "\n",
    "# Extract the graph and position from the dataset sample\n",
    "# This retrieves the first graph and its corresponding position data from the sample.\n",
    "graph, position = dataset_sample[0]\n",
    "print(\"###########\")\n",
    "print(\"## graph ##\")\n",
    "print(\"###########\")\n",
    "# print(\"Shape:\", graph.shape)\n",
    "print(\"Length:\", len(graph))\n",
    "print(\"Type:\", type(graph))\n",
    "print(graph)\n",
    "print(\"\")\n",
    "\n",
    "print(\"##############\")\n",
    "print(\"## position ##\")\n",
    "print(\"##############\")\n",
    "print(\"Shape:\", position.shape)\n",
    "print(\"Length:\", len(position))\n",
    "print(\"Type:\", type(position))\n",
    "print(position)\n",
    "print(\"\")\n",
    "\n",
    "print(f\"first item in valid set is a graph: {graph}\")\n",
    "print(f\"This graph has {graph.num_nodes} nodes and {graph.num_edges} edges.\")\n",
    "print(f\"Each node is a particle and each edge is interaction between two particles.\")\n",
    "print(f\"Each node has {graph.num_node_features} categorial feature (Data.x), which represents type of node.\")\n",
    "print(f\"Each node has a {graph.pos.size(1)}-dim feature vector (Data.pos), which represents positions and velocities of particle (node) in several frames.\")\n",
    "print(f\"Each edge has a {graph.num_edge_features}-dim feature vector (Data.edge_attr), which represents relative distance and displacement between particles.\")\n",
    "print(f\"model is expected to predict a {graph.y.size(1)}-dim vector for each node (Data.y), which represents acceleration of particle.\")\n",
    "\n",
    "# remove directions of edges, because it is a symmetric directed graph.\n",
    "# Convert to an undirected NetworkX graph\n",
    "# This converts the graph to an undirected NetworkX graph, removing edge directions.\n",
    "nx_graph = pyg.utils.to_networkx(graph).to_undirected()\n",
    "print(\"##############\")\n",
    "print(\"## nx_graph ##\")\n",
    "print(\"##############\")\n",
    "# print(\"Shape:\", nx_graph.shape)\n",
    "print(\"Length:\", len(nx_graph))\n",
    "print(\"Type:\", type(nx_graph))\n",
    "print(nx_graph)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# remove self loops, because every node has a self loop.\n",
    "# Remove self-loops from the graph\n",
    "# This removes any self-loops from the graph, where nodes have edges to themselves.\n",
    "nx_graph.remove_edges_from(nx.selfloop_edges(nx_graph))\n",
    "print(\"##############\")\n",
    "print(\"## nx_graph ##\")\n",
    "print(\"##############\")\n",
    "# print(\"Shape:\", nx_graph.shape)\n",
    "print(\"Length:\", len(nx_graph))\n",
    "print(\"Type:\", type(nx_graph))\n",
    "print(nx_graph)\n",
    "print(\"\")\n",
    "\n",
    "# Set up the figure size for the plot\n",
    "# This sets the size of the plot figure to 7x7 inches.\n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "# This draws the graph using the position data, setting the node size to 50.\n",
    "nx.draw(nx_graph, pos={i: tuple(v) for i, v in enumerate(position)}, node_size=50)\n",
    "\n",
    "# Display the plot\n",
    "# This displays the plot in the Jupyter notebook.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nlnk5URCF7pZ"
   },
   "source": [
    "## GNN Model\n",
    "\n",
    "We will walk through implementation of GNN model in this section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vsVI6epJcsn"
   },
   "source": [
    "### Helper class\n",
    "\n",
    "• first define a class for Multi-Layer Perceptron (MLP). \n",
    "\n",
    "• This class generates an MLP given width and depth of it. \n",
    "\n",
    "• Because MLPs are used in several places of GNN, this helper class will make code cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ht-upXnRo0dV"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch_scatter\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"Multi-Layer perceptron\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, layers, layernorm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize a list to hold the layers of the network\n",
    "        # Create an empty list to store the network's layers        \n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        print(\"###############################\")\n",
    "        print(\"## self.layers ## MLP\\_init_ ##\")\n",
    "        print(\"###############################\")\n",
    "        # print(\"Shape:\", self.layers.shape)\n",
    "        print(\"Length:\", len(self.layers))\n",
    "        print(\"Type:\", type(self.layers))\n",
    "        print(self.layers)\n",
    "        print(\"\")\n",
    "\n",
    "        # Add the specified number of layers to the network\n",
    "        # Build the network by adding the given number of layers        \n",
    "        for i in range(layers):\n",
    "            # Add a Linear layer (fully connected layer)\n",
    "            # Add a layer where every input is connected to every output            \n",
    "            self.layers.append(torch.nn.Linear(\n",
    "                input_size if i == 0 else hidden_size, # Use input size for the first layer, hidden size otherwise\n",
    "                output_size if i == layers - 1 else hidden_size, # Use output size for the last layer, hidden size otherwise\n",
    "            ))\n",
    "            \n",
    "            # Add a ReLU activation function for all but the last layer\n",
    "            # Add a non-linear function to help the network learn complex patterns\n",
    "            if i != layers - 1:\n",
    "                self.layers.append(torch.nn.ReLU())\n",
    "                print(\"#################################\")\n",
    "                print(\"## self.layers ## MLP\\_init_\\i ##\")\n",
    "                print(\"#################################\")\n",
    "                # print(\"Shape:\", self.layers.shape)\n",
    "                print(\"Length:\", len(self.layers))\n",
    "                print(\"Type:\", type(self.layers))\n",
    "                print(self.layers)\n",
    "                print(\"\")\n",
    "                \n",
    "        # Optionally add layer normalization to the output\n",
    "        # Optionally make the outputs more stable                \n",
    "        if layernorm:\n",
    "            self.layers.append(torch.nn.LayerNorm(output_size))\n",
    "            print(\"########################\")\n",
    "            print(\"## self.layers ## MLP ##\")\n",
    "            print(\"########################\")\n",
    "            # print(\"Shape:\", self.layers.shape)\n",
    "            print(\"Length:\", len(self.layers))\n",
    "            print(\"Type:\", type(self.layers))\n",
    "            print(self.layers)\n",
    "            print(\"\")\n",
    "            \n",
    "        # Initialize the weights and biases of the layers\n",
    "        # Set the starting values for the network's weights and biases            \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize each layer's parameters\n",
    "        # Set the initial values for weights and biases        \n",
    "        for layer in self.layers:\n",
    "            print(\"###################################\")\n",
    "            print(\"## layer ## MLP\\reset_parameters ##\")\n",
    "            print(\"###################################\")\n",
    "            # print(\"Shape:\", layer.shape)\n",
    "            # print(\"Length:\", len(layer))\n",
    "            print(\"Type:\", type(layer))\n",
    "            print(layer)\n",
    "            print(\"\")\n",
    "            \n",
    "            if isinstance(layer, torch.nn.Linear):\n",
    "                # Set the weights using a normal distribution\n",
    "                # Randomly set the weights around zero                \n",
    "                layer.weight.data.normal_(0, 1 / math.sqrt(layer.in_features))\n",
    "                print(\"###################################\")\n",
    "                print(\"## layer ## MLP\\reset_parameters ##\")\n",
    "                print(\"###################################\")\n",
    "                # print(\"Shape:\", layer.shape)\n",
    "                # print(\"Length:\", len(layer))\n",
    "                print(\"Type:\", type(layer))\n",
    "                print(layer)\n",
    "                print(\"\")\n",
    "                \n",
    "                # Set the biases to zero\n",
    "                # Initialize the biases with zeros                \n",
    "                layer.bias.data.fill_(0)\n",
    "                print(\"###################################\")\n",
    "                print(\"## layer ## MLP\\reset_parameters ##\")\n",
    "                print(\"###################################\")\n",
    "                # print(\"Shape:\", layer.shape)\n",
    "                # print(\"Length:\", len(layer))\n",
    "                print(\"Type:\", type(layer))\n",
    "                print(layer)\n",
    "                print(\"\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through each layer\n",
    "        # Process the input step by step through the network        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Return the final output\n",
    "        # Give back the result after processing        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_pkzDgqJ_ED"
   },
   "source": [
    "### GNN layers\n",
    "\n",
    "In following code block, we implement one type of GNN layer named `InteractionNetwork` (IN), which is proposed by paper *Interaction Networks for Learning about Objects,\n",
    "Relations and Physics*.\n",
    "\n",
    "• For a graph $G$, let feature of node $i$ be $v_i$, feature of edge $(i, j)$ be $e_{i, j}$. \n",
    "\n",
    "• three stages for IN to generate new features of nodes and edges.\n",
    "\n",
    "1. **Message generation.**\n",
    "\n",
    "• If there is an edge pointing from node $i$ to node $j$, node $i$ sends a message to node $j$. \n",
    "\n",
    "• message carries information of edge and its two nodes, so it is generated by following equation $\\mathrm{Msg}_{i,j} = \\mathrm{MLP}(v_i, v_j, e_{i,j})$.\n",
    "\n",
    "2. **Message aggregation.**\n",
    "\n",
    "• In this stage, each node of graph aggregates all messages it received to a fixed-sized representation. \n",
    "\n",
    "• In IN, aggregation means summing all messages up, i.e., $\\mathrm{Agg}_i=\\sum_{(j,i)\\in G}\\mathrm{Msg}_{i,j}$.\n",
    "\n",
    "3. **Update.**\n",
    "\n",
    "• update features of nodes and edges with results of previous stages. \n",
    "\n",
    "• For each edge, its new feature is sum of its old feature and correspond message, i.e., $e'_{i,j}=e_{i,j}+\\mathrm{Msg}_{i,j}$. \n",
    "\n",
    "• For each node, new feature is determined by its old feature and aggregated message, i.e., $v'_i=v_i+\\mathrm{MLP}(v_i, \\mathrm{Agg}_i)$.\n",
    "\n",
    "• In PyG, GNN layers are implemented as subclass of `MessagePassing`. \n",
    "\n",
    "• must override three critical functions to implement `InteractionNetwork` GNN layer. \n",
    "\n",
    "• Each function corresponds to one stage of GNN layer.\n",
    "\n",
    "1. `message()` -> message generation\n",
    "\n",
    "• This function controls how a message is generated on each edge of graph. \n",
    "\n",
    "• It takes three arguments:\n",
    "\n",
    "• (1) `x_i`, features of source nodes; \n",
    "\n",
    "• (2) `x_j`, features of target nodes; \n",
    "\n",
    "• (3) `edge_feature`, features of edges themselves. \n",
    "\n",
    "• In IN, concatenate all these features and generate messages with an MLP.\n",
    "\n",
    "1. `aggregate()` -> message aggregation\n",
    "\n",
    "• This function aggregates messages for nodes. \n",
    "\n",
    "• It depends on two arguments:\n",
    "\n",
    "• (1) `inputs`, messages; \n",
    "\n",
    "• (2) `index`, graph structure. \n",
    "\n",
    "• handle over task of message aggregation to function `torch_scatter.scatter` and specifies in argument `reduce` that want to sum messages up. \n",
    "\n",
    "• Because want to retain messages themselves to update edge features, return both messages and aggregated messages.\n",
    "\n",
    "1. `forward()` -> update\n",
    "\n",
    "• This function puts everything together. \n",
    "\n",
    "• `x` is node features, `edge_index` is graph structure and `edge_feature` is edge features. \n",
    "\n",
    "• function`MessagePassing.propagate` invokes functions `message` and `aggregate` for us. \n",
    "\n",
    "• Then, update node features and edge features and return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nobE0LcXJ6RR"
   },
   "outputs": [],
   "source": [
    "class InteractionNetwork(pyg.nn.MessagePassing):\n",
    "    \"\"\"Interaction Network as proposed in this paper:\n",
    "    https://proceedings.neurips.cc/paper/2016/hash/3147da8ab4a0437c15ef51a5cc7f2dc4-Abstract.html\"\"\"\n",
    "    def __init__(self, hidden_size, layers):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize an MLP for processing edge features\n",
    "        # Create a neural network to handle the connections between particles        \n",
    "        self.lin_edge = MLP(hidden_size * 3, hidden_size, hidden_size, layers)\n",
    "\n",
    "        # Initialize an MLP for processing node features\n",
    "        # Create a neural network to handle the properties of individual particles\n",
    "        self.lin_node = MLP(hidden_size * 2, hidden_size, hidden_size, layers)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_feature):\n",
    "        # Perform message passing and aggregate results\n",
    "        # Exchange information between particles and combine it        \n",
    "        edge_out, aggr = self.propagate(edge_index, x=(x, x), edge_feature=edge_feature)\n",
    "        print(\"############################################\")\n",
    "        print(\"## edge_out ## InteractionNetwork\\forward ##\")\n",
    "        print(\"############################################\")\n",
    "        print(\"Shape:\", edge_out.shape)\n",
    "        print(\"Length:\", len(edge_out))\n",
    "        print(\"Type:\", type(edge_out))\n",
    "        print(edge_out)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Apply the node MLP to the aggregated results\n",
    "        # Process the combined information using the node network        \n",
    "        node_out = self.lin_node(torch.cat((x, aggr), dim=-1))\n",
    "        print(\"############################################\")\n",
    "        print(\"## node_out ## InteractionNetwork\\forward ##\")\n",
    "        print(\"############################################\")\n",
    "        print(\"Shape:\", node_out.shape)\n",
    "        print(\"Length:\", len(node_out))\n",
    "        print(\"Type:\", type(node_out))\n",
    "        print(node_out)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update the edge features\n",
    "        # Adjust the connection information        \n",
    "        edge_out = edge_feature + edge_out\n",
    "        print(\"############################################\")\n",
    "        print(\"## edge_out ## InteractionNetwork\\forward ##\")\n",
    "        print(\"############################################\")\n",
    "        print(\"Shape:\", edge_out.shape)\n",
    "        print(\"Length:\", len(edge_out))\n",
    "        print(\"Type:\", type(edge_out))\n",
    "        print(edge_out)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update the node features\n",
    "        # Adjust the particle properties        \n",
    "        node_out = x + node_out\n",
    "        print(\"############################################\")\n",
    "        print(\"## node_out ## InteractionNetwork\\forward ##\")\n",
    "        print(\"############################################\")\n",
    "        print(\"Shape:\", node_out.shape)\n",
    "        print(\"Length:\", len(node_out))\n",
    "        print(\"Type:\", type(node_out))\n",
    "        print(node_out)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Return the updated node and edge features\n",
    "        # Give back the new particle and connection information        \n",
    "        return node_out, edge_out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_feature):\n",
    "        # Concatenate node features and edge features\n",
    "        # Combine the properties of connected particles and their connection        \n",
    "        x = torch.cat((x_i, x_j, edge_feature), dim=-1)\n",
    "        print(\"#####################################\")\n",
    "        print(\"## x ## InteractionNetwork\\message ##\")\n",
    "        print(\"#####################################\")\n",
    "        print(\"Shape:\", x.shape)\n",
    "        print(\"Length:\", len(x))\n",
    "        print(\"Type:\", type(x))\n",
    "        print(x)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Apply the edge MLP to the concatenated features\n",
    "        # Process the combined information using the edge network        \n",
    "        x = self.lin_edge(x)\n",
    "        print(\"#####################################\")\n",
    "        print(\"## x ## InteractionNetwork\\message ##\")\n",
    "        print(\"#####################################\")\n",
    "        print(\"Shape:\", x.shape)\n",
    "        print(\"Length:\", len(x))\n",
    "        print(\"Type:\", type(x))\n",
    "        print(x)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Return the processed features\n",
    "        # Give back the adjusted connection information        \n",
    "        return x\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size=None):\n",
    "        # Sum the inputs based on the index\n",
    "        # Combine the information from connected particles        \n",
    "        out = torch_scatter.scatter(inputs, index, dim=self.node_dim, dim_size=dim_size, reduce=\"sum\")\n",
    "        print(\"#########################################\")\n",
    "        print(\"## out ## InteractionNetwork\\aggregate ##\")\n",
    "        print(\"#########################################\")\n",
    "        print(\"Shape:\", out.shape)\n",
    "        print(\"Length:\", len(out))\n",
    "        print(\"Type:\", type(out))\n",
    "        print(out)\n",
    "        print(\"\")\n",
    "\n",
    "        # Return the inputs and the aggregated result\n",
    "        # Give back the combined information and the original inputs        \n",
    "        return (inputs, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-Aa9znXKH40"
   },
   "source": [
    "### GNN\n",
    "\n",
    "• Now its time to stack GNN layers to a GNN. \n",
    "\n",
    "• Besides GNN layers, pre-processing and post-processing blocks in GNN. \n",
    "\n",
    "• Before GNN layers, input features are transformed by MLP so expressiveness of GNN is improved without increasing GNN layers. \n",
    "\n",
    "• After GNN layers, final outputs (accelerations of particles in case) are extracted from features generated by GNN layers to meet requirement of task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZoB4_A6YJ7FP"
   },
   "outputs": [],
   "source": [
    "class LearnedSimulator(torch.nn.Module):\n",
    "    \"\"\"Graph Network-based Simulators(GNS)\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=128,\n",
    "        n_mp_layers=10, # number of GNN layers\n",
    "        num_particle_types=9,\n",
    "        particle_type_dim=16, # embedding dimension of particle types\n",
    "        dim=2, # dimension of world, typical 2D or 3D\n",
    "        window_size=5, # model looks into W frames before frame to be predicted\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Store the window size for future use\n",
    "        # Keep the size of the frame sequence        \n",
    "        self.window_size = window_size\n",
    "        print(\"############################################\")\n",
    "        print(\"## window_size ## LearnedSimulator\\_init_ ##\")\n",
    "        print(\"############################################\")\n",
    "        print(window_size)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Initialize embedding for particle types\n",
    "        # Create a lookup table for particle types        \n",
    "        self.embed_type = torch.nn.Embedding(num_particle_types, particle_type_dim)\n",
    "        print(\"#####################\")\n",
    "        print(\"## self.embed_type ## LearnedSimulator\\_init_ ##\")\n",
    "        print(\"#####################\")\n",
    "        # print(\"Shape:\", self.embed_type.shape)\n",
    "        # print(\"Length:\", len(self.embed_type))\n",
    "        print(\"Type:\", type(self.embed_type))\n",
    "        print(self.embed_type)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Define the input MLP for nodes\n",
    "        # Create a neural network to process particle features        \n",
    "        self.node_in = MLP(particle_type_dim + dim * (window_size + 2), hidden_size, hidden_size, 3)\n",
    "        print(\"#############################################\")\n",
    "        print(\"## self.node_in ## LearnedSimulator\\_init_ ##\")\n",
    "        print(\"#############################################\")\n",
    "        # print(\"Shape:\", self.node_in.shape)\n",
    "        # print(\"Length:\", len(self.node_in))\n",
    "        print(\"Type:\", type(self.node_in))\n",
    "        print(self.node_in)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Define the input MLP for edges\n",
    "        # Create a neural network to process connections between particles        \n",
    "        self.edge_in = MLP(dim + 1, hidden_size, hidden_size, 3)\n",
    "        print(\"#############################################\")\n",
    "        print(\"## self.edge_in ## LearnedSimulator\\_init_ ##\")\n",
    "        print(\"#############################################\")\n",
    "        # print(\"Shape:\", self.edge_in.shape)\n",
    "        # print(\"Length:\", len(self.edge_in))\n",
    "        print(\"Type:\", type(self.edge_in))\n",
    "        print(self.edge_in)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Define the output MLP for nodes\n",
    "        # Create a neural network to produce the final particle features        \n",
    "        self.node_out = MLP(hidden_size, hidden_size, dim, 3, layernorm=False)\n",
    "        print(\"##############################################\")\n",
    "        print(\"## self.node_out ## LearnedSimulator\\_init_ ##\")\n",
    "        print(\"##############################################\")\n",
    "        # print(\"Shape:\", self.node_out.shape)\n",
    "        # print(\"Length:\", len(self.node_out))\n",
    "        print(\"Type:\", type(self.node_out))\n",
    "        print(self.node_out)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Store the number of message-passing layers\n",
    "        # Remember how many layers of information exchange to use        \n",
    "        self.n_mp_layers = n_mp_layers\n",
    "        print(\"############################################\")\n",
    "        print(\"## n_mp_layers ## LearnedSimulator\\_init_ ##\")\n",
    "        print(\"############################################\")\n",
    "        # print(\"Shape:\", n_mp_layers.shape)\n",
    "        # print(\"Length:\", len(n_mp_layers))\n",
    "        print(\"Type:\", type(n_mp_layers))\n",
    "        print(n_mp_layers)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Initialize the message-passing layers\n",
    "        # Create multiple layers for particles to exchange information        \n",
    "        self.layers = torch.nn.ModuleList([InteractionNetwork(\n",
    "            hidden_size, 3\n",
    "        ) for _ in range(n_mp_layers)])\n",
    "\n",
    "        # Reset the parameters of the network\n",
    "        # Initialize the network weights and biases        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize the embedding weights using Xavier uniform distribution\n",
    "        # Set the initial values for the particle type embeddings        \n",
    "        torch.nn.init.xavier_uniform_(self.embed_type.weight)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # pre-processing\n",
    "        # node feature: combine categorial feature data.x and contiguous feature data.pos.\n",
    "\n",
    "        # Concatenate particle type embeddings and positions to form node features\n",
    "        # Combine the particle types and their positions into one feature set        \n",
    "        node_feature = torch.cat((self.embed_type(data.x), data.pos), dim=-1)\n",
    "        print(\"##############################################\")\n",
    "        print(\"## node_feature ## LearnedSimulator\\forward ##\")\n",
    "        print(\"##############################################\")\n",
    "        print(\"Shape:\", node_feature.shape)\n",
    "        print(\"Length:\", len(node_feature))\n",
    "        print(\"Type:\", type(node_feature))\n",
    "        print(node_feature)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Process node features through the input MLP\n",
    "        # Use the neural network to refine particle features        \n",
    "        node_feature = self.node_in(node_feature)\n",
    "        print(\"##############################################\")\n",
    "        print(\"## node_feature ## LearnedSimulator\\forward ##\")\n",
    "        print(\"##############################################\")\n",
    "        print(\"Shape:\", node_feature.shape)\n",
    "        print(\"Length:\", len(node_feature))\n",
    "        print(\"Type:\", type(node_feature))\n",
    "        print(node_feature)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Process edge features through the input MLP\n",
    "        # Use the neural network to refine connection features        \n",
    "        edge_feature = self.edge_in(data.edge_attr)\n",
    "        print(\"##############################################\")\n",
    "        print(\"## edge_feature ## LearnedSimulator\\forward ##\")\n",
    "        print(\"##############################################\")\n",
    "        print(\"Shape:\", edge_feature.shape)\n",
    "        print(\"Length:\", len(edge_feature))\n",
    "        print(\"Type:\", type(edge_feature))\n",
    "        print(edge_feature)\n",
    "        print(\"\")\n",
    "        \n",
    "        \n",
    "        # stack of GNN layers\n",
    "        # Multiple layers of particle information exchange        \n",
    "        for i in range(self.n_mp_layers):\n",
    "            # Apply each interaction network layer\n",
    "            # Let particles exchange information through each layer            \n",
    "            node_feature, edge_feature = self.layers[i](node_feature, data.edge_index, edge_feature=edge_feature)\n",
    "            print(\"################################################\")\n",
    "            print(\"## node_feature ## LearnedSimulator\\forward\\i ##\")\n",
    "            print(\"################################################\")\n",
    "            print(\"Shape:\", node_feature.shape)\n",
    "            print(\"Length:\", len(node_feature))\n",
    "            print(\"Type:\", type(node_feature))\n",
    "            print(node_feature)\n",
    "            print(\"\")\n",
    "            print(\"\")            \n",
    "            print(\"################################################\")\n",
    "            print(\"## edge_feature ## LearnedSimulator\\forward\\i ##\")\n",
    "            print(\"################################################\")\n",
    "            print(\"Shape:\", edge_feature.shape)\n",
    "            print(\"Length:\", len(edge_feature))\n",
    "            print(\"Type:\", type(edge_feature))\n",
    "            print(edge_feature)\n",
    "            print(\"\")\n",
    "            \n",
    "            \n",
    "        # post-processing\n",
    "\n",
    "        # Process the final node features through the output MLP\n",
    "        # Use the neural network to produce the final particle features        \n",
    "        out = self.node_out(node_feature)\n",
    "        \n",
    "        # Return the processed output\n",
    "        # Give back the final particle information        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7LUIouahvhW"
   },
   "source": [
    "## Training\n",
    "\n",
    "• Before start training model, let's configure hyperparameters! \n",
    "\n",
    "• Since accessible computaion power is limited in Colab, will only run 1 epoch of training, which takes about 1.5 hour. \n",
    "\n",
    "• won't produce as accurate results as shown in original paper in this Colab. \n",
    "\n",
    "• provide a checkpoint of training model on entire WaterDrop dataset for 5 epochs, which takes about 14 hours with a GeForce RTX 3080 Ti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Czpr3hJTiCuC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############\n",
      "## data_path ##\n",
      "###############\n",
      "/home/admin1/Desktop/gnndataset/datasets/WaterDrop\n",
      "\n",
      "################\n",
      "## model_path ##\n",
      "################\n",
      "temp/models/WaterDrop\n",
      "\n",
      "##################\n",
      "## rollout_path ##\n",
      "##################\n",
      "temp/rollouts/WaterDrop\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = OUTPUT_DIR\n",
    "print(\"###############\")\n",
    "print(\"## data_path ##\")\n",
    "print(\"###############\")\n",
    "print(data_path)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "model_path = os.path.join(\"temp\", \"models\", DATASET_NAME)\n",
    "print(\"################\")\n",
    "print(\"## model_path ##\")\n",
    "print(\"################\")\n",
    "print(model_path)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "rollout_path = os.path.join(\"temp\", \"rollouts\", DATASET_NAME)\n",
    "print(\"##################\")\n",
    "print(\"## rollout_path ##\")\n",
    "print(\"##################\")\n",
    "print(rollout_path)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "!mkdir -p \"$model_path\"\n",
    "!mkdir -p \"$rollout_path\"\n",
    "\n",
    "params = {\n",
    "    #\"epoch\": 1,\n",
    "    \"epoch\": 20,\n",
    "    \"batch_size\": 4,\n",
    "    \"lr\": 1e-4,\n",
    "    \"noise\": 3e-4,\n",
    "    \"save_interval\": 1000,\n",
    "    \"eval_interval\": 1000,\n",
    "    \"rollout_interval\": 200000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gN_P6D4tK7FQ"
   },
   "source": [
    "Below are some helper functions for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OSIAlxN7KvuZ"
   },
   "outputs": [],
   "source": [
    "def rollout(model, data, metadata, noise_std):\n",
    "    # Get the device the model is on (CPU or GPU)\n",
    "    # Determine if the model is running on a CPU or GPU    \n",
    "    device = next(model.parameters()).device\n",
    "    print(\"#######################\")\n",
    "    print(\"## device ## rollout ##\")\n",
    "    print(\"#######################\")\n",
    "    print(device)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    # Put the model in a mode where it won't learn but will make predictions    \n",
    "    model.eval()\n",
    "    \n",
    "    # Calculate the window size for the model\n",
    "    # Determine the number of frames the model will look at, including the current one\n",
    "     window_size = model.window_size + 1\n",
    "    print(\"############################\")\n",
    "    print(\"## window_size ## rollout ##\")\n",
    "    print(\"############################\")\n",
    "    print(window_size)\n",
    "    print(\"\")\n",
    "\n",
    "    # Get the total number of time steps in the data\n",
    "    # Find out how many time points are in the dataset    \n",
    "    total_time = data[\"position\"].size(0)\n",
    "    print(\"###########################\")\n",
    "    print(\"## total_time ## rollout ##\")\n",
    "    print(\"###########################\")\n",
    "    print(total_time)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Extract the initial sequence of positions for the given window size\n",
    "    # Get the starting positions of particles for the first few frames    \n",
    "    traj = data[\"position\"][:window_size]\n",
    "    print(\"#####################\")\n",
    "    print(\"## traj ## rollout ##\")\n",
    "    print(\"#####################\")\n",
    "    print(traj)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Rearrange the dimensions of the trajectory tensor\n",
    "    # Change the order of dimensions to have particles as the first dimension    \n",
    "    traj = traj.permute(1, 0, 2)\n",
    "    print(\"#####################\")\n",
    "    print(\"## traj ## rollout ##\")\n",
    "    print(\"#####################\")\n",
    "    print(traj)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Get the particle types from the data\n",
    "    # Identify the types of particles we are working with    \n",
    "    particle_type = data[\"particle_type\"]\n",
    "    print(\"##############################\")\n",
    "    print(\"## particle_type ## rollout ##\")\n",
    "    print(\"##############################\")\n",
    "    print(particle_type)\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "    # Loop over each time step after the initial window\n",
    "    # Process each frame of data one by one after the initial frames\n",
    "    for time in range(total_time - window_size):\n",
    "        with torch.no_grad():\n",
    "            # Preprocess the data and create a graph\n",
    "            # Prepare the particle data and connections for the model            \n",
    "            graph = preprocess(particle_type, traj[:, -window_size:], None, metadata, 0.0)\n",
    "            print(\"###########################\")\n",
    "            print(\"## graph ## rollout\\time ##\")\n",
    "            print(\"###########################\")\n",
    "            print(graph)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Move the graph to the device (CPU or GPU)\n",
    "            # Send the data to the same device where the model is located            \n",
    "            graph = graph.to(device)\n",
    "            print(\"###########################\")\n",
    "            print(\"## graph ## rollout\\time ##\")\n",
    "            print(\"###########################\")\n",
    "            print(graph)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Use the model to predict acceleration\n",
    "            # Get the predicted changes in velocity from the model            \n",
    "            acceleration = model(graph).cpu()\n",
    "            print(\"##################################\")\n",
    "            print(\"## acceleration ## rollout\\time ##\")\n",
    "            print(\"##################################\")\n",
    "            print(acceleration)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Scale the acceleration based on metadata and noise\n",
    "            # Adjust the predicted acceleration using known values and noise            \n",
    "            acceleration = acceleration * torch.sqrt(torch.tensor(metadata[\"acc_std\"]) ** 2 + noise_std ** 2) + torch.tensor(metadata[\"acc_mean\"])\n",
    "            print(\"##################################\")\n",
    "            print(\"## acceleration ## rollout\\time ##\")\n",
    "            print(\"##################################\")\n",
    "            print(acceleration)\n",
    "            print(\"\")\n",
    "                        \n",
    "            # Get the most recent positions of the particles\n",
    "            # Find out where the particles were in the last frame\n",
    "            recent_position = traj[:, -1]\n",
    "            print(\"#####################################\")\n",
    "            print(\"## recent_position ## rollout\\time ##\")\n",
    "            print(\"#####################################\")\n",
    "            print(recent_position)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Calculate the recent velocities of the particles\n",
    "            # Determine the speed and direction of the particles between the last two frames\n",
    "            recent_velocity = recent_position - traj[:, -2]\n",
    "            print(\"#####################################\")\n",
    "            print(\"## recent_velocity ## rollout\\time ##\")\n",
    "            print(\"#####################################\")\n",
    "            print(recent_velocity)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Calculate the new velocities by adding the acceleration\n",
    "            # Update the speed and direction using the predicted acceleration            \n",
    "            new_velocity = recent_velocity + acceleration\n",
    "            print(\"##################################\")\n",
    "            print(\"## new_velocity ## rollout\\time ##\")\n",
    "            print(\"##################################\")\n",
    "            print(new_velocity)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Calculate the new positions by adding the new velocities\n",
    "            # Find out where the particles will be in the next frame            \n",
    "            new_position = recent_position + new_velocity\n",
    "            print(\"##################################\")\n",
    "            print(\"## new_position ## rollout\\time ##\")\n",
    "            print(\"##################################\")\n",
    "            print(new_position)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Append the new positions to the trajectory\n",
    "            # Add the new particle positions to the sequence of frames            \n",
    "            traj = torch.cat((traj, new_position.unsqueeze(1)), dim=1)\n",
    "            print(\"##########################\")\n",
    "            print(\"## traj ## rollout\\time ##\")\n",
    "            print(\"##########################\")\n",
    "            print(traj)\n",
    "            print(\"\")\n",
    "            \n",
    "    # Return the complete trajectory\n",
    "    # Give back the full sequence of particle positions over time\n",
    "    return traj\n",
    "\n",
    "\n",
    "def oneStepMSE(simulator, dataloader, metadata, noise):\n",
    "    \"\"\"Returns two values, loss and MSE\"\"\"\n",
    "\n",
    "    # Initialize the total loss\n",
    "    # Start the total loss count at zero    \n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Initialize the total mean squared error (MSE)\n",
    "    # Start the total MSE count at zero    \n",
    "    total_mse = 0.0\n",
    "\n",
    "    # Initialize the batch counter\n",
    "    # Start the count of batches at zero    \n",
    "    batch_count = 0\n",
    "\n",
    "    # Set the simulator to evaluation mode\n",
    "    # Put the simulator in a mode where it makes predictions without learning    \n",
    "    simulator.eval()\n",
    "\n",
    "    # Do not compute gradients\n",
    "    # Turn off the calculations needed for learning    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Calculate the scale for normalization using metadata and noise\n",
    "        # Compute a scaling factor to adjust the predictions based on known values and noise\n",
    "        scale = torch.sqrt(torch.tensor(metadata[\"acc_std\"]) ** 2 + noise ** 2).cuda()\n",
    "\n",
    "        # Loop over each batch of data in the dataloader\n",
    "        # Process each group of data one by one        \n",
    "        for data in valid_loader:\n",
    "            # Move data to the GPU\n",
    "            # Transfer the data to the GPU for faster processing            \n",
    "            data = data.cuda()\n",
    "            print(\"#############################\")\n",
    "            print(\"## data ## oneStepMSE\\data ##\")\n",
    "            print(\"#############################\")\n",
    "            print(data)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Get the predictions from the simulator\n",
    "            # Use the simulator to predict the next positions of particles            \n",
    "            pred = simulator(data)\n",
    "            print(\"#############################\")\n",
    "            print(\"## pred ## oneStepMSE\\data ##\")\n",
    "            print(\"#############################\")\n",
    "            print(pred)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Calculate the mean squared error (MSE) scaled by the normalization factor\n",
    "            # Measure the average squared difference between the predicted and actual positions\n",
    "            mse = ((pred - data.y) * scale) ** 2\n",
    "            print(\"############################\")\n",
    "            print(\"## mse ## oneStepMSE\\data ##\")\n",
    "            print(\"############################\")\n",
    "            print(mse)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Sum the MSE across dimensions and compute the mean\n",
    "            # Add up the squared differences and calculate the average            \n",
    "            mse = mse.sum(dim=-1).mean()\n",
    "            print(\"############################\")\n",
    "            print(\"## mse ## oneStepMSE\\data ##\")\n",
    "            print(\"############################\")\n",
    "            print(mse)\n",
    "            print(\"\")\n",
    "            \n",
    "           # Calculate the loss as the mean of squared differences without scaling\n",
    "            # Measure the average squared difference directly between the predicted and actual positions\n",
    "            loss = ((pred - data.y) ** 2).mean()\n",
    "            print(\"#############################\")\n",
    "            print(\"## loss ## oneStepMSE\\data ##\")\n",
    "            print(\"#############################\")\n",
    "            print(loss)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Accumulate the total MSE\n",
    "            # Add the current MSE to the total MSE count            \n",
    "            total_mse += mse.item()\n",
    "            print(\"##################################\")\n",
    "            print(\"## total_mse ## oneStepMSE\\data ##\")\n",
    "            print(\"##################################\")\n",
    "            print(total_mse)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Accumulate the total loss\n",
    "            # Add the current loss to the total loss count            \n",
    "            total_loss += loss.item()\n",
    "            print(\"###################################\")\n",
    "            print(\"## total_loss ## oneStepMSE\\data ##\")\n",
    "            print(\"###################################\")\n",
    "            print(total_loss)\n",
    "            print(\"\")\n",
    "           \n",
    "            # Increment the batch counter\n",
    "            # Increase the count of processed batches by one            \n",
    "            batch_count += 1\n",
    "            print(\"####################################\")\n",
    "            print(\"## batch_count ## oneStepMSE\\data ##\")\n",
    "            print(\"####################################\")\n",
    "            print(batch_count)\n",
    "            print(\"\")\n",
    "            \n",
    "    # Return the average loss and MSE\n",
    "    # Give back the mean loss and MSE across all batches            \n",
    "    return total_loss / batch_count, total_mse / batch_count\n",
    "\n",
    "\n",
    "def rolloutMSE(simulator, dataset, noise):\n",
    "    # Initialize the total loss\n",
    "    # Start the total loss count at zero    \n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Initialize the batch counter\n",
    "    # Start the count of batches at zero    \n",
    "    batch_count = 0\n",
    "\n",
    "    # Set the simulator to evaluation mode\n",
    "    # Put the simulator in a mode where it makes predictions without learning    \n",
    "    simulator.eval()\n",
    "\n",
    "    # Do not compute gradients\n",
    "    # Turn off the calculations needed for learning    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Loop over each rollout data in the dataset\n",
    "        # Process each sequence of data one by one        \n",
    "        for rollout_data in dataset:\n",
    "            print(\"#############################################\")\n",
    "            print(\"## rollout_data ## rolloutMSE\\rollout_data ##\")\n",
    "            print(\"#############################################\")\n",
    "            print(\"Shape:\", rollout_data.shape)\n",
    "            print(\"Length:\", len(rollout_data))\n",
    "            print(\"Type:\", type(rollout_data))\n",
    "            print(rollout_data)\n",
    "            print(\"\")\n",
    "\n",
    "            # Get the predicted positions by running the rollout function\n",
    "            # Use the simulator to predict the positions over multiple time steps\n",
    "            rollout_out = rollout(simulator, rollout_data, dataset.metadata, noise)\n",
    "            print(\"############################################\")\n",
    "            print(\"## rollout_out ## rolloutMSE\\rollout_data ##\")\n",
    "            print(\"############################################\")\n",
    "            print(\"Shape:\", rollout_out.shape)\n",
    "            print(\"Length:\", len(rollout_out))\n",
    "            print(\"Type:\", type(rollout_out))\n",
    "            print(rollout_out)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Rearrange the dimensions of the output tensor\n",
    "            # Change the order of dimensions to match the expected format            \n",
    "            rollout_out = rollout_out.permute(1, 0, 2)\n",
    "            print(\"############################################\")\n",
    "            print(\"## rollout_out ## rolloutMSE\\rollout_data ##\")\n",
    "            print(\"############################################\")\n",
    "            print(\"Shape:\", rollout_out.shape)\n",
    "            print(\"Length:\", len(rollout_out))\n",
    "            print(\"Type:\", type(rollout_out))\n",
    "            print(rollout_out)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Calculate the loss as the mean squared error between the predicted and actual positions\n",
    "            # Measure the average squared difference between the predicted positions and the actual positions\n",
    "            loss = (rollout_out - rollout_data[\"position\"]) ** 2\n",
    "            print(\"#####################################\")\n",
    "            print(\"## loss ## rolloutMSE\\rollout_data ##\")\n",
    "            print(\"#####################################\")\n",
    "            print(loss)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Sum the loss across dimensions and compute the mean\n",
    "            # Add up the squared differences and calculate the average            \n",
    "            loss = loss.sum(dim=-1).mean()\n",
    "            print(\"#####################################\")\n",
    "            print(\"## loss ## rolloutMSE\\rollout_data ##\")\n",
    "            print(\"#####################################\")\n",
    "            print(loss)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Accumulate the total loss\n",
    "            # Add the current loss to the total loss count            \n",
    "            total_loss += loss.item()\n",
    "            print(\"###########################################\")\n",
    "            print(\"## total_loss ## rolloutMSE\\rollout_data ##\")\n",
    "            print(\"###########################################\")\n",
    "            print(total_loss)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Increment the batch counter\n",
    "            # Increase the count of processed batches by one            \n",
    "            batch_count += 1\n",
    "            print(\"############################################\")\n",
    "            print(\"## batch_count ## rolloutMSE\\rollout_data ##\")\n",
    "            print(\"############################################\")\n",
    "            print(batch_count)\n",
    "            print(\"\")\n",
    "            \n",
    "    # Return the average loss\n",
    "    # Give back the mean loss across all batches            \n",
    "    return total_loss / batch_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwvseROOFqt0"
   },
   "source": [
    "Here is main training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "oRsKEIX6XAwN"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(params, simulator, train_loader, valid_loader, valid_rollout_dataset):\n",
    "    # Define the loss function\n",
    "    # Set up a way to measure the difference between predicted and actual values\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    print(\"######################\")\n",
    "    print(\"## loss_fn ## train ##\")\n",
    "    print(\"######################\")\n",
    "    # print(\"Shape:\", loss_fn.shape)\n",
    "    # print(\"Length:\", len(loss_fn))\n",
    "    print(\"Type:\", type(loss_fn))\n",
    "    print(loss_fn)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Define the optimizer\n",
    "    # Set up a method to adjust the simulator's parameters to minimize the loss\n",
    "    optimizer = torch.optim.Adam(simulator.parameters(), lr=params[\"lr\"])\n",
    "    print(\"########################\")\n",
    "    print(\"## optimizer ## train ##\")\n",
    "    print(\"########################\")\n",
    "    # print(\"Shape:\", optimizer.shape)\n",
    "    # print(\"Length:\", len(optimizer))\n",
    "    print(\"Type:\", type(optimizer))\n",
    "    print(optimizer)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Define the optimizer\n",
    "    # Set up a method to adjust the simulator's parameters to minimize the loss\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1 ** (1 / 5e6))\n",
    "    print(\"########################\")\n",
    "    print(\"## scheduler ## train ##\")\n",
    "    print(\"########################\")\n",
    "    # print(\"Shape:\", scheduler.shape)\n",
    "    # print(\"Length:\", len(scheduler))\n",
    "    print(\"Type:\", type(scheduler))\n",
    "    print(scheduler)\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # recording loss curve\n",
    "    # Initialize lists to record the loss curves\n",
    "    # Prepare to track how the loss changes during training    \n",
    "    train_loss_list = []\n",
    "    eval_loss_list = []\n",
    "    onestep_mse_list = []\n",
    "    rollout_mse_list = []\n",
    "\n",
    "    # Initialize the total step counter\n",
    "    # Start counting the total number of steps taken during training    \n",
    "    total_step = 0\n",
    "\n",
    "    # Loop over the number of epochs specified in params\n",
    "    # Repeat the training process for a set number of times    \n",
    "    for i in range(params[\"epoch\"]):\n",
    "\n",
    "        # Set the simulator to training mode\n",
    "        # Put the simulator in a mode where it can learn from the data        \n",
    "        simulator.train()\n",
    "        \n",
    "        # Create a progress bar for the current epoch\n",
    "        # Display the progress of the current training epoch        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {i}\")\n",
    "        print(\"#############################\")\n",
    "        print(\"## progress_bar ## train\\i ##\")\n",
    "        print(\"#############################\")\n",
    "        print(progress_bar)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Initialize the total loss for the current epoch\n",
    "        # Start counting the total loss for the current round of training        \n",
    "        total_loss = 0\n",
    "        print(\"###########################\")\n",
    "        print(\"## total_loss ## train\\i ##\")\n",
    "        print(\"###########################\")\n",
    "        print(total_loss)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Initialize the batch counter\n",
    "        # Start counting the number of batches processed in the current epoch        \n",
    "        batch_count = 0\n",
    "        print(\"############################\")\n",
    "        print(\"## batch_count ## train\\i ##\")\n",
    "        print(\"############################\")\n",
    "        print(batch_count)\n",
    "        print(\"\")\n",
    "        \n",
    "        \n",
    "\n",
    "        # Loop over each batch of data in the training loader\n",
    "        # Process each group of data one by one        \n",
    "        for data in progress_bar:\n",
    "\n",
    "            # Reset the gradients\n",
    "            # Clear the gradients from the previous step            \n",
    "            optimizer.zero_grad()\n",
    "            print(\"###############################\")\n",
    "            print(\"## optimizer ## train\\i\\data ##\")\n",
    "            print(\"###############################\")\n",
    "            # print(\"Shape:\", optimizer.shape)\n",
    "            # print(\"Length:\", len(optimizer))\n",
    "            print(\"Type:\", type(optimizer))\n",
    "            print(optimizer)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Move data to the GPU\n",
    "            # Transfer the data to the GPU for faster processing            \n",
    "            data = data.cuda()\n",
    "            print(\"##########################\")\n",
    "            print(\"## data ## train\\i\\data ##\")\n",
    "            print(\"##########################\")\n",
    "            # print(\"Shape:\", data.shape)\n",
    "            print(\"Length:\", len(data))\n",
    "            print(\"Type:\", type(data))\n",
    "            print(data)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Get the predictions from the simulator\n",
    "            # Use the simulator to predict the next positions of particles            \n",
    "            pred = simulator(data)\n",
    "            print(\"##########################\")\n",
    "            print(\"## pred ## train\\i\\data ##\")\n",
    "            print(\"##########################\")\n",
    "            print(\"Shape:\", pred.shape)\n",
    "            print(\"Length:\", len(pred))\n",
    "            print(\"Type:\", type(pred))\n",
    "            print(pred)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Calculate the loss\n",
    "            # Measure the difference between the predicted and actual positions            \n",
    "            loss = loss_fn(pred, data.y)\n",
    "            print(\"##########\")\n",
    "            print(\"## loss ## train\\i\\data ##\")\n",
    "            print(\"##########\")\n",
    "            print(\"Shape:\", loss.shape)\n",
    "            # print(\"Length:\", len(loss))\n",
    "            print(\"Type:\", type(loss))\n",
    "            print(loss)\n",
    "            print(\"\")\n",
    "            \n",
    "            # Backpropagate the loss\n",
    "            # Compute the gradients to adjust the simulator's parameters            \n",
    "            loss.backward()\n",
    "\n",
    "            # Update the simulator's parameters\n",
    "            # Adjust the simulator's parameters to minimize the loss\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate\n",
    "            # Adjust the learning rate according to the scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            # Accumulate the total loss for the current epoch\n",
    "            # Add the current loss to the total loss count\n",
    "            total_loss += loss.item()\n",
    "            print(\"################\")\n",
    "            print(\"## total_loss ## train\\i\\data ##\")\n",
    "            print(\"################\")\n",
    "            print(total_loss)\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "            # Increment the batch counter\n",
    "            # Increase the count of processed batches by one\n",
    "            batch_count += 1\n",
    "            print(\"#################################\")\n",
    "            print(\"## batch_count ## train\\i\\data ##\")\n",
    "            print(\"#################################\")\n",
    "            print(batch_count)\n",
    "            print(\"\")\n",
    "\n",
    "            # Update the progress bar with the current loss and average loss\n",
    "            # Display the current and average loss on the progress bar\n",
    "            progress_bar.set_postfix({\"loss\": loss.item(), \"avg_loss\": total_loss / batch_count, \"lr\": optimizer.param_groups[0][\"lr\"]})\n",
    "\n",
    "            # Increment the total step counter\n",
    "            # Increase the count of total steps taken by one\n",
    "            total_step += 1\n",
    "            print(\"################################\")\n",
    "            print(\"## total_step ## train\\i\\data ##\")\n",
    "            print(\"################################\")\n",
    "            print(total_step)\n",
    "            print(\"\")\n",
    "\n",
    "            # Record the training loss\n",
    "            # Save the current loss value for later analysis\n",
    "            train_loss_list.append((total_step, loss.item()))\n",
    "            print(\"#####################################\")\n",
    "            print(\"## train_loss_list ## train\\i\\data ##\")\n",
    "            print(\"#####################################\")\n",
    "            # print(\"Shape:\", train_loss_list.shape)\n",
    "            print(\"Length:\", len(train_loss_list))\n",
    "            print(\"Type:\", type(train_loss_list))\n",
    "            print(train_loss_list)\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "            # evaluation\n",
    "            # Evaluate the model periodically\n",
    "            # Check the model's performance on the validation set at regular intervals\n",
    "            if total_step % params[\"eval_interval\"] == 0:\n",
    "                # Set the simulator to evaluation mode\n",
    "                # Put the simulator in a mode where it makes predictions without learning\n",
    "                simulator.eval()\n",
    "\n",
    "                # Calculate the evaluation loss and one-step MSE\n",
    "                # Measure the simulator's performance on the validation set                \n",
    "                eval_loss, onestep_mse = oneStepMSE(simulator, valid_loader, valid_dataset.metadata, params[\"noise\"])\n",
    "                print(\"###############################\")\n",
    "                print(\"## eval_loss ## train\\i\\data ##\")\n",
    "                print(\"###############################\")\n",
    "                print(\"Shape:\", eval_loss.shape)\n",
    "                print(\"Length:\", len(eval_loss))\n",
    "                print(\"Type:\", type(eval_loss))\n",
    "                print(eval_loss)\n",
    "                print(\"\")\n",
    "\n",
    "                # Record the evaluation loss and one-step MSE\n",
    "                # Save the evaluation results for later analysis\n",
    "                eval_loss_list.append((total_step, eval_loss))\n",
    "                print(\"####################################\")\n",
    "                print(\"## eval_loss_list ## train\\i\\data ##\")\n",
    "                print(\"####################################\")\n",
    "                print(\"Shape:\", eval_loss_list.shape)\n",
    "                print(\"Length:\", len(eval_loss_list))\n",
    "                print(\"Type:\", type(eval_loss_list))\n",
    "                print(eval_loss_list)\n",
    "                print(\"\")\n",
    "\n",
    "\n",
    "                onestep_mse_list.append((total_step, onestep_mse))\n",
    "                print(\"######################################\")\n",
    "                print(\"## onestep_mse_list ## train\\i\\data ##\")\n",
    "                print(\"######################################\")\n",
    "                print(\"Shape:\", onestep_mse_list.shape)\n",
    "                print(\"Length:\", len(onestep_mse_list))\n",
    "                print(\"Type:\", type(onestep_mse_list))\n",
    "                print(onestep_mse_list)\n",
    "                print(\"\")\n",
    "\n",
    "\n",
    "                # Print the evaluation results\n",
    "                # Display the evaluation loss and one-step MSE\n",
    "                tqdm.write(f\"\\nEval: Loss: {eval_loss}, One Step MSE: {onestep_mse}\")\n",
    "\n",
    "                # Set the simulator back to training mode\n",
    "                # Put the simulator back in a mode where it can learn from the data\n",
    "                simulator.train()\n",
    "\n",
    "            # do rollout on valid set\n",
    "            # Perform rollout evaluation periodically\n",
    "            # Check the model's performance on multi-step predictions at regular intervals\n",
    "            if total_step % params[\"rollout_interval\"] == 0:\n",
    "                # Set the simulator to evaluation mode\n",
    "                # Put the simulator in a mode where it makes predictions without learning\n",
    "                simulator.eval()\n",
    "\n",
    "                # Calculate the rollout MSE\n",
    "                # Measure the simulator's performance on multi-step predictions\n",
    "                rollout_mse = rolloutMSE(simulator, valid_rollout_dataset, params[\"noise\"])\n",
    "                print(\"#################################\")\n",
    "                print(\"## rollout_mse ## train\\i\\data ##\")\n",
    "                print(\"#################################\")\n",
    "                print(rollout_mse)\n",
    "                print(\"\")\n",
    "\n",
    "                # Record the rollout MSE\n",
    "                # Save the rollout MSE for later analysis\n",
    "                rollout_mse_list.append((total_step, rollout_mse))\n",
    "                print(\"######################################\")\n",
    "                print(\"## rollout_mse_list ## train\\i\\data ##\")\n",
    "                print(\"######################################\")\n",
    "                print(rollout_mse_list)\n",
    "                print(\"\")\n",
    "\n",
    "                # Print the rollout MSE\n",
    "                # Display the rollout MSE\n",
    "                tqdm.write(f\"\\nEval: Rollout MSE: {rollout_mse}\")\n",
    "\n",
    "                # Set the simulator back to training mode\n",
    "                # Put the simulator back in a mode where it can learn from the data\n",
    "                simulator.train()\n",
    "\n",
    "            # save model\n",
    "            # Save the model periodically\n",
    "            # Save the current state of the model at regular intervals            \n",
    "            if total_step % params[\"save_interval\"] == 0:\n",
    "                print(\"################################\")\n",
    "                print(\"## total_step ## train\\i\\data ##\")\n",
    "                print(\"################################\")\n",
    "                print(total_step)\n",
    "                print(\"\")\n",
    "\n",
    "                print(\"###########################################\")\n",
    "                print(\"## params[save_interval] ## train\\i\\data ##\")\n",
    "                print(\"###########################################\")\n",
    "                print(params[\"save_interval\"])\n",
    "                print(\"\")\n",
    "                \n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"model\": simulator.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"scheduler\": scheduler.state_dict(),\n",
    "                    },\n",
    "                    os.path.join(model_path, f\"checkpoint_{total_step}.pt\")\n",
    "                )\n",
    "\n",
    "    # Return the recorded loss curves\n",
    "    # Give back the recorded training and evaluation loss values\n",
    "    return train_loss_list, eval_loss_list, onestep_mse_list, rollout_mse_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZ-U-nlEakHF"
   },
   "source": [
    "• let's load dataset and train model! \n",
    "\n",
    "• It takes roughly 1.5 hour to run this block on Colab with default parameters. \n",
    "\n",
    "• **If you are impatient, highly recommend you to skip next 2 blocks and load checkpoint provided to save some time;**\n",
    "\n",
    "• **otherwise, make a cup of tea/coffee and come back later to see results of training!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1HWNWqbE6db",
    "outputId": "fa143d3b-f28c-4484-d9cb-70c2f9b8ae63",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################\n",
      "## data_path ## OneStepDataset ##\n",
      "#################################\n",
      "/home/admin1/Desktop/gnndataset/datasets/WaterDrop\n",
      "\n",
      "#####################################\n",
      "## self.metadata ## OneStepDataset ##\n",
      "#####################################\n",
      "Length: 9\n",
      "Type: <class 'dict'>\n",
      "{'bounds': [[0.1, 0.9], [0.1, 0.9]], 'sequence_length': 1000, 'default_connectivity_radius': 0.015, 'dim': 2, 'dt': 0.0025, 'vel_mean': [-3.964619574176163e-05, -0.00026272129664401046], 'vel_std': [0.0013722809722366911, 0.0013119977252142715], 'acc_mean': [2.602686518497945e-08, 1.0721623948191945e-07], 'acc_std': [6.742962470925277e-05, 8.700719180424815e-05]}\n",
      "\n",
      "###################################\n",
      "## self.offset ## OneStepDataset ## 1\n",
      "###################################\n",
      "Length: 100\n",
      "Type: <class 'dict'>\n",
      "{'0': {'particle_type': {'offset': 0, 'shape': [678]}, 'position': {'offset': 0, 'shape': [1001, 678, 2]}}, '1': {'particle_type': {'offset': 678, 'shape': [355]}, 'position': {'offset': 1357356, 'shape': [1001, 355, 2]}}, '2': {'particle_type': {'offset': 1033, 'shape': [461]}, 'position': {'offset': 2068066, 'shape': [1001, 461, 2]}}, '3': {'particle_type': {'offset': 1494, 'shape': [307]}, 'position': {'offset': 2990988, 'shape': [1001, 307, 2]}}, '4': {'particle_type': {'offset': 1801, 'shape': [300]}, 'position': {'offset': 3605602, 'shape': [1001, 300, 2]}}, '5': {'particle_type': {'offset': 2101, 'shape': [398]}, 'position': {'offset': 4206202, 'shape': [1001, 398, 2]}}, '6': {'particle_type': {'offset': 2499, 'shape': [362]}, 'position': {'offset': 5002998, 'shape': [1001, 362, 2]}}, '7': {'particle_type': {'offset': 2861, 'shape': [317]}, 'position': {'offset': 5727722, 'shape': [1001, 317, 2]}}, '8': {'particle_type': {'offset': 3178, 'shape': [451]}, 'position': {'offset': 6362356, 'shape': [1001, 451, 2]}}, '9': {'particle_type': {'offset': 3629, 'shape': [285]}, 'position': {'offset': 7265258, 'shape': [1001, 285, 2]}}, '10': {'particle_type': {'offset': 3914, 'shape': [271]}, 'position': {'offset': 7835828, 'shape': [1001, 271, 2]}}, '11': {'particle_type': {'offset': 4185, 'shape': [401]}, 'position': {'offset': 8378370, 'shape': [1001, 401, 2]}}, '12': {'particle_type': {'offset': 4586, 'shape': [1052]}, 'position': {'offset': 9181172, 'shape': [1001, 1052, 2]}}, '13': {'particle_type': {'offset': 5638, 'shape': [613]}, 'position': {'offset': 11287276, 'shape': [1001, 613, 2]}}, '14': {'particle_type': {'offset': 6251, 'shape': [682]}, 'position': {'offset': 12514502, 'shape': [1001, 682, 2]}}, '15': {'particle_type': {'offset': 6933, 'shape': [389]}, 'position': {'offset': 13879866, 'shape': [1001, 389, 2]}}, '16': {'particle_type': {'offset': 7322, 'shape': [403]}, 'position': {'offset': 14658644, 'shape': [1001, 403, 2]}}, '17': {'particle_type': {'offset': 7725, 'shape': [353]}, 'position': {'offset': 15465450, 'shape': [1001, 353, 2]}}, '18': {'particle_type': {'offset': 8078, 'shape': [445]}, 'position': {'offset': 16172156, 'shape': [1001, 445, 2]}}, '19': {'particle_type': {'offset': 8523, 'shape': [298]}, 'position': {'offset': 17063046, 'shape': [1001, 298, 2]}}, '20': {'particle_type': {'offset': 8821, 'shape': [519]}, 'position': {'offset': 17659642, 'shape': [1001, 519, 2]}}, '21': {'particle_type': {'offset': 9340, 'shape': [630]}, 'position': {'offset': 18698680, 'shape': [1001, 630, 2]}}, '22': {'particle_type': {'offset': 9970, 'shape': [272]}, 'position': {'offset': 19959940, 'shape': [1001, 272, 2]}}, '23': {'particle_type': {'offset': 10242, 'shape': [670]}, 'position': {'offset': 20504484, 'shape': [1001, 670, 2]}}, '24': {'particle_type': {'offset': 10912, 'shape': [560]}, 'position': {'offset': 21845824, 'shape': [1001, 560, 2]}}, '25': {'particle_type': {'offset': 11472, 'shape': [520]}, 'position': {'offset': 22966944, 'shape': [1001, 520, 2]}}, '26': {'particle_type': {'offset': 11992, 'shape': [586]}, 'position': {'offset': 24007984, 'shape': [1001, 586, 2]}}, '27': {'particle_type': {'offset': 12578, 'shape': [445]}, 'position': {'offset': 25181156, 'shape': [1001, 445, 2]}}, '28': {'particle_type': {'offset': 13023, 'shape': [421]}, 'position': {'offset': 26072046, 'shape': [1001, 421, 2]}}, '29': {'particle_type': {'offset': 13444, 'shape': [456]}, 'position': {'offset': 26914888, 'shape': [1001, 456, 2]}}, '30': {'particle_type': {'offset': 13900, 'shape': [504]}, 'position': {'offset': 27827800, 'shape': [1001, 504, 2]}}, '31': {'particle_type': {'offset': 14404, 'shape': [505]}, 'position': {'offset': 28836808, 'shape': [1001, 505, 2]}}, '32': {'particle_type': {'offset': 14909, 'shape': [658]}, 'position': {'offset': 29847818, 'shape': [1001, 658, 2]}}, '33': {'particle_type': {'offset': 15567, 'shape': [480]}, 'position': {'offset': 31165134, 'shape': [1001, 480, 2]}}, '34': {'particle_type': {'offset': 16047, 'shape': [438]}, 'position': {'offset': 32126094, 'shape': [1001, 438, 2]}}, '35': {'particle_type': {'offset': 16485, 'shape': [593]}, 'position': {'offset': 33002970, 'shape': [1001, 593, 2]}}, '36': {'particle_type': {'offset': 17078, 'shape': [520]}, 'position': {'offset': 34190156, 'shape': [1001, 520, 2]}}, '37': {'particle_type': {'offset': 17598, 'shape': [518]}, 'position': {'offset': 35231196, 'shape': [1001, 518, 2]}}, '38': {'particle_type': {'offset': 18116, 'shape': [787]}, 'position': {'offset': 36268232, 'shape': [1001, 787, 2]}}, '39': {'particle_type': {'offset': 18903, 'shape': [378]}, 'position': {'offset': 37843806, 'shape': [1001, 378, 2]}}, '40': {'particle_type': {'offset': 19281, 'shape': [598]}, 'position': {'offset': 38600562, 'shape': [1001, 598, 2]}}, '41': {'particle_type': {'offset': 19879, 'shape': [513]}, 'position': {'offset': 39797758, 'shape': [1001, 513, 2]}}, '42': {'particle_type': {'offset': 20392, 'shape': [644]}, 'position': {'offset': 40824784, 'shape': [1001, 644, 2]}}, '43': {'particle_type': {'offset': 21036, 'shape': [478]}, 'position': {'offset': 42114072, 'shape': [1001, 478, 2]}}, '44': {'particle_type': {'offset': 21514, 'shape': [418]}, 'position': {'offset': 43071028, 'shape': [1001, 418, 2]}}, '45': {'particle_type': {'offset': 21932, 'shape': [473]}, 'position': {'offset': 43907864, 'shape': [1001, 473, 2]}}, '46': {'particle_type': {'offset': 22405, 'shape': [694]}, 'position': {'offset': 44854810, 'shape': [1001, 694, 2]}}, '47': {'particle_type': {'offset': 23099, 'shape': [702]}, 'position': {'offset': 46244198, 'shape': [1001, 702, 2]}}, '48': {'particle_type': {'offset': 23801, 'shape': [869]}, 'position': {'offset': 47649602, 'shape': [1001, 869, 2]}}, '49': {'particle_type': {'offset': 24670, 'shape': [659]}, 'position': {'offset': 49389340, 'shape': [1001, 659, 2]}}, '50': {'particle_type': {'offset': 25329, 'shape': [713]}, 'position': {'offset': 50708658, 'shape': [1001, 713, 2]}}, '51': {'particle_type': {'offset': 26042, 'shape': [682]}, 'position': {'offset': 52136084, 'shape': [1001, 682, 2]}}, '52': {'particle_type': {'offset': 26724, 'shape': [484]}, 'position': {'offset': 53501448, 'shape': [1001, 484, 2]}}, '53': {'particle_type': {'offset': 27208, 'shape': [589]}, 'position': {'offset': 54470416, 'shape': [1001, 589, 2]}}, '54': {'particle_type': {'offset': 27797, 'shape': [1058]}, 'position': {'offset': 55649594, 'shape': [1001, 1058, 2]}}, '55': {'particle_type': {'offset': 28855, 'shape': [362]}, 'position': {'offset': 57767710, 'shape': [1001, 362, 2]}}, '56': {'particle_type': {'offset': 29217, 'shape': [419]}, 'position': {'offset': 58492434, 'shape': [1001, 419, 2]}}, '57': {'particle_type': {'offset': 29636, 'shape': [523]}, 'position': {'offset': 59331272, 'shape': [1001, 523, 2]}}, '58': {'particle_type': {'offset': 30159, 'shape': [480]}, 'position': {'offset': 60378318, 'shape': [1001, 480, 2]}}, '59': {'particle_type': {'offset': 30639, 'shape': [534]}, 'position': {'offset': 61339278, 'shape': [1001, 534, 2]}}, '60': {'particle_type': {'offset': 31173, 'shape': [359]}, 'position': {'offset': 62408346, 'shape': [1001, 359, 2]}}, '61': {'particle_type': {'offset': 31532, 'shape': [198]}, 'position': {'offset': 63127064, 'shape': [1001, 198, 2]}}, '62': {'particle_type': {'offset': 31730, 'shape': [549]}, 'position': {'offset': 63523460, 'shape': [1001, 549, 2]}}, '63': {'particle_type': {'offset': 32279, 'shape': [666]}, 'position': {'offset': 64622558, 'shape': [1001, 666, 2]}}, '64': {'particle_type': {'offset': 32945, 'shape': [632]}, 'position': {'offset': 65955890, 'shape': [1001, 632, 2]}}, '65': {'particle_type': {'offset': 33577, 'shape': [801]}, 'position': {'offset': 67221154, 'shape': [1001, 801, 2]}}, '66': {'particle_type': {'offset': 34378, 'shape': [1020]}, 'position': {'offset': 68824756, 'shape': [1001, 1020, 2]}}, '67': {'particle_type': {'offset': 35398, 'shape': [821]}, 'position': {'offset': 70866796, 'shape': [1001, 821, 2]}}, '68': {'particle_type': {'offset': 36219, 'shape': [321]}, 'position': {'offset': 72510438, 'shape': [1001, 321, 2]}}, '69': {'particle_type': {'offset': 36540, 'shape': [550]}, 'position': {'offset': 73153080, 'shape': [1001, 550, 2]}}, '70': {'particle_type': {'offset': 37090, 'shape': [505]}, 'position': {'offset': 74254180, 'shape': [1001, 505, 2]}}, '71': {'particle_type': {'offset': 37595, 'shape': [555]}, 'position': {'offset': 75265190, 'shape': [1001, 555, 2]}}, '72': {'particle_type': {'offset': 38150, 'shape': [974]}, 'position': {'offset': 76376300, 'shape': [1001, 974, 2]}}, '73': {'particle_type': {'offset': 39124, 'shape': [298]}, 'position': {'offset': 78326248, 'shape': [1001, 298, 2]}}, '74': {'particle_type': {'offset': 39422, 'shape': [857]}, 'position': {'offset': 78922844, 'shape': [1001, 857, 2]}}, '75': {'particle_type': {'offset': 40279, 'shape': [402]}, 'position': {'offset': 80638558, 'shape': [1001, 402, 2]}}, '76': {'particle_type': {'offset': 40681, 'shape': [540]}, 'position': {'offset': 81443362, 'shape': [1001, 540, 2]}}, '77': {'particle_type': {'offset': 41221, 'shape': [971]}, 'position': {'offset': 82524442, 'shape': [1001, 971, 2]}}, '78': {'particle_type': {'offset': 42192, 'shape': [285]}, 'position': {'offset': 84468384, 'shape': [1001, 285, 2]}}, '79': {'particle_type': {'offset': 42477, 'shape': [455]}, 'position': {'offset': 85038954, 'shape': [1001, 455, 2]}}, '80': {'particle_type': {'offset': 42932, 'shape': [519]}, 'position': {'offset': 85949864, 'shape': [1001, 519, 2]}}, '81': {'particle_type': {'offset': 43451, 'shape': [954]}, 'position': {'offset': 86988902, 'shape': [1001, 954, 2]}}, '82': {'particle_type': {'offset': 44405, 'shape': [449]}, 'position': {'offset': 88898810, 'shape': [1001, 449, 2]}}, '83': {'particle_type': {'offset': 44854, 'shape': [698]}, 'position': {'offset': 89797708, 'shape': [1001, 698, 2]}}, '84': {'particle_type': {'offset': 45552, 'shape': [325]}, 'position': {'offset': 91195104, 'shape': [1001, 325, 2]}}, '85': {'particle_type': {'offset': 45877, 'shape': [957]}, 'position': {'offset': 91845754, 'shape': [1001, 957, 2]}}, '86': {'particle_type': {'offset': 46834, 'shape': [372]}, 'position': {'offset': 93761668, 'shape': [1001, 372, 2]}}, '87': {'particle_type': {'offset': 47206, 'shape': [382]}, 'position': {'offset': 94506412, 'shape': [1001, 382, 2]}}, '88': {'particle_type': {'offset': 47588, 'shape': [325]}, 'position': {'offset': 95271176, 'shape': [1001, 325, 2]}}, '89': {'particle_type': {'offset': 47913, 'shape': [554]}, 'position': {'offset': 95921826, 'shape': [1001, 554, 2]}}, '90': {'particle_type': {'offset': 48467, 'shape': [1034]}, 'position': {'offset': 97030934, 'shape': [1001, 1034, 2]}}, '91': {'particle_type': {'offset': 49501, 'shape': [453]}, 'position': {'offset': 99101002, 'shape': [1001, 453, 2]}}, '92': {'particle_type': {'offset': 49954, 'shape': [834]}, 'position': {'offset': 100007908, 'shape': [1001, 834, 2]}}, '93': {'particle_type': {'offset': 50788, 'shape': [479]}, 'position': {'offset': 101677576, 'shape': [1001, 479, 2]}}, '94': {'particle_type': {'offset': 51267, 'shape': [422]}, 'position': {'offset': 102636534, 'shape': [1001, 422, 2]}}, '95': {'particle_type': {'offset': 51689, 'shape': [351]}, 'position': {'offset': 103481378, 'shape': [1001, 351, 2]}}, '96': {'particle_type': {'offset': 52040, 'shape': [990]}, 'position': {'offset': 104184080, 'shape': [1001, 990, 2]}}, '97': {'particle_type': {'offset': 53030, 'shape': [727]}, 'position': {'offset': 106166060, 'shape': [1001, 727, 2]}}, '98': {'particle_type': {'offset': 53757, 'shape': [501]}, 'position': {'offset': 107621514, 'shape': [1001, 501, 2]}}, '99': {'particle_type': {'offset': 54258, 'shape': [513]}, 'position': {'offset': 108624516, 'shape': [1001, 513, 2]}}}\n",
      "\n",
      "###################################\n",
      "## self.offset ## OneStepDataset ## 2\n",
      "###################################\n",
      "Length: 100\n",
      "Type: <class 'dict'>\n",
      "{0: {'particle_type': {'offset': 0, 'shape': [678]}, 'position': {'offset': 0, 'shape': [1001, 678, 2]}}, 1: {'particle_type': {'offset': 678, 'shape': [355]}, 'position': {'offset': 1357356, 'shape': [1001, 355, 2]}}, 2: {'particle_type': {'offset': 1033, 'shape': [461]}, 'position': {'offset': 2068066, 'shape': [1001, 461, 2]}}, 3: {'particle_type': {'offset': 1494, 'shape': [307]}, 'position': {'offset': 2990988, 'shape': [1001, 307, 2]}}, 4: {'particle_type': {'offset': 1801, 'shape': [300]}, 'position': {'offset': 3605602, 'shape': [1001, 300, 2]}}, 5: {'particle_type': {'offset': 2101, 'shape': [398]}, 'position': {'offset': 4206202, 'shape': [1001, 398, 2]}}, 6: {'particle_type': {'offset': 2499, 'shape': [362]}, 'position': {'offset': 5002998, 'shape': [1001, 362, 2]}}, 7: {'particle_type': {'offset': 2861, 'shape': [317]}, 'position': {'offset': 5727722, 'shape': [1001, 317, 2]}}, 8: {'particle_type': {'offset': 3178, 'shape': [451]}, 'position': {'offset': 6362356, 'shape': [1001, 451, 2]}}, 9: {'particle_type': {'offset': 3629, 'shape': [285]}, 'position': {'offset': 7265258, 'shape': [1001, 285, 2]}}, 10: {'particle_type': {'offset': 3914, 'shape': [271]}, 'position': {'offset': 7835828, 'shape': [1001, 271, 2]}}, 11: {'particle_type': {'offset': 4185, 'shape': [401]}, 'position': {'offset': 8378370, 'shape': [1001, 401, 2]}}, 12: {'particle_type': {'offset': 4586, 'shape': [1052]}, 'position': {'offset': 9181172, 'shape': [1001, 1052, 2]}}, 13: {'particle_type': {'offset': 5638, 'shape': [613]}, 'position': {'offset': 11287276, 'shape': [1001, 613, 2]}}, 14: {'particle_type': {'offset': 6251, 'shape': [682]}, 'position': {'offset': 12514502, 'shape': [1001, 682, 2]}}, 15: {'particle_type': {'offset': 6933, 'shape': [389]}, 'position': {'offset': 13879866, 'shape': [1001, 389, 2]}}, 16: {'particle_type': {'offset': 7322, 'shape': [403]}, 'position': {'offset': 14658644, 'shape': [1001, 403, 2]}}, 17: {'particle_type': {'offset': 7725, 'shape': [353]}, 'position': {'offset': 15465450, 'shape': [1001, 353, 2]}}, 18: {'particle_type': {'offset': 8078, 'shape': [445]}, 'position': {'offset': 16172156, 'shape': [1001, 445, 2]}}, 19: {'particle_type': {'offset': 8523, 'shape': [298]}, 'position': {'offset': 17063046, 'shape': [1001, 298, 2]}}, 20: {'particle_type': {'offset': 8821, 'shape': [519]}, 'position': {'offset': 17659642, 'shape': [1001, 519, 2]}}, 21: {'particle_type': {'offset': 9340, 'shape': [630]}, 'position': {'offset': 18698680, 'shape': [1001, 630, 2]}}, 22: {'particle_type': {'offset': 9970, 'shape': [272]}, 'position': {'offset': 19959940, 'shape': [1001, 272, 2]}}, 23: {'particle_type': {'offset': 10242, 'shape': [670]}, 'position': {'offset': 20504484, 'shape': [1001, 670, 2]}}, 24: {'particle_type': {'offset': 10912, 'shape': [560]}, 'position': {'offset': 21845824, 'shape': [1001, 560, 2]}}, 25: {'particle_type': {'offset': 11472, 'shape': [520]}, 'position': {'offset': 22966944, 'shape': [1001, 520, 2]}}, 26: {'particle_type': {'offset': 11992, 'shape': [586]}, 'position': {'offset': 24007984, 'shape': [1001, 586, 2]}}, 27: {'particle_type': {'offset': 12578, 'shape': [445]}, 'position': {'offset': 25181156, 'shape': [1001, 445, 2]}}, 28: {'particle_type': {'offset': 13023, 'shape': [421]}, 'position': {'offset': 26072046, 'shape': [1001, 421, 2]}}, 29: {'particle_type': {'offset': 13444, 'shape': [456]}, 'position': {'offset': 26914888, 'shape': [1001, 456, 2]}}, 30: {'particle_type': {'offset': 13900, 'shape': [504]}, 'position': {'offset': 27827800, 'shape': [1001, 504, 2]}}, 31: {'particle_type': {'offset': 14404, 'shape': [505]}, 'position': {'offset': 28836808, 'shape': [1001, 505, 2]}}, 32: {'particle_type': {'offset': 14909, 'shape': [658]}, 'position': {'offset': 29847818, 'shape': [1001, 658, 2]}}, 33: {'particle_type': {'offset': 15567, 'shape': [480]}, 'position': {'offset': 31165134, 'shape': [1001, 480, 2]}}, 34: {'particle_type': {'offset': 16047, 'shape': [438]}, 'position': {'offset': 32126094, 'shape': [1001, 438, 2]}}, 35: {'particle_type': {'offset': 16485, 'shape': [593]}, 'position': {'offset': 33002970, 'shape': [1001, 593, 2]}}, 36: {'particle_type': {'offset': 17078, 'shape': [520]}, 'position': {'offset': 34190156, 'shape': [1001, 520, 2]}}, 37: {'particle_type': {'offset': 17598, 'shape': [518]}, 'position': {'offset': 35231196, 'shape': [1001, 518, 2]}}, 38: {'particle_type': {'offset': 18116, 'shape': [787]}, 'position': {'offset': 36268232, 'shape': [1001, 787, 2]}}, 39: {'particle_type': {'offset': 18903, 'shape': [378]}, 'position': {'offset': 37843806, 'shape': [1001, 378, 2]}}, 40: {'particle_type': {'offset': 19281, 'shape': [598]}, 'position': {'offset': 38600562, 'shape': [1001, 598, 2]}}, 41: {'particle_type': {'offset': 19879, 'shape': [513]}, 'position': {'offset': 39797758, 'shape': [1001, 513, 2]}}, 42: {'particle_type': {'offset': 20392, 'shape': [644]}, 'position': {'offset': 40824784, 'shape': [1001, 644, 2]}}, 43: {'particle_type': {'offset': 21036, 'shape': [478]}, 'position': {'offset': 42114072, 'shape': [1001, 478, 2]}}, 44: {'particle_type': {'offset': 21514, 'shape': [418]}, 'position': {'offset': 43071028, 'shape': [1001, 418, 2]}}, 45: {'particle_type': {'offset': 21932, 'shape': [473]}, 'position': {'offset': 43907864, 'shape': [1001, 473, 2]}}, 46: {'particle_type': {'offset': 22405, 'shape': [694]}, 'position': {'offset': 44854810, 'shape': [1001, 694, 2]}}, 47: {'particle_type': {'offset': 23099, 'shape': [702]}, 'position': {'offset': 46244198, 'shape': [1001, 702, 2]}}, 48: {'particle_type': {'offset': 23801, 'shape': [869]}, 'position': {'offset': 47649602, 'shape': [1001, 869, 2]}}, 49: {'particle_type': {'offset': 24670, 'shape': [659]}, 'position': {'offset': 49389340, 'shape': [1001, 659, 2]}}, 50: {'particle_type': {'offset': 25329, 'shape': [713]}, 'position': {'offset': 50708658, 'shape': [1001, 713, 2]}}, 51: {'particle_type': {'offset': 26042, 'shape': [682]}, 'position': {'offset': 52136084, 'shape': [1001, 682, 2]}}, 52: {'particle_type': {'offset': 26724, 'shape': [484]}, 'position': {'offset': 53501448, 'shape': [1001, 484, 2]}}, 53: {'particle_type': {'offset': 27208, 'shape': [589]}, 'position': {'offset': 54470416, 'shape': [1001, 589, 2]}}, 54: {'particle_type': {'offset': 27797, 'shape': [1058]}, 'position': {'offset': 55649594, 'shape': [1001, 1058, 2]}}, 55: {'particle_type': {'offset': 28855, 'shape': [362]}, 'position': {'offset': 57767710, 'shape': [1001, 362, 2]}}, 56: {'particle_type': {'offset': 29217, 'shape': [419]}, 'position': {'offset': 58492434, 'shape': [1001, 419, 2]}}, 57: {'particle_type': {'offset': 29636, 'shape': [523]}, 'position': {'offset': 59331272, 'shape': [1001, 523, 2]}}, 58: {'particle_type': {'offset': 30159, 'shape': [480]}, 'position': {'offset': 60378318, 'shape': [1001, 480, 2]}}, 59: {'particle_type': {'offset': 30639, 'shape': [534]}, 'position': {'offset': 61339278, 'shape': [1001, 534, 2]}}, 60: {'particle_type': {'offset': 31173, 'shape': [359]}, 'position': {'offset': 62408346, 'shape': [1001, 359, 2]}}, 61: {'particle_type': {'offset': 31532, 'shape': [198]}, 'position': {'offset': 63127064, 'shape': [1001, 198, 2]}}, 62: {'particle_type': {'offset': 31730, 'shape': [549]}, 'position': {'offset': 63523460, 'shape': [1001, 549, 2]}}, 63: {'particle_type': {'offset': 32279, 'shape': [666]}, 'position': {'offset': 64622558, 'shape': [1001, 666, 2]}}, 64: {'particle_type': {'offset': 32945, 'shape': [632]}, 'position': {'offset': 65955890, 'shape': [1001, 632, 2]}}, 65: {'particle_type': {'offset': 33577, 'shape': [801]}, 'position': {'offset': 67221154, 'shape': [1001, 801, 2]}}, 66: {'particle_type': {'offset': 34378, 'shape': [1020]}, 'position': {'offset': 68824756, 'shape': [1001, 1020, 2]}}, 67: {'particle_type': {'offset': 35398, 'shape': [821]}, 'position': {'offset': 70866796, 'shape': [1001, 821, 2]}}, 68: {'particle_type': {'offset': 36219, 'shape': [321]}, 'position': {'offset': 72510438, 'shape': [1001, 321, 2]}}, 69: {'particle_type': {'offset': 36540, 'shape': [550]}, 'position': {'offset': 73153080, 'shape': [1001, 550, 2]}}, 70: {'particle_type': {'offset': 37090, 'shape': [505]}, 'position': {'offset': 74254180, 'shape': [1001, 505, 2]}}, 71: {'particle_type': {'offset': 37595, 'shape': [555]}, 'position': {'offset': 75265190, 'shape': [1001, 555, 2]}}, 72: {'particle_type': {'offset': 38150, 'shape': [974]}, 'position': {'offset': 76376300, 'shape': [1001, 974, 2]}}, 73: {'particle_type': {'offset': 39124, 'shape': [298]}, 'position': {'offset': 78326248, 'shape': [1001, 298, 2]}}, 74: {'particle_type': {'offset': 39422, 'shape': [857]}, 'position': {'offset': 78922844, 'shape': [1001, 857, 2]}}, 75: {'particle_type': {'offset': 40279, 'shape': [402]}, 'position': {'offset': 80638558, 'shape': [1001, 402, 2]}}, 76: {'particle_type': {'offset': 40681, 'shape': [540]}, 'position': {'offset': 81443362, 'shape': [1001, 540, 2]}}, 77: {'particle_type': {'offset': 41221, 'shape': [971]}, 'position': {'offset': 82524442, 'shape': [1001, 971, 2]}}, 78: {'particle_type': {'offset': 42192, 'shape': [285]}, 'position': {'offset': 84468384, 'shape': [1001, 285, 2]}}, 79: {'particle_type': {'offset': 42477, 'shape': [455]}, 'position': {'offset': 85038954, 'shape': [1001, 455, 2]}}, 80: {'particle_type': {'offset': 42932, 'shape': [519]}, 'position': {'offset': 85949864, 'shape': [1001, 519, 2]}}, 81: {'particle_type': {'offset': 43451, 'shape': [954]}, 'position': {'offset': 86988902, 'shape': [1001, 954, 2]}}, 82: {'particle_type': {'offset': 44405, 'shape': [449]}, 'position': {'offset': 88898810, 'shape': [1001, 449, 2]}}, 83: {'particle_type': {'offset': 44854, 'shape': [698]}, 'position': {'offset': 89797708, 'shape': [1001, 698, 2]}}, 84: {'particle_type': {'offset': 45552, 'shape': [325]}, 'position': {'offset': 91195104, 'shape': [1001, 325, 2]}}, 85: {'particle_type': {'offset': 45877, 'shape': [957]}, 'position': {'offset': 91845754, 'shape': [1001, 957, 2]}}, 86: {'particle_type': {'offset': 46834, 'shape': [372]}, 'position': {'offset': 93761668, 'shape': [1001, 372, 2]}}, 87: {'particle_type': {'offset': 47206, 'shape': [382]}, 'position': {'offset': 94506412, 'shape': [1001, 382, 2]}}, 88: {'particle_type': {'offset': 47588, 'shape': [325]}, 'position': {'offset': 95271176, 'shape': [1001, 325, 2]}}, 89: {'particle_type': {'offset': 47913, 'shape': [554]}, 'position': {'offset': 95921826, 'shape': [1001, 554, 2]}}, 90: {'particle_type': {'offset': 48467, 'shape': [1034]}, 'position': {'offset': 97030934, 'shape': [1001, 1034, 2]}}, 91: {'particle_type': {'offset': 49501, 'shape': [453]}, 'position': {'offset': 99101002, 'shape': [1001, 453, 2]}}, 92: {'particle_type': {'offset': 49954, 'shape': [834]}, 'position': {'offset': 100007908, 'shape': [1001, 834, 2]}}, 93: {'particle_type': {'offset': 50788, 'shape': [479]}, 'position': {'offset': 101677576, 'shape': [1001, 479, 2]}}, 94: {'particle_type': {'offset': 51267, 'shape': [422]}, 'position': {'offset': 102636534, 'shape': [1001, 422, 2]}}, 95: {'particle_type': {'offset': 51689, 'shape': [351]}, 'position': {'offset': 103481378, 'shape': [1001, 351, 2]}}, 96: {'particle_type': {'offset': 52040, 'shape': [990]}, 'position': {'offset': 104184080, 'shape': [1001, 990, 2]}}, 97: {'particle_type': {'offset': 53030, 'shape': [727]}, 'position': {'offset': 106166060, 'shape': [1001, 727, 2]}}, 98: {'particle_type': {'offset': 53757, 'shape': [501]}, 'position': {'offset': 107621514, 'shape': [1001, 501, 2]}}, 99: {'particle_type': {'offset': 54258, 'shape': [513]}, 'position': {'offset': 108624516, 'shape': [1001, 513, 2]}}}\n",
      "\n",
      "#####################################\n",
      "## window_length ## OneStepDataset ##\n",
      "#####################################\n",
      "Type: <class 'int'>\n",
      "7\n",
      "\n",
      "#################################\n",
      "## noise_std ## OneStepDataset ##\n",
      "#################################\n",
      "Type: <class 'float'>\n",
      "0.0003\n",
      "\n",
      "##################################\n",
      "## return_pos ## OneStepDataset ##\n",
      "##################################\n",
      "Type: <class 'bool'>\n",
      "False\n",
      "\n",
      "##########################################\n",
      "## self.particle_type ## OneStepDataset ##\n",
      "##########################################\n",
      "Shape: (54771,)\n",
      "Length: 54771\n",
      "Type: <class 'numpy.memmap'>\n",
      "[5 5 5 ... 5 5 5]\n",
      "\n",
      "#####################################\n",
      "## self.position ## OneStepDataset ##\n",
      "#####################################\n",
      "Shape: (109651542,)\n",
      "Length: 109651542\n",
      "Type: <class 'numpy.memmap'>\n",
      "[0.75829923 0.44826713 0.7454544  ... 0.12831761 0.30728665 0.11043041]\n",
      "\n",
      "################################\n",
      "## self.dim ## OneStepDataset ##\n",
      "################################\n",
      "Type: <class 'int'>\n",
      "2\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "678\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "355\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "461\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "307\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "300\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "398\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "362\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "317\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "451\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "285\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "271\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "401\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "1052\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "613\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "682\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "389\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "403\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "353\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "445\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "298\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "519\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "630\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "272\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "670\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "560\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "520\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "586\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "445\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "421\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "456\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "504\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "505\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "658\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "480\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "438\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "593\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "520\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "518\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "787\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "378\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "598\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "513\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "644\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "478\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "418\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "473\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "694\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "702\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "869\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "659\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "713\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "682\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "484\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "589\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "1058\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "362\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "419\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "523\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "480\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "534\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "359\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "198\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "549\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "666\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "632\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "801\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "1020\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "821\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "321\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "550\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "505\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "555\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "974\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "298\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "857\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "402\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "540\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "971\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "285\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "455\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "519\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "954\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "449\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "698\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "325\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "957\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "372\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "382\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "325\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "554\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "1034\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "453\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "834\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "479\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "422\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "351\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "990\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "727\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "501\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "513\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "###################\n",
      "## train_dataset ##\n",
      "###################\n",
      "Length: 99500\n",
      "Type: <class '__main__.OneStepDataset'>\n",
      "OneStepDataset(99500)\n",
      "\n",
      "#################################\n",
      "## data_path ## OneStepDataset ##\n",
      "#################################\n",
      "/home/admin1/Desktop/gnndataset/datasets/WaterDrop\n",
      "\n",
      "#####################################\n",
      "## self.metadata ## OneStepDataset ##\n",
      "#####################################\n",
      "Length: 9\n",
      "Type: <class 'dict'>\n",
      "{'bounds': [[0.1, 0.9], [0.1, 0.9]], 'sequence_length': 1000, 'default_connectivity_radius': 0.015, 'dim': 2, 'dt': 0.0025, 'vel_mean': [-3.964619574176163e-05, -0.00026272129664401046], 'vel_std': [0.0013722809722366911, 0.0013119977252142715], 'acc_mean': [2.602686518497945e-08, 1.0721623948191945e-07], 'acc_std': [6.742962470925277e-05, 8.700719180424815e-05]}\n",
      "\n",
      "###################################\n",
      "## self.offset ## OneStepDataset ## 1\n",
      "###################################\n",
      "Length: 5\n",
      "Type: <class 'dict'>\n",
      "{'0': {'particle_type': {'offset': 0, 'shape': [482]}, 'position': {'offset': 0, 'shape': [1001, 482, 2]}}, '1': {'particle_type': {'offset': 482, 'shape': [362]}, 'position': {'offset': 964964, 'shape': [1001, 362, 2]}}, '2': {'particle_type': {'offset': 844, 'shape': [537]}, 'position': {'offset': 1689688, 'shape': [1001, 537, 2]}}, '3': {'particle_type': {'offset': 1381, 'shape': [544]}, 'position': {'offset': 2764762, 'shape': [1001, 544, 2]}}, '4': {'particle_type': {'offset': 1925, 'shape': [696]}, 'position': {'offset': 3853850, 'shape': [1001, 696, 2]}}}\n",
      "\n",
      "###################################\n",
      "## self.offset ## OneStepDataset ## 2\n",
      "###################################\n",
      "Length: 5\n",
      "Type: <class 'dict'>\n",
      "{0: {'particle_type': {'offset': 0, 'shape': [482]}, 'position': {'offset': 0, 'shape': [1001, 482, 2]}}, 1: {'particle_type': {'offset': 482, 'shape': [362]}, 'position': {'offset': 964964, 'shape': [1001, 362, 2]}}, 2: {'particle_type': {'offset': 844, 'shape': [537]}, 'position': {'offset': 1689688, 'shape': [1001, 537, 2]}}, 3: {'particle_type': {'offset': 1381, 'shape': [544]}, 'position': {'offset': 2764762, 'shape': [1001, 544, 2]}}, 4: {'particle_type': {'offset': 1925, 'shape': [696]}, 'position': {'offset': 3853850, 'shape': [1001, 696, 2]}}}\n",
      "\n",
      "#####################################\n",
      "## window_length ## OneStepDataset ##\n",
      "#####################################\n",
      "Type: <class 'int'>\n",
      "7\n",
      "\n",
      "#################################\n",
      "## noise_std ## OneStepDataset ##\n",
      "#################################\n",
      "Type: <class 'float'>\n",
      "0.0003\n",
      "\n",
      "##################################\n",
      "## return_pos ## OneStepDataset ##\n",
      "##################################\n",
      "Type: <class 'bool'>\n",
      "False\n",
      "\n",
      "##########################################\n",
      "## self.particle_type ## OneStepDataset ##\n",
      "##########################################\n",
      "Shape: (2621,)\n",
      "Length: 2621\n",
      "Type: <class 'numpy.memmap'>\n",
      "[5 5 5 ... 5 5 5]\n",
      "\n",
      "#####################################\n",
      "## self.position ## OneStepDataset ##\n",
      "#####################################\n",
      "Shape: (5247242,)\n",
      "Length: 5247242\n",
      "Type: <class 'numpy.memmap'>\n",
      "[0.75829923 0.32826713 0.7454544  ... 0.12680247 0.88399655 0.12111415]\n",
      "\n",
      "################################\n",
      "## self.dim ## OneStepDataset ##\n",
      "################################\n",
      "Type: <class 'int'>\n",
      "2\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "482\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "362\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "537\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "544\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "############################\n",
      "## size ## OneStepDataset ##\n",
      "############################\n",
      "Type: <class 'int'>\n",
      "696\n",
      "\n",
      "############\n",
      "## length ## OneStepDataset ##\n",
      "############\n",
      "Type: <class 'int'>\n",
      "995\n",
      "\n",
      "###################\n",
      "## valid_dataset ##\n",
      "###################\n",
      "Length: 4975\n",
      "Type: <class '__main__.OneStepDataset'>\n",
      "OneStepDataset(4975)\n",
      "\n",
      "##################\n",
      "## train_loader ##\n",
      "##################\n",
      "Length: 24875\n",
      "Type: <class 'torch_geometric.loader.dataloader.DataLoader'>\n",
      "<torch_geometric.loader.dataloader.DataLoader object at 0x7f56c3d4f450>\n",
      "\n",
      "##################\n",
      "## valid_loader ##\n",
      "##################\n",
      "Length: 1244\n",
      "Type: <class 'torch_geometric.loader.dataloader.DataLoader'>\n",
      "<torch_geometric.loader.dataloader.DataLoader object at 0x7f56d0555bd0>\n",
      "\n",
      "############################################\n",
      "## self.metadata ## RolloutDataset\\_init_ ##\n",
      "############################################\n",
      "Length: 9\n",
      "Type: <class 'dict'>\n",
      "{'bounds': [[0.1, 0.9], [0.1, 0.9]], 'sequence_length': 1000, 'default_connectivity_radius': 0.015, 'dim': 2, 'dt': 0.0025, 'vel_mean': [-3.964619574176163e-05, -0.00026272129664401046], 'vel_std': [0.0013722809722366911, 0.0013119977252142715], 'acc_mean': [2.602686518497945e-08, 1.0721623948191945e-07], 'acc_std': [6.742962470925277e-05, 8.700719180424815e-05]}\n",
      "\n",
      "##########################################\n",
      "## self.offset ## RolloutDataset\\_init_ ##\n",
      "##########################################\n",
      "Length: 5\n",
      "Type: <class 'dict'>\n",
      "{'0': {'particle_type': {'offset': 0, 'shape': [482]}, 'position': {'offset': 0, 'shape': [1001, 482, 2]}}, '1': {'particle_type': {'offset': 482, 'shape': [362]}, 'position': {'offset': 964964, 'shape': [1001, 362, 2]}}, '2': {'particle_type': {'offset': 844, 'shape': [537]}, 'position': {'offset': 1689688, 'shape': [1001, 537, 2]}}, '3': {'particle_type': {'offset': 1381, 'shape': [544]}, 'position': {'offset': 2764762, 'shape': [1001, 544, 2]}}, '4': {'particle_type': {'offset': 1925, 'shape': [696]}, 'position': {'offset': 3853850, 'shape': [1001, 696, 2]}}}\n",
      "\n",
      "##########################################\n",
      "## self.offset ## RolloutDataset\\_init_ ##\n",
      "##########################################\n",
      "Length: 5\n",
      "Type: <class 'dict'>\n",
      "{0: {'particle_type': {'offset': 0, 'shape': [482]}, 'position': {'offset': 0, 'shape': [1001, 482, 2]}}, 1: {'particle_type': {'offset': 482, 'shape': [362]}, 'position': {'offset': 964964, 'shape': [1001, 362, 2]}}, 2: {'particle_type': {'offset': 844, 'shape': [537]}, 'position': {'offset': 1689688, 'shape': [1001, 537, 2]}}, 3: {'particle_type': {'offset': 1381, 'shape': [544]}, 'position': {'offset': 2764762, 'shape': [1001, 544, 2]}}, 4: {'particle_type': {'offset': 1925, 'shape': [696]}, 'position': {'offset': 3853850, 'shape': [1001, 696, 2]}}}\n",
      "\n",
      "############################################\n",
      "## window_length ## RolloutDataset\\_init_ ##\n",
      "############################################\n",
      "Type: <class 'int'>\n",
      "7\n",
      "\n",
      "#################################################\n",
      "## self.particle_type ## RolloutDataset\\_init_ ##\n",
      "#################################################\n",
      "Shape: (2621,)\n",
      "Length: 2621\n",
      "Type: <class 'numpy.memmap'>\n",
      "[5 5 5 ... 5 5 5]\n",
      "\n",
      "############################################\n",
      "## self.position ## RolloutDataset\\_init_ ##\n",
      "############################################\n",
      "Shape: (5247242,)\n",
      "Length: 5247242\n",
      "Type: <class 'numpy.memmap'>\n",
      "[0.75829923 0.32826713 0.7454544  ... 0.12680247 0.88399655 0.12111415]\n",
      "\n",
      "###########################\n",
      "## valid_rollout_dataset ##\n",
      "###########################\n",
      "Length: 5\n",
      "Type: <class '__main__.RolloutDataset'>\n",
      "RolloutDataset(5)\n",
      "\n",
      "############################################\n",
      "## window_size ## LearnedSimulator\\_init_ ##\n",
      "############################################\n",
      "5\n",
      "\n",
      "#####################\n",
      "## self.embed_type ## LearnedSimulator\\_init_ ##\n",
      "#####################\n",
      "Type: <class 'torch.nn.modules.sparse.Embedding'>\n",
      "Embedding(9, 16)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=30, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=30, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=30, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=30, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=30, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=30, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "#############################################\n",
      "## self.node_in ## LearnedSimulator\\_init_ ##\n",
      "#############################################\n",
      "Type: <class '__main__.MLP'>\n",
      "MLP(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=30, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=3, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=3, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=3, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "#############################################\n",
      "## self.edge_in ## LearnedSimulator\\_init_ ##\n",
      "#############################################\n",
      "Type: <class '__main__.MLP'>\n",
      "MLP(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=2, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=2, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=2, bias=True)\n",
      "\n",
      "##############################################\n",
      "## self.node_out ## LearnedSimulator\\_init_ ##\n",
      "##############################################\n",
      "Type: <class '__main__.MLP'>\n",
      "MLP(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "############################################\n",
      "## n_mp_layers ## LearnedSimulator\\_init_ ##\n",
      "############################################\n",
      "Type: <class 'int'>\n",
      "10\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=384, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "###############################\n",
      "## self.layers ## MLP\\_init_ ##\n",
      "###############################\n",
      "Length: 0\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList()\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 2\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "\n",
      "#################################\n",
      "## self.layers ## MLP\\_init_\\i ##\n",
      "#################################\n",
      "Length: 4\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "\n",
      "########################\n",
      "## self.layers ## MLP ##\n",
      "########################\n",
      "Length: 6\n",
      "Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.activation.ReLU'>\n",
      "ReLU()\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=128, out_features=128, bias=True)\n",
      "\n",
      "###################################\n",
      "## layer ## MLP\r",
      "eset_parameters ##\n",
      "###################################\n",
      "Type: <class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31850/3827762460.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0msimulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \"\"\"\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GNN/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \"\"\"\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# Training model is time-consuming. We highly recommend you to skip this block and load checkpoint in next block.\n",
    "\n",
    "# load dataset\n",
    "# Load training dataset\n",
    "# Get the data used for training the model\n",
    "train_dataset = OneStepDataset(data_path, \"train\", noise_std=params[\"noise\"])\n",
    "print(\"###################\")\n",
    "print(\"## train_dataset ##\")\n",
    "print(\"###################\")\n",
    "# print(\"Shape:\", train_dataset.shape)\n",
    "print(\"Length:\", len(train_dataset))\n",
    "print(\"Type:\", type(train_dataset))\n",
    "print(train_dataset)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Load validation dataset\n",
    "# Get the data used for validating the model's performance\n",
    "valid_dataset = OneStepDataset(data_path, \"valid\", noise_std=params[\"noise\"])\n",
    "print(\"###################\")\n",
    "print(\"## valid_dataset ##\")\n",
    "print(\"###################\")\n",
    "# print(\"Shape:\", valid_dataset.shape)\n",
    "print(\"Length:\", len(valid_dataset))\n",
    "print(\"Type:\", type(valid_dataset))\n",
    "print(valid_dataset)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Create data loader for training set\n",
    "# Set up a method to load the training data in batches\n",
    "train_loader = pyg.loader.DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True, pin_memory=True, num_workers=2)\n",
    "print(\"##################\")\n",
    "print(\"## train_loader ##\")\n",
    "print(\"##################\")\n",
    "# print(\"Shape:\", train_loader.shape)\n",
    "print(\"Length:\", len(train_loader))\n",
    "print(\"Type:\", type(train_loader))\n",
    "print(train_loader)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Create data loader for validation set\n",
    "# Set up a method to load the validation data in batches\n",
    "valid_loader = pyg.loader.DataLoader(valid_dataset, batch_size=params[\"batch_size\"], shuffle=False, pin_memory=True, num_workers=2)\n",
    "print(\"##################\")\n",
    "print(\"## valid_loader ##\")\n",
    "print(\"##################\")\n",
    "# print(\"Shape:\", valid_loader.shape)\n",
    "print(\"Length:\", len(valid_loader))\n",
    "print(\"Type:\", type(valid_loader))\n",
    "print(valid_loader)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Load rollout dataset for validation\n",
    "# Get the data used for multi-step validation of the model\n",
    "valid_rollout_dataset = RolloutDataset(data_path, \"valid\")\n",
    "print(\"###########################\")\n",
    "print(\"## valid_rollout_dataset ##\")\n",
    "print(\"###########################\")\n",
    "# print(\"Shape:\", valid_rollout_dataset.shape)\n",
    "print(\"Length:\", len(valid_rollout_dataset))\n",
    "print(\"Type:\", type(valid_rollout_dataset))\n",
    "print(valid_rollout_dataset)\n",
    "print(\"\")\n",
    "\n",
    "# Build the model\n",
    "# Create an instance of the LearnedSimulator model\n",
    "simulator = LearnedSimulator()\n",
    "\n",
    "# Move the model to the GPU\n",
    "# Transfer the model to the GPU for faster computation\n",
    "simulator = simulator.cuda()\n",
    "\n",
    "# train model\n",
    "# Train the model and collect various metrics\n",
    "# This comment indicates the next line of code will train the model and collect metrics like loss and MSE.\n",
    "# Train the model using the specified parameters, simulator, and data loaders.\n",
    "# The train function is called with parameters including the training and validation data loaders and the validation rollout dataset.\n",
    "# The function returns four lists containing the training loss, evaluation loss, one-step MSE, and rollout MSE over the course of the training process.\n",
    "train_loss_list, eval_loss_list, onestep_mse_list, rollout_mse_list = train(params, simulator, train_loader, valid_loader, valid_rollout_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "MBw_qGjz62_S",
    "outputId": "f69959c9-a199-4e82-ea12-694112546893"
   },
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "# This comment indicates the next lines of code will save the trained model to disk.\n",
    "\n",
    "# Define the file path for saving the model\n",
    "# This sets the file path where the model will be saved. The file will be named \"simulator_model_20epoch.pth\".\n",
    "model_save_path = \"simulator_model_20epoch.pth\"\n",
    "\n",
    "# Save the model's state dictionary to the specified file\n",
    "# This saves the state dictionary of the simulator model, which contains the model's parameters, to the specified file path.\n",
    "torch.save(simulator.state_dict(), model_save_path)\n",
    "\n",
    "# Print a message confirming the model has been saved\n",
    "# This prints a confirmation message indicating that the model has been successfully saved to the specified file path.\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "MBw_qGjz62_S",
    "outputId": "f69959c9-a199-4e82-ea12-694112546893"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loss_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31850/2411702948.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# visualize the loss curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meval_loss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iterations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loss_list' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Enables inline plotting in Jupyter notebooks\n",
    "# This ensures that plots appear within the Jupyter notebook.\n",
    "# Comment for above\n",
    "\n",
    "# Import matplotlib for plotting\n",
    "# This is used for creating and displaying plots.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize loss curve\n",
    "# Create a new figure\n",
    "# This starts a new figure for the plot.\n",
    "plt.figure()\n",
    "\n",
    "# Plot the training loss curve\n",
    "# This plots the training loss data, where train_loss_list contains tuples of (iteration, loss).\n",
    "plt.plot(*zip(*train_loss_list), label=\"train\")\n",
    "\n",
    "# Plot the validation loss curve\n",
    "# This plots the validation loss data, where eval_loss_list contains tuples of (iteration, loss).\n",
    "plt.plot(*zip(*eval_loss_list), label=\"valid\")\n",
    "\n",
    "# Set the x-axis label\n",
    "# This sets the label for the x-axis to 'Iterations'.\n",
    "plt.xlabel('Iterations')\n",
    "\n",
    "# Set the y-axis label\n",
    "# This sets the label for the y-axis to 'Loss'.\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Set the title of the plot\n",
    "# This sets the title of the plot to 'Loss'.\n",
    "plt.title('Loss')\n",
    "\n",
    "# Display the legend\n",
    "# This displays a legend to differentiate between the training and validation loss curves.\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "# This renders the plot and displays it.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Y2Pf_TcbR0K"
   },
   "source": [
    "• Load checkpoint trained by us. \n",
    "\n",
    "• Do **not** run this block if you have trained your model in previous block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cw26wx-9B_Q3"
   },
   "outputs": [],
   "source": [
    "################\n",
    "## LOAD MODEL ##\n",
    "################\n",
    "# Instantiate the LearnedSimulator model\n",
    "# Create the simulation model that will be trained\n",
    "simulator = LearnedSimulator()\n",
    "\n",
    "# Move the model to the GPU\n",
    "# Transfer the model to the GPU for faster computation\n",
    "simulator = simulator.cuda()\n",
    "\n",
    "\n",
    "#!wget -O temp/models/WaterDrop_checkpoint.pt https://storage.googleapis.com/cs224w_course_project_dataset/Checkpoints/WaterDrop_checkpoint.pt\n",
    "# checkpoint = torch.load(\"simulator_model_20epoch.pth\")\n",
    "# simulator.load_state_dict(checkpoint[\"model\"])\n",
    "model_save_path = \"simulator_model_20epoch.pth\"\n",
    "\n",
    "\n",
    "simulator.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eDD-SySxKFt"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "Since video is 1000 frames long, it might take a few minutes to rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNhrML1SrWIe"
   },
   "outputs": [],
   "source": [
    "# Load the rollout dataset for validation\n",
    "# This loads the dataset used for generating rollout predictions from the validation data.\n",
    "rollout_dataset = RolloutDataset(data_path, \"valid\")\n",
    "\n",
    "# Set the simulator model to evaluation mode\n",
    "# This puts the model in evaluation mode, turning off certain features like dropout.\n",
    "simulator.eval()\n",
    "\n",
    "# Get the first sample from the rollout dataset\n",
    "# This retrieves the first data sample from the validation dataset for processing.\n",
    "rollout_data = rollout_dataset[0]\n",
    "\n",
    "# Perform rollout prediction\n",
    "# This runs the rollout prediction using the simulator model, taking into account metadata and noise parameters.\n",
    "rollout_out = rollout(simulator, rollout_data, rollout_dataset.metadata, params[\"noise\"])\n",
    "\n",
    "# Rearrange the dimensions of the output tensor\n",
    "# This reorders the dimensions of the output tensor for easier handling and visualization.\n",
    "rollout_out = rollout_out.permute(1, 0, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "H0W5Tm4rws0_",
    "outputId": "a9a21fec-2d17-4ab5-f37e-b7bf3fc05114"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Enable inline plotting in Jupyter notebooks\n",
    "# This allows the plots to be displayed directly within the Jupyter notebook.\n",
    "# Comment for above\n",
    "\n",
    "# Import the matplotlib library for plotting\n",
    "# This imports the matplotlib library, which is used for creating visualizations in Python.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the animation module from matplotlib\n",
    "# This imports the animation module from matplotlib, which allows for creating animations.\n",
    "from matplotlib import animation\n",
    "\n",
    "# Import the HTML display module from IPython\n",
    "# This imports the HTML display module, which helps in displaying HTML content in Jupyter notebooks.\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Define a dictionary to map particle types to colors\n",
    "# This dictionary maps different particle types to specific colors for visualization.\n",
    "TYPE_TO_COLOR = {\n",
    "    3: \"black\",\n",
    "    0: \"green\",\n",
    "    7: \"magenta\",\n",
    "    6: \"gold\",\n",
    "    5: \"blue\",\n",
    "}\n",
    "\n",
    "\n",
    "def visualize_prepare(ax, particle_type, position, metadata):\n",
    "    # Get the bounds from metadata\n",
    "    # This extracts the boundary information from the metadata.\n",
    "    bounds = metadata[\"bounds\"]\n",
    "    # Set the x-axis limits\n",
    "    # This sets the limits for the x-axis based on the boundary information.\n",
    "    ax.set_xlim(bounds[0][0], bounds[0][1])\n",
    "    # Set the y-axis limits\n",
    "    # This sets the limits for the y-axis based on the boundary information.\n",
    "    ax.set_ylim(bounds[1][0], bounds[1][1])\n",
    "    # Remove x-axis ticks\n",
    "    # This removes the ticks on the x-axis for a cleaner plot.\n",
    "    ax.set_xticks([])\n",
    "    # Remove y-axis ticks\n",
    "    # This removes the ticks on the y-axis for a cleaner plot.\n",
    "    ax.set_yticks([])\n",
    "    # Set the aspect ratio to be equal\n",
    "    # This sets the aspect ratio of the plot to be equal, ensuring a square plot.\n",
    "    ax.set_aspect(1.0)\n",
    "    # This initializes empty plots for each particle type with specific colors and markers.\n",
    "    points = {type_: ax.plot([], [], \"o\", ms=2, color=color)[0] for type_, color in TYPE_TO_COLOR.items()}\n",
    "    # Return the axis, position, and points\n",
    "    # This returns the axis object, positions, and points dictionary for further use.\n",
    "    return ax, position, points\n",
    "\n",
    "\n",
    "def visualize_pair(particle_type, position_pred, position_gt, metadata):\n",
    "    # Create a subplot with two side-by-side plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # This creates a figure with two subplots side by side, each with a size of 10x5 inches.    \n",
    "    # Prepare the axes for ground truth and predicted positions\n",
    "    # This prepares the subplots for displaying ground truth and predicted positions.\n",
    "    plot_info = [\n",
    "        visualize_prepare(axes[0], particle_type, position_gt, metadata),\n",
    "        visualize_prepare(axes[1], particle_type, position_pred, metadata),\n",
    "    ]\n",
    "\n",
    "    # Set the title for the ground truth plot\n",
    "    # This sets the title of the first subplot to \"Ground truth\".    \n",
    "    axes[0].set_title(\"Ground truth\")\n",
    "\n",
    "    # Set the title for the prediction plot\n",
    "    # This sets the title of the second subplot to \"Prediction\".    \n",
    "    axes[1].set_title(\"Prediction\")\n",
    "\n",
    "    # Close the plot to avoid displaying a static image\n",
    "    # This closes the plot to prevent it from displaying as a static image.    \n",
    "    plt.close()\n",
    "\n",
    "    def update(step_i):\n",
    "        # Initialize an empty list for outputs\n",
    "        # This initializes an empty list to store the updated points for animation.\n",
    "        outputs = []\n",
    "\n",
    "        # Loop through the plot information\n",
    "        # This loops through each item in the plot information list.\n",
    "        for _, position, points in plot_info:\n",
    "\n",
    "            # Loop through each particle type and corresponding line\n",
    "            for type_, line in points.items():\n",
    "                # Create a mask for the current particle type\n",
    "                # This creates a boolean mask for the current particle type.\n",
    "                mask = particle_type == type_\n",
    "\n",
    "                # Update the line data\n",
    "                # This updates the line data with the positions of particles of the current type.\n",
    "                line.set_data(position[step_i, mask, 0], position[step_i, mask, 1])\n",
    "\n",
    "            # Append the updated line to the outputs list\n",
    "            # This appends the updated line to the outputs list.\n",
    "            outputs.append(line)\n",
    "\n",
    "        # Return the updated lines\n",
    "        # This returns the updated lines for the animation frame.        \n",
    "        return outputs\n",
    "\n",
    "    # Create the animation\n",
    "    # This creates an animation using the update function, iterating through the frames.\n",
    "    return animation.FuncAnimation(fig, update, frames=np.arange(0, position_gt.size(0)), interval=10, blit=True)\n",
    "\n",
    "# Generate the animation\n",
    "# This generates the animation by visualizing the predicted and ground truth particle positions.\n",
    "anim = visualize_pair(rollout_data[\"particle_type\"], rollout_out, rollout_data[\"position\"], rollout_dataset.metadata)\n",
    "\n",
    "# Display the animation as an HTML5 video\n",
    "# This displays the generated animation as an HTML5 video in the notebook.\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3t753E3BzEC"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "• Hope this Colab is helpful for you to understand how to apply GNN in a real-world application like simulating complex physics! \n",
    "\n",
    "• If you're interested in technical details, read [medium post](https://) or see [original paper](https://arxiv.org/abs/2002.09405) by DeepMind. \n",
    "\n",
    "• Thanks for spending your time with us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

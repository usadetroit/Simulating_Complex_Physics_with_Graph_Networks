{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGKCXbWIVjT6"
   },
   "source": [
    "# Simulating Complex Physics with Graph Networks: step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1E_PEIxaHr8"
   },
   "source": [
    "## Overview\n",
    "\n",
    "• By Peng Chen, Shiyu Li, Haochen Shi as part of Stanford CS224W course project. \n",
    "\n",
    "• This tutorial provides a step-by-step guide for how to build a Graph Network to simulate complex physics.\n",
    "\n",
    "**Before we get started:**\n",
    "- This Colab includes a concise PyG implementation of paper ***Learning to Simulate Complex Physics with Graph Networks*.\n",
    "- We adapted our code from open-source tensorflow implementation by DeepMind.\n",
    "    - Link to pdf of this paper: https://arxiv.org/abs/2002.09405\n",
    "    - Link to Deepmind's implementation: https://github.com/deepmind/deepmind-research/tree/master/learning_to_simulate\n",
    "    - Link to video site by DeepMind: https://sites.google.com/view/learning-to-simulate\n",
    "- Run **sequentially run all cells in each section**, so intermediate variables / packages will carry over to next cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wgi77SUya5RT"
   },
   "source": [
    "## Device\n",
    "\n",
    "We recommend using a GPU for this Colab. Click `Runtime` then `Change runtime type`. Then set `hardware accelerator` to **GPU**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPBthm7fHqNC"
   },
   "source": [
    "## Setup\n",
    "\n",
    "installation of PyG on Colab can be a little bit tricky. Before we get started, let's check which version of PyTorch you are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGMo0gTp-GLe",
    "outputId": "1a45454d-a1a0-4ca7-8471-0a0919907b7f"
   },
   "outputs": [],
   "source": [
    "#!pip install rectpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGMo0gTp-GLe",
    "outputId": "1a45454d-a1a0-4ca7-8471-0a0919907b7f"
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def identify_text(text):\n",
    "#     # Define a regex pattern to extract the text between the timestamp and newline\n",
    "#     pattern = r\"\\[\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\] - (.*?)\\n\"\n",
    "    \n",
    "#     # Search for the pattern in the text\n",
    "#     match = re.search(pattern, text)\n",
    "    \n",
    "#     if match:\n",
    "#         # Extract the text between the timestamp and newline\n",
    "#         extracted_text = match.group(1)\n",
    "#         print(extracted_text)\n",
    "#     else:\n",
    "#         print(\"Pattern not found\")\n",
    "\n",
    "# # Sample texts\n",
    "# text1 = \"[2024-08-31 15:38:16] - rollout_path\\ntemp/rollouts/WaterDrop\"\n",
    "# text2 = \"[2024-08-31 15:38:17] - self.metadata - OneStepDataset\\nLength:9\\nType:<class 'dict'>...\"\n",
    "\n",
    "# # Test the function\n",
    "# identify_text(text1)  # Should print \"rollout_path\"\n",
    "# identify_text(text2)  # Should print \"self.metadata - OneStepDataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGMo0gTp-GLe",
    "outputId": "1a45454d-a1a0-4ca7-8471-0a0919907b7f"
   },
   "outputs": [],
   "source": [
    "# Dataset Source #1:\n",
    "# https://drive.google.com/file/d/1ZmiKpsQVLFxPOIff-LfFkZwe5ZYG1FEb/view?usp=drive_link\n",
    "\n",
    "# Dataset Source #2:\n",
    "# https://drive.google.com/drive/mobile/folders/11uuYl0peqPg2DQno64YPYMODPu8fjDXU?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGMo0gTp-GLe",
    "outputId": "1a45454d-a1a0-4ca7-8471-0a0919907b7f"
   },
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCy3zaaOGrrS"
   },
   "outputs": [],
   "source": [
    "!export LD_LIBRARY_PATH=/home/admin1/anaconda3/envs/GNN/lib:$LD_LIBRARY_PATH\n",
    "!export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGMo0gTp-GLe",
    "outputId": "1a45454d-a1a0-4ca7-8471-0a0919907b7f"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageOps\n",
    "imageindex = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGMo0gTp-GLe",
    "outputId": "1a45454d-a1a0-4ca7-8471-0a0919907b7f"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_timestamp(log_entry):\n",
    "    # Use regex to match the timestamp pattern and remove it\n",
    "    cleaned_entry = re.sub(r'\\[\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\] - ', '', log_entry)\n",
    "    return cleaned_entry\n",
    "\n",
    "def text_to_image_function(text, font_size, output_file, selected_font_name):\n",
    "    # Remove timestamp\n",
    "    text = remove_timestamp(text)\n",
    "\n",
    "    # Define the initial image size and other properties\n",
    "    initial_max_width = 640\n",
    "    initial_max_height = 640\n",
    "    background_color = \"white\"\n",
    "    text_color = \"black\"\n",
    "    border_color = \"black\"\n",
    "    padding = 20\n",
    "    border_width = 1\n",
    "\n",
    "    # Initialize variables for the actual size\n",
    "    required_width = initial_max_width\n",
    "    required_height = initial_max_height\n",
    "\n",
    "    # Load the font\n",
    "    font = ImageFont.truetype(selected_font_name, font_size)\n",
    "\n",
    "    # Create a temporary image to measure the text size\n",
    "    temp_image = Image.new(\"RGB\", (initial_max_width, initial_max_height), background_color)\n",
    "    draw = ImageDraw.Draw(temp_image)\n",
    "    \n",
    "    # Measure the text size\n",
    "    text_size = draw.textsize(text, font=font)\n",
    "    #text_size = draw.textbbox((0, 0), text, font=font)\n",
    "\n",
    "    # Calculate the required size based on the measured text size\n",
    "    required_width = text_size[0] + 2 * (padding + border_width)\n",
    "    required_height = text_size[1] + 2 * (padding + border_width)\n",
    "\n",
    "    # Ensure the image is not smaller than the initial size\n",
    "    required_width = max(required_width, initial_max_width)\n",
    "    required_height = max(required_height, initial_max_height)\n",
    "\n",
    "    # Create the final image with the calculated size\n",
    "    image = Image.new(\"RGB\", (required_width, required_height), background_color)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Draw the text on the image\n",
    "    text_position = (padding, padding)\n",
    "    draw.text(text_position, text, fill=text_color, font=font)\n",
    "\n",
    "    # Draw a border around the text\n",
    "    border_rectangle = [\n",
    "        padding - border_width, \n",
    "        padding - border_width, \n",
    "        padding + text_size[0] + border_width, \n",
    "        padding + text_size[1] + border_width\n",
    "    ]\n",
    "    draw.rectangle(border_rectangle, outline=border_color, width=border_width)\n",
    "\n",
    "    border_rectangle1 = [\n",
    "        padding - border_width, \n",
    "        padding - border_width, \n",
    "        padding + text_size[0] + border_width + 1, \n",
    "        padding + text_size[1] + border_width + 1\n",
    "    ]\n",
    "    \n",
    "    # Crop the image to the size of the border\n",
    "    cropped_image = image.crop(border_rectangle1)\n",
    "\n",
    "    # Save the cropped image\n",
    "    cropped_image.save(output_file, \"PNG\")\n",
    "\n",
    "    # Optionally, copy the cropped image to the clipboard (requires `pyperclip` and `Pillow` integration)\n",
    "    # pyperclip.copy(cropped_image)  # Not directly supported; requires custom implementation\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGMo0gTp-GLe",
    "outputId": "1a45454d-a1a0-4ca7-8471-0a0919907b7f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "print(f\"PyTorch has version {torch.__version__} with cuda {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgnNePtAcNJu"
   },
   "source": [
    "• Download necessary packages for PyG. \n",
    "\n",
    "• ensure your version of torch matches output from cell above. \n",
    "\n",
    "• In case of any issues, more information may be found on [PyG's installation page](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fS00k2JAKAzR",
    "outputId": "6d88f1c4-b8b5-4f1f-80c9-f0deb7e8ce87"
   },
   "source": [
    "!pip3 install torch==1.12.1+cu102 torchvision==0.13.1+cu102 torchaudio==0.12.1 torchtext --extra-index-url https://download.pytorch.org/whl/cu102\n",
    "\n",
    "!pip install https://data.pyg.org/whl/torch-1.12.0%2Bcu102/torch_cluster-1.6.0%2Bpt112cu102-cp37-cp37m-linux_x86_64.whl\n",
    "\n",
    "!pip install https://data.pyg.org/whl/torch-1.12.0%2Bcu102/torch_scatter-2.1.0%2Bpt112cu102-cp37-cp37m-linux_x86_64.whl\n",
    "\n",
    "!pip install https://data.pyg.org/whl/torch-1.12.0%2Bcu102/torch_sparse-0.6.16%2Bpt112cu102-cp37-cp37m-linux_x86_64.whl\n",
    "\n",
    "!pip install torch-geometric\n",
    "\n",
    "!pip install matplotlib\n",
    "\n",
    "!pip install networkx\n",
    "\n",
    "\n",
    "# Dataset Preparation\n",
    "!cd /home/admin1/Desktop/gnndataset/datasets/WaterDrop/\n",
    "\n",
    "# metadata.json\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1o6cKxgbnfUUFPTX1JngBzB928w2bUIwk' -O metadata.json\n",
    "\n",
    "# test_offset.json\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vr4JiVliKCQNWVV4kziyusxNVUvQuAYL' -O test_offset.json\n",
    "\n",
    "# test_particle_type.dat\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Z_r9ivdKqKZzVJG80gb2uY6JDVRd0wAt' -O test_particle_type.dat\n",
    "\n",
    "# test_position.dat\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1wCeBz1pZ5hxmlqWw4eylajg6pzFgQjIJ' -O test_position.dat\n",
    "\n",
    "# train_offset.json\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=160wnp9PEc1HuzsBi7kO0ryMu3tnon2tI' -O train_offset.json\n",
    "\n",
    "# train_particle_type.dat\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1LVtGLld7assF4sPk0mF2Bz2F7FBaxU0O' -O train_particle_type.dat\n",
    "\n",
    "# train_position.dat\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1YCXcir_fmJZLvXkbPjchsrr8VuuWugH0' -O train_position.dat\n",
    "\n",
    "# valid_offset.json\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1tiDP5uHMJQDTNxyRNSb6sEZCWAADPu8a' -O valid_offset.json\n",
    "\n",
    "# valid_particle_type.dat\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1fXIw9RWM0xzfK2sGn1H0DaAOxzm59ZEd' -O valid_particle_type.dat\n",
    "\n",
    "# valid_position.dat\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1U9QuV3Ra0E1tDD1HgXYCYyn4SeLKXQGs' -O valid_position.dat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z4HbK-OPUK8"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "• Dataset WaterDropSmall includes 100 videos of dropping water to ground rendered in a particle-based physics simulator. \n",
    "\n",
    "• It is a cropped version of WaterDrop dataset by Deepmind. \n",
    "\n",
    "• will download this dataset from Google Cloud stoarge to folder `temp/datasets` in file system. \n",
    "\n",
    "• may inspect downloaded files on **Files** menu on left of this Colab.\n",
    "\n",
    "`metadata.json` file in dataset includes following information:\n",
    "1. sequence length of each video data point\n",
    "2. dimensionality, 2d or 3d\n",
    "3. box bounds - specify bounding box for scene\n",
    "4. default connectivity radius - defines size of each particle's neighborhood\n",
    "5. statistics for normalization e.g. velocity mean and standard deviation and acceleration of particles\n",
    "\n",
    "\n",
    "Each data point in dataset includes following information:\n",
    "1. Particle type, such as water\n",
    "2. particle positions at each frame in video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxevKO1yPb4d",
    "outputId": "d4f3df44-fc86-4822-bb36-ca383dfc235d"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import inspect\n",
    "import os\n",
    "\n",
    "# Global flags to enable/disable debugging and verbosity\n",
    "DEBUG_ENABLED = True\n",
    "VERBOSE_ENABLED = False\n",
    "\n",
    "# Global dictionary to store logged headers and their counts\n",
    "logged_header_counts = {}\n",
    "folders_created = []  # Initialize an empty list\n",
    "\n",
    "def debug_log(theVariable, functionName=None, ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName=None):\n",
    "    \n",
    "    print(\"#################\")\n",
    "    print(\"## theVariable ##\")\n",
    "    print(\"#################\")\n",
    "    print(theVariable)\n",
    "    print(\"\")    \n",
    "    \n",
    "    global logged_header_counts  # Access the global dictionary\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    frame = inspect.currentframe().f_back\n",
    "    variable_names = [name for name, val in frame.f_locals.items() if val is theVariable]\n",
    "    theVariableName = variable_names[0] if variable_names else ExplicitVariableName\n",
    "\n",
    "    print(\"#####################\")\n",
    "    print(\"## theVariableName ##\")\n",
    "    print(\"#####################\")\n",
    "    print(theVariableName)\n",
    "    print(\"\")    \n",
    "    \n",
    "    thefilename = ''\n",
    "    if functionName is not None:\n",
    "        functionName = functionName.replace(\"\\\\\", \"_\")\n",
    "\n",
    "    if DEBUG_ENABLED:\n",
    "        \n",
    "        if ExplicitVariableName is None:\n",
    "            # INCLUDE functionName\n",
    "            if functionName:\n",
    "                header = f\"{theVariableName} - {functionName}\"\n",
    "                log_message = f\"{timestamp} {header}\\n\"\n",
    "                thefilename = header\n",
    "            else:\n",
    "                # EXCLUDE functionName\n",
    "                header = f\"{theVariableName}\"\n",
    "                log_message = f\"{timestamp} {header}\\n\"\n",
    "                thefilename = header\n",
    "        else:\n",
    "                header = ExplicitVariableName\n",
    "                log_message = f\"{timestamp} {header}\\n\"\n",
    "                thefilename = header\n",
    "\n",
    "\n",
    "        print(\"############\")\n",
    "        print(\"## header ##\")\n",
    "        print(\"############\")\n",
    "        print(header)\n",
    "        print(\"\")\n",
    "\n",
    "            \n",
    "        # Check if the header has been logged less than 2 times\n",
    "        if header in logged_header_counts:\n",
    "            if logged_header_counts[header] >= 1:\n",
    "            # if logged_header_counts[header] >= 2:\n",
    "                return  # Skip logging if the header has been logged twice\n",
    "            else:\n",
    "                logged_header_counts[header] += 1  # Increment the count\n",
    "        else:\n",
    "            logged_header_counts[header] = 1  # Add new header to the dictionary with a count of 1\n",
    "\n",
    "        if ShowShape:\n",
    "            log_message += \"Shape:\" + str(theVariable.shape) + \"\\n\"\n",
    "        if ShowLength:\n",
    "            if isinstance(theVariable, torch.Tensor):\n",
    "                if theVariable.dim() == 1:\n",
    "                    log_message += \"Length:\" + str(len(theVariable)) + \"\\n\"\n",
    "                else:\n",
    "                    length = theVariable.numel()\n",
    "                    log_message += \"Length:\" + str(length) + \"\\n\"\n",
    "            else:\n",
    "                log_message += \"Length:\" + str(len(theVariable)) + \"\\n\"\n",
    "        if ShowType:\n",
    "            if isinstance(theVariable, torch.Tensor):\n",
    "                log_message += \"Type:\" + str(theVariable.dtype) + \"\\n\"\n",
    "            else:\n",
    "                log_message += \"Type:\" + str(type(theVariable)) + \"\\n\"\n",
    "\n",
    "        # VARIABLE CONTENTS\n",
    "        log_message += str(theVariable) + \"\\n\"\n",
    "\n",
    "        # Create an image\n",
    "        global imageindex\n",
    "        imageindex = imageindex + 1\n",
    "        thefilename = thefilename.replace(\"\\\\\", \"_\")\n",
    "        text = log_message\n",
    "        # Create the folder if it does not exist\n",
    "        print(\"##################\")\n",
    "        print(\"## functionName ##\")\n",
    "        print(\"##################\")\n",
    "        print(functionName)\n",
    "        print(\"\")\n",
    "\n",
    "        print(\"#################\")\n",
    "        print(\"## thefilename ##\")\n",
    "        print(\"#################\")\n",
    "        print(thefilename)\n",
    "        print(\"\")\n",
    "\n",
    "        # Create Folder if it does nto exist\n",
    "        os.makedirs(\"outputpng\", exist_ok=True)\n",
    "        os.makedirs(r\"outputpng/\" + str(functionName), exist_ok=True)\n",
    "        \n",
    "        folder_path = str(functionName)\n",
    "        # Append the string only if it doesn't already exist in the list\n",
    "        if folder_path not in folders_created:\n",
    "            folders_created.append(folder_path)\n",
    "            \n",
    "        output_file = os.path.join(r\"outputpng\", str(functionName), f\"{imageindex:07d}{thefilename}.png\")\n",
    "        # output_file = os.path.join(\"outputpng\\\\\" + str(functionName), f\"{imageindex:07d}{thefilename}.png\")\n",
    "        text_to_image_function(text, 16, output_file, \"/usr/share/fonts/truetype/freefont/Arial.ttf\")\n",
    "\n",
    "        log_message += \"---------------------------------------------------------\" + \"\\n\"\n",
    "\n",
    "        # Get the current date and time\n",
    "        current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "        # Write to log file\n",
    "        with open(f'debugGNN_{current_date}.txt', 'a') as file:\n",
    "            file.write(log_message)\n",
    "\n",
    "    if VERBOSE_ENABLED:\n",
    "        print(timestamp)\n",
    "        \n",
    "        if ShowShape:\n",
    "            print(\"Shape:\", theVariable.shape)\n",
    "        if ShowLength:\n",
    "            if isinstance(theVariable, torch.Tensor):\n",
    "                if theVariable.dim() == 1:\n",
    "                    print(\"Length:\", str(len(theVariable)))\n",
    "                else:\n",
    "                    length = theVariable.numel()\n",
    "                    print(\"Length:\", str(length))\n",
    "            else:\n",
    "                print(\"Length:\", str(len(theVariable)))                    \n",
    "        if ShowType:\n",
    "            if isinstance(theVariable, torch.Tensor):\n",
    "                print(\"Type:\", str(theVariable.dtype))  \n",
    "            else:\n",
    "                print(\"Type:\", str(type(theVariable))) \n",
    "\n",
    "        # VARIABLE CONTENTS                \n",
    "        if functionName:\n",
    "            print('#' * len(\"## \" + theVariableName + ' ## ' + functionName + \" ##\"))\n",
    "            print(\"## \" + theVariableName + ' ## ' + functionName + \" ##\")\n",
    "            print('#' * len(\"## \" + theVariableName + ' ## ' + functionName + \" ##\"))            \n",
    "            print(str(theVariable))\n",
    "        else:\n",
    "            print('#' * len(\"## \" + theVariableName + \" ##\"))\n",
    "            print(\"## \" + theVariableName + \" ##\")\n",
    "            print('#' * len(\"## \" + theVariableName + \" ##\"))            \n",
    "            print(str(theVariable))\n",
    "\n",
    "print(logged_header_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxevKO1yPb4d",
    "outputId": "d4f3df44-fc86-4822-bb36-ca383dfc235d"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import inspect\n",
    "# Global flags to enable/disable debugging and verbosity\n",
    "DEBUG_ENABLED = True\n",
    "VERBOSE_ENABLED = False\n",
    "\n",
    "def debug_log_old(theVariable, functionName=None, ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName=None):\n",
    "    \n",
    "    print(\"#################\")\n",
    "    print(\"## theVariable ##\")\n",
    "    print(\"#################\")\n",
    "    print(theVariable)\n",
    "    print(\"\")\n",
    "\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    frame = inspect.currentframe().f_back\n",
    "    variable_names = [name for name, val in frame.f_locals.items() if val is theVariable]\n",
    "    # theVariableName = variable_names[0]\n",
    "    theVariableName = variable_names[0] if variable_names else ExplicitVariableName\n",
    "\n",
    "    print(\"#####################\")\n",
    "    print(\"## theVariableName ##\")\n",
    "    print(\"#####################\")\n",
    "    print(theVariableName)\n",
    "    print(\"\")\n",
    "\n",
    "    \n",
    "#     if theVariableName == \"unknown_variable\":\n",
    "#         frame_info = traceback.extract_stack(limit=2)[0]\n",
    "#         log_message = f\"[{timestamp}] - Variable name unknown in {frame_info.filename} at line {frame_info.lineno}\\n\"\n",
    "#     else:\n",
    "#         log_message = f\"[{timestamp}] - {theVariableName}\\n\"\n",
    "    \n",
    "    thefilename=''\n",
    "    if functionName is not None:\n",
    "        functionName=functionName.replace(\"\\\\\", \"_\")\n",
    "    \n",
    "    if DEBUG_ENABLED:\n",
    "        # INCLUDE functionName\n",
    "        if functionName:\n",
    "            log_message = f\"[{timestamp}] - {theVariableName} - {functionName}\\n\"\n",
    "            thefilename = f\"{theVariableName} - {functionName}\"\n",
    "        else:\n",
    "            # EXCLUDE functionName\n",
    "            log_message = f\"[{timestamp}] - {theVariableName}\\n\"\n",
    "            thefilename = f\"{theVariableName}\"\n",
    "            \n",
    "        if ShowShape:\n",
    "            log_message += \"Shape:\" + str(theVariable.shape) + \"\\n\"\n",
    "        if ShowLength:\n",
    "            if isinstance(theVariable, torch.Tensor):\n",
    "                if theVariable.dim() == 1:\n",
    "                    log_message += \"Length:\" + str(len(theVariable)) + \"\\n\"\n",
    "                else:\n",
    "                    length = theVariable.numel()\n",
    "                    log_message += \"Length:\" + str(length) + \"\\n\"\n",
    "            else:\n",
    "                log_message += \"Length:\" + str(len(theVariable)) + \"\\n\"\n",
    "        if ShowType:\n",
    "            if isinstance(theVariable, torch.Tensor):\n",
    "                log_message += \"Type:\" + str(theVariable.dtype) + \"\\n\"\n",
    "            else:\n",
    "                log_message += \"Type:\" + str(type(theVariable)) + \"\\n\"\n",
    "\n",
    "        # VARIABLE CONTENTS\n",
    "        log_message += str(theVariable) + \"\\n\"\n",
    "\n",
    "        # Create an image\n",
    "        global imageindex\n",
    "        imageindex = imageindex + 1\n",
    "        thefilename = thefilename.replace(\"\\\\\", \"_\")\n",
    "        text = log_message\n",
    "        output_file = f\"{imageindex:07d}{thefilename}.png\"\n",
    "        # text_to_image_function(text, 12, output_file, \"/usr/share/fonts/truetype/freefont/FreeSans.ttf\")\n",
    "        text_to_image_function(text, 16, output_file, \"/usr/share/fonts/truetype/freefont/Arial.ttf\")\n",
    "\n",
    "        \n",
    "        log_message += \"---------------------------------------------------------\" + \"\\n\"\n",
    "            \n",
    "        # Get the current date and time\n",
    "        current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        # with open('debugGNN.txt', 'a') as file:\n",
    "        with open(f'debugGNN_{current_date}.txt', 'a') as file:\n",
    "            file.write(log_message)\n",
    "\n",
    "    \n",
    "    if VERBOSE_ENABLED:\n",
    "        print(timestamp)\n",
    "        \n",
    "        if ShowShape:\n",
    "            print(\"Shape:\", theVariable.shape)\n",
    "        if ShowLength:\n",
    "            if isinstance(theVariable, torch.Tensor):\n",
    "                if theVariable.dim() == 1:\n",
    "                    print(\"Length:\", str(len(theVariable)))\n",
    "                else:\n",
    "                    length = theVariable.numel()\n",
    "                    print(\"Length:\", str(length))\n",
    "            else:\n",
    "                print(\"Length:\", str(len(theVariable)))                    \n",
    "        if ShowType:\n",
    "            if isinstance(theVariable, torch.Tensor):\n",
    "                print(\"Type:\", str(theVariable.dtype))  \n",
    "            else:\n",
    "                print(\"Type:\", str(type(theVariable))) \n",
    "\n",
    "        # VARIABLE CONTENTS                \n",
    "        if functionName:\n",
    "            print('#' * len(\"## \" + theVariableName + ' ## ' + functionName + \" ##\"))\n",
    "            print(\"## \" + theVariableName + ' ## ' + functionName + \" ##\")\n",
    "            print('#' * len(\"## \" + theVariableName + ' ## ' + functionName + \" ##\"))            \n",
    "            print(str(theVariable))\n",
    "        else:\n",
    "            print('#' * len(\"## \" + theVariableName + \" ##\"))\n",
    "            print(\"## \" + theVariableName + \" ##\")\n",
    "            print('#' * len(\"## \" + theVariableName + \" ##\"))            \n",
    "            print(str(theVariable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxevKO1yPb4d",
    "outputId": "d4f3df44-fc86-4822-bb36-ca383dfc235d"
   },
   "outputs": [],
   "source": [
    "# Example Usage:\n",
    "abc = 123\n",
    "debug_log(\"abc1\", \"NoFunctionaabbcc\", ShowShape=False,ShowLength=False,ShowType=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxevKO1yPb4d",
    "outputId": "d4f3df44-fc86-4822-bb36-ca383dfc235d"
   },
   "outputs": [],
   "source": [
    "debug_log(\"abc2\", \"NoFunctionaabbcc\", ShowShape=False,ShowLength=False,ShowType=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxevKO1yPb4d",
    "outputId": "d4f3df44-fc86-4822-bb36-ca383dfc235d"
   },
   "outputs": [],
   "source": [
    "debug_log(\"abc3\", \"NoFunctionaabbcc\", ShowShape=False,ShowLength=False,ShowType=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxevKO1yPb4d",
    "outputId": "d4f3df44-fc86-4822-bb36-ca383dfc235d"
   },
   "outputs": [],
   "source": [
    "print(logged_header_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxevKO1yPb4d",
    "outputId": "d4f3df44-fc86-4822-bb36-ca383dfc235d"
   },
   "outputs": [],
   "source": [
    "def debug_log_special(var):\n",
    "    # Use inspect to find the variable name in the caller's frame\n",
    "    frame = inspect.currentframe()\n",
    "    try:\n",
    "        caller_locals = frame.f_back.f_locals\n",
    "        var_name = [name for name, value in caller_locals.items() if value is var]\n",
    "        var_name = var_name[0] if var_name else \"unknown\"\n",
    "    finally:\n",
    "        del frame  # Clean up the frame to avoid reference cycles\n",
    "\n",
    "    # Print the variable name and its content\n",
    "    print(f\"{var_name}: {var}\")    \n",
    "    \n",
    "    with open('debugGNN1.txt', 'a') as file:\n",
    "        file.write(f\"{var_name}: {var}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxevKO1yPb4d",
    "outputId": "d4f3df44-fc86-4822-bb36-ca383dfc235d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "print(f\"PyTorch has version {torch.__version__} with cuda {torch.version.cuda}\")\n",
    "\n",
    "DATASET_NAME = \"WaterDrop\"\n",
    "OUTPUT_DIR = os.path.join(\"/home/admin1/Desktop/GNN/gnndataset/datasets/WaterDrop\")\n",
    "\n",
    "debug_log(DATASET_NAME, ShowShape=False, ShowLength=False, ShowType=False)\n",
    "\n",
    "debug_log(OUTPUT_DIR, ShowShape=False, ShowLength=False, ShowType=False)\n",
    "\n",
    "# BASE_URL = f\"https://storage.googleapis.com/cs224w_course_project_dataset/{DATASET_NAME}\"\n",
    "\n",
    "# !mkdir -p \"$OUTPUT_DIR\"\n",
    "\n",
    "# META_DATA_PATH = f\"{OUTPUT_DIR}/metadata.json\"\n",
    "# CLOUD_PATH = f\"{BASE_URL}/metadata.json\"\n",
    "# !wget -O \"$META_DATA_PATH\" \"$CLOUD_PATH\"\n",
    "# for split in [\"test\", \"train\", \"valid\"]:\n",
    "#   for suffix in [\"offset.json\", \"particle_type.dat\", \"position.dat\"]:\n",
    "#       DATA_PATH = f\"{OUTPUT_DIR}/{split}_{suffix}\"\n",
    "#       CLOUD_PATH = f\"{BASE_URL}/{split}_{suffix}\"\n",
    "#       !wget -O \"$DATA_PATH\" \"$CLOUD_PATH\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NW0_YsEPG68T"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "• Cannot apply raw data in dataset to train GNN model directly, so must perform below steps to convert raw data into graphs with descriptive node features and edge features:\n",
    "1. Apply noise to trajectory to have more diverse training examples\n",
    "1. Construct graph based on distance between particles\n",
    "1. Extract node-level features: particle velocities and their distance to boundary\n",
    "1. Extract edge-level features: displacement and distance between particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCy3zaaOGrrS"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch_geometric as pyg\n",
    "\n",
    "def generate_noise(position_seq, noise_std):\n",
    "    \"\"\"Generate noise for a trajectory\"\"\"\n",
    "    velocity_seq = position_seq[:, 1:] - position_seq[:, :-1]\n",
    "    debug_log(velocity_seq, \"generate_noise\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"velocity_seq-generate_noise\")\n",
    "\n",
    "\n",
    "    time_steps = velocity_seq.size(1)\n",
    "    debug_log(time_steps, \"generate_noise\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"time_steps-generate_noise\")\n",
    "    \n",
    "    velocity_noise = torch.randn_like(velocity_seq) * (noise_std / time_steps ** 0.5)\n",
    "    debug_log(velocity_noise, \"generate_noise\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"velocity-noise1_generate_noise\")\n",
    "    \n",
    "    velocity_noise = velocity_noise.cumsum(dim=1)\n",
    "    debug_log(velocity_noise, \"generate_noise\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"velocity-noise2_generate_noise\")    \n",
    "    \n",
    "    position_noise = velocity_noise.cumsum(dim=1)\n",
    "    debug_log(position_noise, \"generate_noise\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"position-noise1_generate_noise\")\n",
    "       \n",
    "    position_noise = torch.cat((torch.zeros_like(position_noise)[:, 0:1], position_noise), dim=1)\n",
    "    debug_log(position_noise, \"generate_noise\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"position-noise2_generate_noise\")\n",
    "    \n",
    "    return position_noise\n",
    "\n",
    "\n",
    "def preprocess(particle_type, position_seq, target_position, metadata, noise_std):\n",
    "    \"\"\"Preprocess a trajectory and construct graph\"\"\"\n",
    "    # apply noise to trajectory\n",
    "    position_noise = generate_noise(position_seq, noise_std)\n",
    "    debug_log(position_noise, \"preprocess\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"position_noise-preprocess\")\n",
    "    \n",
    "    position_seq = position_seq + position_noise\n",
    "    debug_log(position_seq, \"preprocess\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"position_seq-preprocess\")\n",
    "\n",
    "    # calculate velocities of particles\n",
    "    recent_position = position_seq[:, -1]\n",
    "    debug_log(recent_position, \"preprocess\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"recent_position-preprocess\")\n",
    "    \n",
    "    \n",
    "    velocity_seq = position_seq[:, 1:] - position_seq[:, :-1]\n",
    "    debug_log(velocity_seq, \"preprocess\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"velocity_seq-preprocess\")\n",
    "    \n",
    "    \n",
    "    # construct graph based on distances between particles\n",
    "    n_particle = recent_position.size(0)\n",
    "    debug_log(n_particle, \"preprocess\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"n_particle-preprocess\")\n",
    "    \n",
    "    \n",
    "    edge_index = pyg.nn.radius_graph(recent_position, metadata[\"default_connectivity_radius\"], loop=True, max_num_neighbors=n_particle)\n",
    "    debug_log(edge_index, \"preprocess\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"edge_index-preprocess\")\n",
    "\n",
    "    \n",
    "    # node-level features: velocity, distance to boundary\n",
    "    normal_velocity_seq = (velocity_seq - torch.tensor(metadata[\"vel_mean\"])) / torch.sqrt(torch.tensor(metadata[\"vel_std\"]) ** 2 + noise_std ** 2)\n",
    "    debug_log(normal_velocity_seq, \"preprocess\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"normal_velocity_seq-preprocess\")\n",
    "    \n",
    "    boundary = torch.tensor(metadata[\"bounds\"])\n",
    "    debug_log(boundary, \"preprocess\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"boundary-preprocess\")\n",
    "        \n",
    "    distance_to_lower_boundary = recent_position - boundary[:, 0]\n",
    "    debug_log(distance_to_lower_boundary, \"preprocess\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"distance_to_lower_boundary-preprocess\")\n",
    "        \n",
    "    distance_to_upper_boundary = boundary[:, 1] - recent_position\n",
    "    debug_log(distance_to_upper_boundary, \"preprocess\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"distance_to_upper_boundary-preprocess\")\n",
    "    \n",
    "    distance_to_boundary = torch.cat((distance_to_lower_boundary, distance_to_upper_boundary), dim=-1)\n",
    "    debug_log(distance_to_boundary, \"preprocess\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"distance_to_boundary1-preprocess\")    \n",
    "    \n",
    "    \n",
    "    distance_to_boundary = torch.clip(distance_to_boundary / metadata[\"default_connectivity_radius\"], -1.0, 1.0)\n",
    "    debug_log(distance_to_boundary, \"preprocess\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"distance_to_boundary2-preprocess\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # edge-level features: displacement, distance\n",
    "    dim = recent_position.size(-1)\n",
    "    debug_log(dim, \"preprocess\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"dim-preprocess\")\n",
    "    \n",
    "    edge_displacement = (torch.gather(recent_position, dim=0, index=edge_index[0].unsqueeze(-1).expand(-1, dim)) - torch.gather(recent_position, dim=0, index=edge_index[1].unsqueeze(-1).expand(-1, dim)))\n",
    "    debug_log(edge_displacement, \"preprocess\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"edge_displacement1-preprocess\")    \n",
    "    \n",
    "    \n",
    "    edge_displacement /= metadata[\"default_connectivity_radius\"]\n",
    "    debug_log(edge_displacement, \"preprocess\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"edge_displacement2-preprocess\")\n",
    "    \n",
    "    \n",
    "    edge_distance = torch.norm(edge_displacement, dim=-1, keepdim=True)\n",
    "    debug_log(edge_distance, \"preprocess\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"edge_distance-preprocess\")    \n",
    "\n",
    "\n",
    "    \n",
    "    # ground truth for training\n",
    "    if target_position is not None:\n",
    "        last_velocity = velocity_seq[:, -1]\n",
    "        debug_log(last_velocity, \"preprocess\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"last_velocity-preprocess\")        \n",
    "       \n",
    "        \n",
    "        next_velocity = target_position + position_noise[:, -1] - recent_position\n",
    "        debug_log(next_velocity, \"preprocess\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"next_velocity-preprocess\")        \n",
    "        \n",
    "        \n",
    "        \n",
    "        acceleration = next_velocity - last_velocity\n",
    "        debug_log(acceleration, \"preprocess\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"acceleration1-preprocess\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        acceleration = (acceleration - torch.tensor(metadata[\"acc_mean\"])) / torch.sqrt(torch.tensor(metadata[\"acc_std\"]) ** 2 + noise_std ** 2)\n",
    "        debug_log(acceleration, \"preprocess\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"acceleration2-preprocess\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        acceleration = None\n",
    "\n",
    "    # return graph with features\n",
    "    graph = pyg.data.Data(\n",
    "        x=particle_type,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=torch.cat((edge_displacement, edge_distance), dim=-1),\n",
    "        y=acceleration,\n",
    "        pos=torch.cat((velocity_seq.reshape(velocity_seq.size(0), -1), distance_to_boundary), dim=-1)\n",
    "    )\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqfx4NcguDEY"
   },
   "source": [
    "### One Step Dataset\n",
    "\n",
    "• Each datapoint in this dataset contains trajectories sliced to short time windows. \n",
    "\n",
    "• We use this dataset in training phase because history of particles' states are necessary for model to make predictions. \n",
    "\n",
    "• But in meantime, since long-horizon prediction is inaccurate and time-consuming, sliced trajectories to short time windows to improve perfomance of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2HrUjPnsF_4"
   },
   "outputs": [],
   "source": [
    "class OneStepDataset(pyg.data.Dataset):\n",
    "    def __init__(self, data_path, split, window_length=7, noise_std=0.0, return_pos=False):\n",
    "        super().__init__()\n",
    "\n",
    "        debug_log(data_path, \"OneStepDataset\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"data_path-OneStepDataset\")\n",
    "\n",
    "        \n",
    "        # load dataset from disk\n",
    "        with open(os.path.join(data_path, \"metadata.json\")) as f:\n",
    "            self.metadata = json.load(f)\n",
    "            debug_log(self.metadata, \"OneStepDataset\", ShowShape=False, ShowLength=True, ShowType=True, ExplicitVariableName = \"self.metadata-OneStepDataset\")\n",
    "            \n",
    "        with open(os.path.join(data_path, f\"{split}_offset.json\")) as f:\n",
    "            self.offset = json.load(f)\n",
    "            # debug_log(self.offset, \"OneStepDataset 1\", ShowShape=False, ShowLength=True, ShowType=True)        \n",
    "            debug_log(self.offset, \"OneStepDataset 1\", ShowShape=False, ShowLength=True, ShowType=True, ExplicitVariableName = \"self.offset1-OneStepDataset\")\n",
    "            \n",
    "            \n",
    "        self.offset = {int(k): v for k, v in self.offset.items()}\n",
    "        # debug_log(self.offset, \"OneStepDataset 2\", ShowShape=False, ShowLength=True, ShowType=True)        \n",
    "        debug_log(self.offset, \"OneStepDataset 2\", ShowShape=False, ShowLength=True, ShowType=True, ExplicitVariableName = \"self.offset2-OneStepDataset\")\n",
    "        \n",
    "        \n",
    "        self.window_length = window_length\n",
    "        debug_log(window_length, \"OneStepDataset\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"window_length-OneStepDataset\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.noise_std = noise_std\n",
    "        debug_log(noise_std, \"OneStepDataset\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"noise_std-OneStepDataset\")\n",
    "\n",
    "        \n",
    "        self.return_pos = return_pos\n",
    "        debug_log(return_pos, \"OneStepDataset\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"return_pos-OneStepDataset\")\n",
    "        \n",
    "        \n",
    "\n",
    "        self.particle_type = np.memmap(os.path.join(data_path, f\"{split}_particle_type.dat\"), dtype=np.int64, mode=\"r\")\n",
    "        # debug_log(self.particle_type, \"OneStepDataset\", ShowShape=True, ShowLength=True, ShowType=True)\n",
    "        debug_log(self.particle_type, \"OneStepDataset\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"self.particle_type-OneStepDataset\")\n",
    "        \n",
    "        self.position = np.memmap(os.path.join(data_path, f\"{split}_position.dat\"), dtype=np.float32, mode=\"r\")\n",
    "        # debug_log(self.position, \"OneStepDataset\", ShowShape=True, ShowLength=True, ShowType=True)\n",
    "        debug_log(self.position, \"OneStepDataset\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"self.position-OneStepDataset\")\n",
    "        \n",
    "        for traj in self.offset.values():\n",
    "            self.dim = traj[\"position\"][\"shape\"][2]\n",
    "            # debug_log(self.dim, \"OneStepDataset\", ShowShape=False, ShowLength=False, ShowType=True)\n",
    "            debug_log(self.dim, \"OneStepDataset\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"self.dim-OneStepDataset\")\n",
    "            \n",
    "            break\n",
    "\n",
    "        # cut particle trajectories according to time slices\n",
    "        self.windows = []\n",
    "        for traj in self.offset.values():\n",
    "            size = traj[\"position\"][\"shape\"][1]\n",
    "            debug_log(size, \"OneStepDataset\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"size-OneStepDataset\")\n",
    "            \n",
    "            length = traj[\"position\"][\"shape\"][0] - window_length + 1\n",
    "            debug_log(length, \"OneStepDataset\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"length-OneStepDataset\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            for i in range(length):\n",
    "                desc = {\n",
    "                    \"size\": size,\n",
    "                    \"type\": traj[\"particle_type\"][\"offset\"],\n",
    "                    \"pos\": traj[\"position\"][\"offset\"] + i * size * self.dim,\n",
    "                }\n",
    "                self.windows.append(desc)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.windows)\n",
    "\n",
    "    def get(self, idx):\n",
    "        # load corresponding data for this time slice\n",
    "        window = self.windows[idx]\n",
    "        debug_log(window, \"get\", ShowShape=False, ShowLength=True, ShowType=True, ExplicitVariableName = \"window-get\")\n",
    "        \n",
    "        \n",
    "        size = window[\"size\"]\n",
    "        debug_log(size, \"get\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"size-get\")\n",
    "        \n",
    "        \n",
    "        particle_type = self.particle_type[window[\"type\"]: window[\"type\"] + size].copy()\n",
    "        debug_log(particle_type, \"get\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"particle_type1-get\")\n",
    "                \n",
    "        \n",
    "        particle_type = torch.from_numpy(particle_type)\n",
    "        debug_log(particle_type, \"get\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"particle_type2-get\")\n",
    "        \n",
    "        \n",
    "        position_seq = self.position[window[\"pos\"]: window[\"pos\"] + self.window_length * size * self.dim].copy()\n",
    "        debug_log(position_seq, \"get\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"position_seq1-get\")\n",
    "        \n",
    "        position_seq.resize(self.window_length, size, self.dim)\n",
    "        debug_log(position_seq, \"get\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"position_seq2-get\")\n",
    "        \n",
    "        position_seq = position_seq.transpose(1, 0, 2)\n",
    "        debug_log(position_seq, \"get\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"position_seq3-get\")\n",
    "        \n",
    "        target_position = position_seq[:, -1]\n",
    "        debug_log(target_position, \"get\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"target_position1-get\")\n",
    "        \n",
    "        \n",
    "        position_seq = position_seq[:, :-1]\n",
    "        debug_log(position_seq, \"get\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"position_seq4-get\")\n",
    "        \n",
    "        target_position = torch.from_numpy(target_position)\n",
    "        debug_log(target_position, \"get\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"target_position2-get\")\n",
    "        \n",
    "        \n",
    "        position_seq = torch.from_numpy(position_seq)\n",
    "        debug_log(position_seq, \"get\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"position_seq5-get\")\n",
    "        \n",
    "\n",
    "        # construct graph\n",
    "        with torch.no_grad():\n",
    "            graph = preprocess(particle_type, position_seq, target_position, self.metadata, self.noise_std)\n",
    "        if self.return_pos:\n",
    "            return graph, position_seq[:, -1]\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqhPcZeKthYq"
   },
   "source": [
    "### Rollout Dataset\n",
    "\n",
    "• Each datapoint in this dataset contains trajectories of particles over 1000 time frames. \n",
    "\n",
    "• This dataset used in evaluation phase to measure model's ability to make long-horizon predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kuk2Z-I8sFv7"
   },
   "outputs": [],
   "source": [
    "class RolloutDataset(pyg.data.Dataset):\n",
    "    def __init__(self, data_path, split, window_length=7):\n",
    "        super().__init__()\n",
    "\n",
    "        # load data from disk\n",
    "        with open(os.path.join(data_path, \"metadata.json\")) as f:\n",
    "            self.metadata = json.load(f)\n",
    "            # debug_log(self.metadata, \"RolloutDataset\\_init_\", ShowShape=False, ShowLength=True, ShowType=True)\n",
    "            debug_log(self.metadata, \"RolloutDataset\\_init_\", ShowShape=False, ShowLength=True, ShowType=True, ExplicitVariableName = \"self.metadata-RolloutDataset\\_init_\")\n",
    "            \n",
    "        with open(os.path.join(data_path, f\"{split}_offset.json\")) as f:\n",
    "            self.offset = json.load(f)\n",
    "            # debug_log(self.offset, \"RolloutDataset\\_init_\", ShowShape=False, ShowLength=True, ShowType=True)\n",
    "            debug_log(self.offset, \"RolloutDataset\\_init_\", ShowShape=False, ShowLength=True, ShowType=True, ExplicitVariableName = \"self.offset-RolloutDataset1\\_init_\")\n",
    "            \n",
    "        self.offset = {int(k): v for k, v in self.offset.items()}\n",
    "        # debug_log(self.offset, \"RolloutDataset\\_init_\", ShowShape=False, ShowLength=True, ShowType=True)\n",
    "        debug_log(self.offset, \"RolloutDataset\\_init_\", ShowShape=False, ShowLength=True, ShowType=True, ExplicitVariableName = \"self.offset-RolloutDataset2\\_init_\")\n",
    "        \n",
    "        self.window_length = window_length\n",
    "        debug_log(window_length, \"RolloutDataset\\_init_\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"window_length-RolloutDataset\\_init_\")\n",
    "        \n",
    "        \n",
    "\n",
    "        self.particle_type = np.memmap(os.path.join(data_path, f\"{split}_particle_type.dat\"), dtype=np.int64, mode=\"r\")\n",
    "        # debug_log(self.particle_type, \"RolloutDataset\\_init_\", ShowShape=True, ShowLength=True, ShowType=True)\n",
    "        debug_log(self.particle_type, \"RolloutDataset\\_init_\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"self.particle_type-RolloutDataset\\_init_\")\n",
    "        \n",
    "        self.position = np.memmap(os.path.join(data_path, f\"{split}_position.dat\"), dtype=np.float32, mode=\"r\")\n",
    "        # debug_log(self.position, \"RolloutDataset\\_init_\", ShowShape=True, ShowLength=True, ShowType=True)\n",
    "        debug_log(self.position, \"RolloutDataset\\_init_\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"self.position-RolloutDataset\\_init_\")\n",
    "        \n",
    "        for traj in self.offset.values():\n",
    "            self.dim = traj[\"position\"][\"shape\"][2]\n",
    "            break\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.offset)\n",
    "\n",
    "    def get(self, idx):\n",
    "        traj = self.offset[idx]\n",
    "        debug_log(traj, \"RolloutDataset\\get\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"traj-RolloutDataset\\get\")\n",
    "        \n",
    "        size = traj[\"position\"][\"shape\"][1]\n",
    "        debug_log(size, \"RolloutDataset\\get\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"size-RolloutDataset\\get\")\n",
    "        \n",
    "        \n",
    "        time_step = traj[\"position\"][\"shape\"][0]\n",
    "        debug_log(time_step, \"RolloutDataset\\get\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"time_step-RolloutDataset\\get\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        particle_type = self.particle_type[traj[\"particle_type\"][\"offset\"]: traj[\"particle_type\"][\"offset\"] + size].copy()\n",
    "        debug_log(particle_type, \"RolloutDataset\\get\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"particle_type-RolloutDataset1\\get\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        particle_type = torch.from_numpy(particle_type)\n",
    "        debug_log(particle_type, \"RolloutDataset\\get\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"particle_type-RolloutDataset2\\get\")\n",
    "        \n",
    "        \n",
    "        position = self.position[traj[\"position\"][\"offset\"]: traj[\"position\"][\"offset\"] + time_step * size * self.dim].copy()\n",
    "        debug_log(position, \"RolloutDataset\\get\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"position-RolloutDataset1\\get\")\n",
    "                \n",
    "        \n",
    "        position.resize(traj[\"position\"][\"shape\"])\n",
    "        debug_log(position, \"RolloutDataset\\get\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"position-RolloutDataset2\\get\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        position = torch.from_numpy(position)\n",
    "        debug_log(position, \"RolloutDataset\\get\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"position-RolloutDataset3\\get\")\n",
    "        \n",
    "        \n",
    "        data = {\"particle_type\": particle_type, \"position\": position}\n",
    "        debug_log(data, \"RolloutDataset\\get\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"data-RolloutDataset\\get\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVjrldn4kD-P"
   },
   "source": [
    "### Visualize a graph in dataset\n",
    "\n",
    "• Each data point in dataset is a `pyg.data.Data` object which describes a graph. \n",
    "\n",
    "• explain contents of first data point, visualize graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 656
    },
    "id": "_4MJuOhjkTjx",
    "outputId": "6ef28c82-9055-43d5-ec66-19b70e8a3ddf"
   },
   "outputs": [],
   "source": [
    "#!pip install numpy==1.23\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nlnk5URCF7pZ"
   },
   "source": [
    "## GNN Model\n",
    "\n",
    "We will walk through implementation of GNN model in this section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vsVI6epJcsn"
   },
   "source": [
    "### Helper class\n",
    "\n",
    "• first define a class for Multi-Layer Perceptron (MLP). \n",
    "\n",
    "• This class generates an MLP given width and depth of it. \n",
    "\n",
    "• Because MLPs are used in several places of GNN, this helper class will make code cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ht-upXnRo0dV"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch_scatter\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"Multi-Layer perceptron\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, layers, layernorm=True):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        # debug_log(self.layers, \"MLP\\_init_\", ShowShape=False, ShowLength=True, ShowType=True)\n",
    "        debug_log(self.layers, \"MLP\\_init_\", ShowShape=False, ShowLength=True, ShowType=True, ExplicitVariableName = \"self.layers-MLP\\_init_\")\n",
    "        \n",
    "        \n",
    "        for i in range(layers):\n",
    "            self.layers.append(torch.nn.Linear(\n",
    "                input_size if i == 0 else hidden_size,\n",
    "                output_size if i == layers - 1 else hidden_size,\n",
    "            ))\n",
    "            \n",
    "            \n",
    "            if i != layers - 1:\n",
    "                self.layers.append(torch.nn.ReLU())\n",
    "                # debug_log(self.layers, \"MLP\\_init_\\i\", ShowShape=False, ShowLength=True, ShowType=True)\n",
    "                debug_log(self.layers, \"MLP\\_init_\\i\", ShowShape=False, ShowLength=True, ShowType=True, ExplicitVariableName = \"self.layers-MLP\\_init_\\i\")\n",
    "                \n",
    "                \n",
    "        if layernorm:\n",
    "            self.layers.append(torch.nn.LayerNorm(output_size))\n",
    "            # debug_log(self.layers, \"MLP\", ShowShape=False, ShowLength=True, ShowType=True)\n",
    "            debug_log(self.layers, \"MLP\", ShowShape=False, ShowLength=True, ShowType=True, ExplicitVariableName = \"self.layers-MLP\")\n",
    "            \n",
    "            \n",
    "            \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \n",
    "        \n",
    "        for layer in self.layers:\n",
    "            debug_log(layer, \"MLP\\reset_parameters\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"layer-MLP1\\reset_parameters\")\n",
    "            \n",
    "            \n",
    "            if isinstance(layer, torch.nn.Linear):\n",
    "                layer.weight.data.normal_(0, 1 / math.sqrt(layer.in_features))\n",
    "                debug_log(layer, \"MLP\\reset_parameters\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"layer-MLP2\\reset_parameters\")\n",
    "                \n",
    "                layer.bias.data.fill_(0)\n",
    "                debug_log(layer, \"MLP\\reset_parameters\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"layer-MLP3\\reset_parameters\")\n",
    "                \n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_pkzDgqJ_ED"
   },
   "source": [
    "### GNN layers\n",
    "\n",
    "In following code block, we implement one type of GNN layer named `InteractionNetwork` (IN), which is proposed by paper *Interaction Networks for Learning about Objects,\n",
    "Relations and Physics*.\n",
    "\n",
    "• For a graph $G$, let feature of node $i$ be $v_i$, feature of edge $(i, j)$ be $e_{i, j}$. \n",
    "\n",
    "• three stages for IN to generate new features of nodes and edges.\n",
    "\n",
    "1. **Message generation.**\n",
    "\n",
    "• If there is an edge pointing from node $i$ to node $j$, node $i$ sends a message to node $j$. \n",
    "\n",
    "• message carries information of edge and its two nodes, so it is generated by following equation $\\mathrm{Msg}_{i,j} = \\mathrm{MLP}(v_i, v_j, e_{i,j})$.\n",
    "\n",
    "2. **Message aggregation.**\n",
    "\n",
    "• In this stage, each node of graph aggregates all messages it received to a fixed-sized representation. \n",
    "\n",
    "• In IN, aggregation means summing all messages up, i.e., $\\mathrm{Agg}_i=\\sum_{(j,i)\\in G}\\mathrm{Msg}_{i,j}$.\n",
    "\n",
    "3. **Update.**\n",
    "\n",
    "• update features of nodes and edges with results of previous stages. \n",
    "\n",
    "• For each edge, its new feature is sum of its old feature and correspond message, i.e., $e'_{i,j}=e_{i,j}+\\mathrm{Msg}_{i,j}$. \n",
    "\n",
    "• For each node, new feature is determined by its old feature and aggregated message, i.e., $v'_i=v_i+\\mathrm{MLP}(v_i, \\mathrm{Agg}_i)$.\n",
    "\n",
    "• In PyG, GNN layers are implemented as subclass of `MessagePassing`. \n",
    "\n",
    "• must override three critical functions to implement `InteractionNetwork` GNN layer. \n",
    "\n",
    "• Each function corresponds to one stage of GNN layer.\n",
    "\n",
    "1. `message()` -> message generation\n",
    "\n",
    "• This function controls how a message is generated on each edge of graph. \n",
    "\n",
    "• It takes three arguments:\n",
    "\n",
    "• (1) `x_i`, features of source nodes; \n",
    "\n",
    "• (2) `x_j`, features of target nodes; \n",
    "\n",
    "• (3) `edge_feature`, features of edges themselves. \n",
    "\n",
    "• In IN, concatenate all these features and generate messages with an MLP.\n",
    "\n",
    "1. `aggregate()` -> message aggregation\n",
    "\n",
    "• This function aggregates messages for nodes. \n",
    "\n",
    "• It depends on two arguments:\n",
    "\n",
    "• (1) `inputs`, messages; \n",
    "\n",
    "• (2) `index`, graph structure. \n",
    "\n",
    "• handle over task of message aggregation to function `torch_scatter.scatter` and specifies in argument `reduce` that want to sum messages up. \n",
    "\n",
    "• Because want to retain messages themselves to update edge features, return both messages and aggregated messages.\n",
    "\n",
    "1. `forward()` -> update\n",
    "\n",
    "• This function puts everything together. \n",
    "\n",
    "• `x` is node features, `edge_index` is graph structure and `edge_feature` is edge features. \n",
    "\n",
    "• function`MessagePassing.propagate` invokes functions `message` and `aggregate` for us. \n",
    "\n",
    "• Then, update node features and edge features and return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nobE0LcXJ6RR"
   },
   "outputs": [],
   "source": [
    "class InteractionNetwork(pyg.nn.MessagePassing):\n",
    "    \"\"\"Interaction Network as proposed in this paper:\n",
    "    https://proceedings.neurips.cc/paper/2016/hash/3147da8ab4a0437c15ef51a5cc7f2dc4-Abstract.html\"\"\"\n",
    "    def __init__(self, hidden_size, layers):\n",
    "        super().__init__()\n",
    "        self.lin_edge = MLP(hidden_size * 3, hidden_size, hidden_size, layers)\n",
    "        self.lin_node = MLP(hidden_size * 2, hidden_size, hidden_size, layers)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_feature):\n",
    "        edge_out, aggr = self.propagate(edge_index, x=(x, x), edge_feature=edge_feature)\n",
    "        debug_log(edge_out, r\"InteractionNetwork\\forward\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"edge_out1-InteractionNetwork\\forward\")\n",
    "        \n",
    "        node_out = self.lin_node(torch.cat((x, aggr), dim=-1))\n",
    "        debug_log(node_out, r\"InteractionNetwork\\forward\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"node_out1-InteractionNetwork\\forward\")\n",
    "\n",
    "        \n",
    "        \n",
    "        edge_out = edge_feature + edge_out\n",
    "        debug_log(edge_out, r\"InteractionNetwork\\forward\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"edge_out2-InteractionNetwork\\forward\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        node_out = x + node_out\n",
    "        debug_log(node_out, r\"InteractionNetwork\\forward\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"node_out2-InteractionNetwork\\forward\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        return node_out, edge_out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_feature):\n",
    "        x = torch.cat((x_i, x_j, edge_feature), dim=-1)\n",
    "        debug_log(x, \"InteractionNetwork\\message\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"x1-InteractionNetwork\\message\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = self.lin_edge(x)\n",
    "        debug_log(x, \"InteractionNetwork\\message\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"x2-InteractionNetwork\\message\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        return x\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size=None):\n",
    "        out = torch_scatter.scatter(inputs, index, dim=self.node_dim, dim_size=dim_size, reduce=\"sum\")\n",
    "        debug_log(out, \"InteractionNetwork\\aggregate\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"out-InteractionNetwork\\aggregate\")\n",
    "\n",
    "        \n",
    "        return (inputs, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-Aa9znXKH40"
   },
   "source": [
    "### GNN\n",
    "\n",
    "• Now its time to stack GNN layers to a GNN. \n",
    "\n",
    "• Besides GNN layers, pre-processing and post-processing blocks in GNN. \n",
    "\n",
    "• Before GNN layers, input features are transformed by MLP so expressiveness of GNN is improved without increasing GNN layers. \n",
    "\n",
    "• After GNN layers, final outputs (accelerations of particles in case) are extracted from features generated by GNN layers to meet requirement of task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoB4_A6YJ7FP"
   },
   "outputs": [],
   "source": [
    "class LearnedSimulator(torch.nn.Module):\n",
    "    \"\"\"Graph Network-based Simulators(GNS)\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=128,\n",
    "        n_mp_layers=10, # number of GNN layers\n",
    "        num_particle_types=9,\n",
    "        particle_type_dim=16, # embedding dimension of particle types\n",
    "        dim=2, # dimension of world, typical 2D or 3D\n",
    "        window_size=5, # model looks into W frames before frame to be predicted\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        debug_log(window_size, \"LearnedSimulator\\_init_\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"window_size-LearnedSimulator\\_init_\")\n",
    "        \n",
    "        self.embed_type = torch.nn.Embedding(num_particle_types, particle_type_dim)\n",
    "        # debug_log(self.embed_type, \"LearnedSimulator\\_init_\", ShowShape=False, ShowLength=False, ShowType=True)\n",
    "        debug_log(self.embed_type, \"LearnedSimulator\\_init_\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"self.embed_type-LearnedSimulator\\_init_\")\n",
    "        \n",
    "        \n",
    "        self.node_in = MLP(particle_type_dim + dim * (window_size + 2), hidden_size, hidden_size, 3)\n",
    "        # debug_log(self.node_in, \"LearnedSimulator\\_init_\", ShowShape=False, ShowLength=False, ShowType=True)\n",
    "        debug_log(self.node_in, \"LearnedSimulator\\_init_\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"self.node_in-LearnedSimulator\\_init_\")\n",
    "        \n",
    "        \n",
    "        self.edge_in = MLP(dim + 1, hidden_size, hidden_size, 3)\n",
    "        # debug_log(self.node_in, \"LearnedSimulator\\_init_\", ShowShape=False, ShowLength=False, ShowType=True)\n",
    "        debug_log(self.edge_in, \"LearnedSimulator\\_init_\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"self.edge_in-LearnedSimulator\\_init_\")\n",
    "        \n",
    "        \n",
    "        self.node_out = MLP(hidden_size, hidden_size, dim, 3, layernorm=False)\n",
    "        # debug_log(self.node_out, \"LearnedSimulator\\_init_\", ShowShape=False, ShowLength=False, ShowType=True)\n",
    "        debug_log(self.node_out, \"LearnedSimulator\\_init_\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"self.node_out-LearnedSimulator\\_init_\")\n",
    "        \n",
    "        \n",
    "        self.n_mp_layers = n_mp_layers\n",
    "        debug_log(n_mp_layers, \"LearnedSimulator\\_init_\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"n_mp_layers-LearnedSimulator\\_init_\")\n",
    "\n",
    "        \n",
    "        \n",
    "        self.layers = torch.nn.ModuleList([InteractionNetwork(\n",
    "            hidden_size, 3\n",
    "        ) for _ in range(n_mp_layers)])\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.embed_type.weight)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # pre-processing\n",
    "        # node feature: combine categorial feature data.x and contiguous feature data.pos.\n",
    "        node_feature = torch.cat((self.embed_type(data.x), data.pos), dim=-1)\n",
    "        debug_log(node_feature, r\"LearnedSimulator\\forward\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"node_feature1-LearnedSimulator\\forward\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        node_feature = self.node_in(node_feature)\n",
    "        debug_log(node_feature, r\"LearnedSimulator\\forward\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"node_feature2-LearnedSimulator\\forward\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        edge_feature = self.edge_in(data.edge_attr)\n",
    "        debug_log(edge_feature, r\"LearnedSimulator\\forward\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"edge_feature-LearnedSimulator\\forward\")\n",
    "\n",
    "        \n",
    "        \n",
    "        # stack of GNN layers\n",
    "        for i in range(self.n_mp_layers):\n",
    "            node_feature, edge_feature = self.layers[i](node_feature, data.edge_index, edge_feature=edge_feature)\n",
    "            debug_log(node_feature, r\"LearnedSimulator\\forward\\i\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"node_feature-LearnedSimulator\\forward\\i\")\n",
    "            debug_log(edge_feature, r\"LearnedSimulator\\forward\\i\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"edge_feature-LearnedSimulator\\forward\\i\")\n",
    "            \n",
    "        # post-processing\n",
    "        out = self.node_out(node_feature)\n",
    "        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7LUIouahvhW"
   },
   "source": [
    "## Training\n",
    "\n",
    "• Before start training model, let's configure hyperparameters! \n",
    "\n",
    "• Since accessible computaion power is limited in Colab, will only run 1 epoch of training, which takes about 1.5 hour. \n",
    "\n",
    "• won't produce as accurate results as shown in original paper in this Colab. \n",
    "\n",
    "• provide a checkpoint of training model on entire WaterDrop dataset for 5 epochs, which takes about 14 hours with a GeForce RTX 3080 Ti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Czpr3hJTiCuC"
   },
   "outputs": [],
   "source": [
    "data_path = OUTPUT_DIR\n",
    "debug_log(data_path, ShowShape=False, ShowLength=False, ShowType=False)\n",
    "\n",
    "\n",
    "model_path = os.path.join(\"temp\", \"models\", DATASET_NAME)\n",
    "debug_log(model_path, ShowShape=False, ShowLength=False, ShowType=False)\n",
    "\n",
    "\n",
    "rollout_path = os.path.join(\"temp\", \"rollouts\", DATASET_NAME)\n",
    "debug_log(rollout_path, ShowShape=False, ShowLength=False, ShowType=False)\n",
    "\n",
    "\n",
    "!mkdir -p \"$model_path\"\n",
    "!mkdir -p \"$rollout_path\"\n",
    "\n",
    "params = {\n",
    "    \"epoch\": 1,\n",
    "    #\"epoch\": 20,\n",
    "    \"batch_size\": 4,\n",
    "    \"lr\": 1e-4,\n",
    "    \"noise\": 3e-4,\n",
    "    \"save_interval\": 1000,\n",
    "    \"eval_interval\": 1000,\n",
    "    \"rollout_interval\": 200000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gN_P6D4tK7FQ"
   },
   "source": [
    "Below are some helper functions for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSIAlxN7KvuZ"
   },
   "outputs": [],
   "source": [
    "def rollout(model, data, metadata, noise_std):\n",
    "    device = next(model.parameters()).device\n",
    "    debug_log(device, \"rollout\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"device-rollout\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    window_size = model.window_size + 1\n",
    "    debug_log(window_size, \"rollout\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"window_size-rollout\")\n",
    "   \n",
    "    total_time = data[\"position\"].size(0)\n",
    "    debug_log(total_time, \"rollout\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"total_time-rollout\")\n",
    "\n",
    "    \n",
    "    traj = data[\"position\"][:window_size]\n",
    "    debug_log(traj, \"rollout\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"traj1-rollout\")\n",
    "    \n",
    "    \n",
    "    traj = traj.permute(1, 0, 2)\n",
    "    debug_log(traj, \"rollout\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"traj2-rollout\")\n",
    "    \n",
    "    \n",
    "    particle_type = data[\"particle_type\"]\n",
    "    debug_log(particle_type, \"rollout\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"particle_type-rollout\")\n",
    "    \n",
    "\n",
    "    for time in range(total_time - window_size):\n",
    "        with torch.no_grad():\n",
    "            graph = preprocess(particle_type, traj[:, -window_size:], None, metadata, 0.0)\n",
    "            debug_log(graph, \"rollout\\time\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"graph1-rollout\\time\")\n",
    "\n",
    "            \n",
    "            \n",
    "            graph = graph.to(device)\n",
    "            debug_log(graph, \"rollout\\time\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"graph2-rollout\\time\")            \n",
    "\n",
    "            \n",
    "            \n",
    "            acceleration = model(graph).cpu()\n",
    "            debug_log(acceleration, \"rollout\\time\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"acceleration1-rollout\\time\")\n",
    "\n",
    "            \n",
    "            \n",
    "            acceleration = acceleration * torch.sqrt(torch.tensor(metadata[\"acc_std\"]) ** 2 + noise_std ** 2) + torch.tensor(metadata[\"acc_mean\"])\n",
    "            debug_log(acceleration, \"rollout\\time\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"acceleration2-rollout\\time\")\n",
    "\n",
    "                        \n",
    "\n",
    "            recent_position = traj[:, -1]\n",
    "            debug_log(recent_position, \"rollout\\time\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"recent_position-rollout\\time\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            recent_velocity = recent_position - traj[:, -2]\n",
    "            debug_log(recent_velocity, \"rollout\\time\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"recent_velocity-rollout\\time\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            new_velocity = recent_velocity + acceleration\n",
    "            debug_log(new_velocity, \"rollout\\time\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"new_velocity1-rollout\\time\")\n",
    "\n",
    "            \n",
    "            \n",
    "            new_position = recent_position + new_velocity\n",
    "            debug_log(new_velocity, \"rollout\\time\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"new_velocity2-rollout\\time\")            \n",
    "\n",
    "            \n",
    "            \n",
    "            traj = torch.cat((traj, new_position.unsqueeze(1)), dim=1)\n",
    "            debug_log(traj, \"rollout\\time\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"traj-rollout\\time\")            \n",
    "\n",
    "            \n",
    "\n",
    "    return traj\n",
    "\n",
    "\n",
    "def oneStepMSE(simulator, dataloader, metadata, noise):\n",
    "    \"\"\"Returns two values, loss and MSE\"\"\"\n",
    "    total_loss = 0.0\n",
    "    total_mse = 0.0\n",
    "    batch_count = 0\n",
    "    simulator.eval()\n",
    "    with torch.no_grad():\n",
    "        scale = torch.sqrt(torch.tensor(metadata[\"acc_std\"]) ** 2 + noise ** 2).cuda()\n",
    "        for data in valid_loader:\n",
    "            data = data.cuda()\n",
    "            debug_log(data, \"oneStepMSE\\data\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"data-oneStepMSE\\data\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            pred = simulator(data)\n",
    "            debug_log(pred, \"oneStepMSE\\data\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"pred-oneStepMSE\\data\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            mse = ((pred - data.y) * scale) ** 2\n",
    "            debug_log(mse, \"oneStepMSE\\data\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"mse1-oneStepMSE\\data\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            mse = mse.sum(dim=-1).mean()\n",
    "            debug_log(mse, \"oneStepMSE\\data\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"mse2-oneStepMSE\\data\")\n",
    "\n",
    "            \n",
    "            \n",
    "            loss = ((pred - data.y) ** 2).mean()\n",
    "            debug_log(loss, \"oneStepMSE\\data\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"loss-oneStepMSE\\data\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            total_mse += mse.item()\n",
    "            debug_log(total_mse, \"oneStepMSE\\data\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"total_mse-oneStepMSE\\data\")\n",
    "\n",
    "            \n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            debug_log(total_loss, \"oneStepMSE\\data\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"total_loss-oneStepMSE\\data\")\n",
    "            \n",
    "           \n",
    "            \n",
    "            batch_count += 1\n",
    "            debug_log(batch_count, \"oneStepMSE\\data\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"batch_count-oneStepMSE\\data\")\n",
    "            \n",
    "            \n",
    "            \n",
    "    return total_loss / batch_count, total_mse / batch_count\n",
    "\n",
    "\n",
    "def rolloutMSE(simulator, dataset, noise):\n",
    "    total_loss = 0.0\n",
    "    batch_count = 0\n",
    "    simulator.eval()\n",
    "    with torch.no_grad():\n",
    "        for rollout_data in dataset:\n",
    "            debug_log(rollout_data, \"rolloutMSE\\rollout_data\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"rollout_data-rolloutMSE\\rollout_data\")\n",
    "\n",
    "            \n",
    "            rollout_out = rollout(simulator, rollout_data, dataset.metadata, noise)\n",
    "            debug_log(rollout_out, \"rolloutMSE\\rollout_data\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"rollout_out-rolloutMSE1\\rollout_data\")\n",
    "\n",
    "            \n",
    "            \n",
    "            rollout_out = rollout_out.permute(1, 0, 2)\n",
    "            debug_log(rollout_out, \"rolloutMSE\\rollout_data\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"rollout_out-rolloutMSE2\\rollout_data\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            loss = (rollout_out - rollout_data[\"position\"]) ** 2\n",
    "            debug_log(loss, \"rolloutMSE\\rollout_data\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"loss-rolloutMSE1\\rollout_data\")\n",
    "            \n",
    "            \n",
    "            loss = loss.sum(dim=-1).mean()\n",
    "            debug_log(loss, \"rolloutMSE\\rollout_data\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"loss-rolloutMSE2\\rollout_data\")\n",
    "\n",
    "            \n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            debug_log(total_loss, \"rolloutMSE\\rollout_data\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"total_loss-rolloutMSE\\rollout_data\")\n",
    "\n",
    "            \n",
    "            \n",
    "            batch_count += 1\n",
    "            debug_log(batch_count, \"rolloutMSE\\rollout_data\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"batch_count-rolloutMSE\\rollout_data\")\n",
    "\n",
    "            \n",
    "            \n",
    "    return total_loss / batch_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwvseROOFqt0"
   },
   "source": [
    "Here is main training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRsKEIX6XAwN"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(params, simulator, train_loader, valid_loader, valid_rollout_dataset):\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    debug_log(loss_fn, \"train\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"loss_fn-train\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.Adam(simulator.parameters(), lr=params[\"lr\"])\n",
    "    debug_log(optimizer, \"train\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"optimizer-train\")\n",
    "    \n",
    "       \n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1 ** (1 / 5e6))\n",
    "    debug_log(scheduler, \"train\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"scheduler-train\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # recording loss curve\n",
    "    train_loss_list = []\n",
    "    eval_loss_list = []\n",
    "    onestep_mse_list = []\n",
    "    rollout_mse_list = []\n",
    "    total_step = 0\n",
    "\n",
    "    for i in range(params[\"epoch\"]):\n",
    "        simulator.train()\n",
    "        \n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {i}\")\n",
    "        debug_log(progress_bar, \"train\\i\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"progress_bar-train\\i\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        total_loss = 0\n",
    "        debug_log(total_loss, \"train\\i\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"total_loss-train\\i\")\n",
    "\n",
    "        \n",
    "        \n",
    "        batch_count = 0\n",
    "        debug_log(batch_count, \"train\\i\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"batch_count-train\\i\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for data in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            debug_log(optimizer, \"train\\i\\data\", ShowShape=False, ShowLength=False, ShowType=True, ExplicitVariableName = \"optimizer-train\\i\\data\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            data = data.cuda()\n",
    "            debug_log(data, \"train\\i\\data\", ShowShape=False, ShowLength=True, ShowType=True, ExplicitVariableName = \"data-train\\i\\data\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            pred = simulator(data)\n",
    "            debug_log(pred, \"train\\i\\data\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"pred-train\\i\\data\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            loss = loss_fn(pred, data.y)\n",
    "            debug_log(loss, \"train\\i\\data\", ShowShape=True, ShowLength=False, ShowType=True, ExplicitVariableName = \"loss-train\\i\\data\")\n",
    "\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            debug_log(total_loss, \"train\\i\\data\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"total_loss-train\\i\\data\")\n",
    "            \n",
    "\n",
    "\n",
    "            batch_count += 1\n",
    "            debug_log(batch_count, \"train\\i\\data\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"batch_count-train\\i\\data\")\n",
    "            \n",
    "\n",
    "\n",
    "            progress_bar.set_postfix({\"loss\": loss.item(), \"avg_loss\": total_loss / batch_count, \"lr\": optimizer.param_groups[0][\"lr\"]})\n",
    "\n",
    "\n",
    "            total_step += 1\n",
    "            debug_log(total_step, \"train\\i\\data\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"total_step-train\\i\\data\")\n",
    "            \n",
    "\n",
    "\n",
    "            train_loss_list.append((total_step, loss.item()))\n",
    "            debug_log(train_loss_list, \"train\\i\\data\", ShowShape=False, ShowLength=True, ShowType=True, ExplicitVariableName = \"train_loss_list-train\\i\\data\")\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            # evaluation\n",
    "            if total_step % params[\"eval_interval\"] == 0:\n",
    "                simulator.eval()\n",
    "                eval_loss, onestep_mse = oneStepMSE(simulator, valid_loader, valid_dataset.metadata, params[\"noise\"])\n",
    "                debug_log(eval_loss, \"train\\i\\data\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"eval_loss-train\\i\\data\")\n",
    "\n",
    "\n",
    "                eval_loss_list.append((total_step, eval_loss))\n",
    "                debug_log(eval_loss_list, \"train\\i\\data\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"eval_loss_list-train\\i\\data\")\n",
    "                \n",
    "\n",
    "\n",
    "                onestep_mse_list.append((total_step, onestep_mse))\n",
    "                debug_log(onestep_mse_list, \"train\\i\\data\", ShowShape=True, ShowLength=True, ShowType=True, ExplicitVariableName = \"onestep_mse_list-train\\i\\data\")\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "                tqdm.write(f\"\\nEval: Loss: {eval_loss}, One Step MSE: {onestep_mse}\")\n",
    "                simulator.train()\n",
    "\n",
    "            # do rollout on valid set\n",
    "            if total_step % params[\"rollout_interval\"] == 0:\n",
    "                simulator.eval()\n",
    "                rollout_mse = rolloutMSE(simulator, valid_rollout_dataset, params[\"noise\"])\n",
    "                debug_log(rollout_mse, \"train\\i\\data\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"rollout_mse-train\\i\\data\")\n",
    "\n",
    "\n",
    "                rollout_mse_list.append((total_step, rollout_mse))\n",
    "                debug_log(rollout_mse_list, \"train\\i\\data\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"rollout_mse_list-train\\i\\data\")\n",
    "                \n",
    "\n",
    "\n",
    "                tqdm.write(f\"\\nEval: Rollout MSE: {rollout_mse}\")\n",
    "                simulator.train()\n",
    "\n",
    "            # save model\n",
    "            if total_step % params[\"save_interval\"] == 0:\n",
    "                debug_log(total_step, \"train\\i\\data\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"total_step-train\\i\\data\")\n",
    "                debug_log(params[save_interval], \"train\\i\\data\", ShowShape=False, ShowLength=False, ShowType=False, ExplicitVariableName = \"params[save_interval]-train\\i\\data\")\n",
    "\n",
    "                \n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"model\": simulator.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"scheduler\": scheduler.state_dict(),\n",
    "                    },\n",
    "                    os.path.join(model_path, f\"checkpoint_{total_step}.pt\")\n",
    "                )\n",
    "    return train_loss_list, eval_loss_list, onestep_mse_list, rollout_mse_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZ-U-nlEakHF"
   },
   "source": [
    "• let's load dataset and train model! \n",
    "\n",
    "• It takes roughly 1.5 hour to run this block on Colab with default parameters. \n",
    "\n",
    "• **If you are impatient, highly recommend you to skip next 2 blocks and load checkpoint provided to save some time;**\n",
    "\n",
    "• **otherwise, make a cup of tea/coffee and come back later to see results of training!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1HWNWqbE6db",
    "outputId": "fa143d3b-f28c-4484-d9cb-70c2f9b8ae63"
   },
   "outputs": [],
   "source": [
    "# Training model is time-consuming. We highly recommend you to skip this block and load checkpoint in next block.\n",
    "\n",
    "# load dataset\n",
    "train_dataset = OneStepDataset(data_path, \"train\", noise_std=params[\"noise\"])\n",
    "debug_log(train_dataset, ShowShape=False, ShowLength=True, ShowType=True)\n",
    "\n",
    "valid_dataset = OneStepDataset(data_path, \"valid\", noise_std=params[\"noise\"])\n",
    "debug_log(valid_dataset, ShowShape=False, ShowLength=True, ShowType=True)\n",
    "\n",
    "train_loader = pyg.loader.DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True, pin_memory=True, num_workers=2)\n",
    "debug_log(train_loader, ShowShape=False, ShowLength=True, ShowType=True)\n",
    "\n",
    "valid_loader = pyg.loader.DataLoader(valid_dataset, batch_size=params[\"batch_size\"], shuffle=False, pin_memory=True, num_workers=2)\n",
    "debug_log(valid_loader, ShowShape=False, ShowLength=True, ShowType=True)\n",
    "\n",
    "valid_rollout_dataset = RolloutDataset(data_path, \"valid\")\n",
    "debug_log(valid_rollout_dataset, ShowShape=False, ShowLength=True, ShowType=True)\n",
    "\n",
    "# build model\n",
    "simulator = LearnedSimulator()\n",
    "\n",
    "simulator = simulator.cuda()\n",
    "\n",
    "# train model\n",
    "train_loss_list, eval_loss_list, onestep_mse_list, rollout_mse_list = train(params, simulator, train_loader, valid_loader, valid_rollout_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "MBw_qGjz62_S",
    "outputId": "f69959c9-a199-4e82-ea12-694112546893"
   },
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "model_save_path = \"simulator_model_justoneepoch.pth\"\n",
    "\n",
    "\n",
    "torch.save(simulator.state_dict(), model_save_path)\n",
    "\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "MBw_qGjz62_S",
    "outputId": "f69959c9-a199-4e82-ea12-694112546893"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize loss curve\n",
    "plt.figure()\n",
    "plt.plot(*zip(*train_loss_list), label=\"train\")\n",
    "plt.plot(*zip(*eval_loss_list), label=\"valid\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Y2Pf_TcbR0K"
   },
   "source": [
    "• Load checkpoint trained by us. \n",
    "\n",
    "• Do **not** run this block if you have trained your model in previous block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cw26wx-9B_Q3"
   },
   "outputs": [],
   "source": [
    "################\n",
    "## LOAD MODEL ##\n",
    "################\n",
    "simulator = LearnedSimulator()\n",
    "\n",
    "\n",
    "simulator = simulator.cuda()\n",
    "\n",
    "\n",
    "#!wget -O temp/models/WaterDrop_checkpoint.pt https://storage.googleapis.com/cs224w_course_project_dataset/Checkpoints/WaterDrop_checkpoint.pt\n",
    "# checkpoint = torch.load(\"simulator_model_20epoch.pth\")\n",
    "# simulator.load_state_dict(checkpoint[\"model\"])\n",
    "# model_save_path = \"simulator_model_20epoch.pth\"\n",
    "model_save_path = \"simulator_model_justoneepoch.pth\"\n",
    "\n",
    "simulator.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eDD-SySxKFt"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "Since video is 1000 frames long, it might take a few minutes to rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNhrML1SrWIe"
   },
   "outputs": [],
   "source": [
    "rollout_dataset = RolloutDataset(data_path, \"valid\")\n",
    "\n",
    "\n",
    "simulator.eval()\n",
    "\n",
    "\n",
    "rollout_data = rollout_dataset[0]\n",
    "\n",
    "\n",
    "rollout_out = rollout(simulator, rollout_data, rollout_dataset.metadata, params[\"noise\"])\n",
    "\n",
    "\n",
    "rollout_out = rollout_out.permute(1, 0, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "H0W5Tm4rws0_",
    "outputId": "a9a21fec-2d17-4ab5-f37e-b7bf3fc05114"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "TYPE_TO_COLOR = {\n",
    "    3: \"black\",\n",
    "    0: \"green\",\n",
    "    7: \"magenta\",\n",
    "    6: \"gold\",\n",
    "    5: \"blue\",\n",
    "}\n",
    "\n",
    "\n",
    "def visualize_prepare(ax, particle_type, position, metadata):\n",
    "    bounds = metadata[\"bounds\"]\n",
    "    ax.set_xlim(bounds[0][0], bounds[0][1])\n",
    "    ax.set_ylim(bounds[1][0], bounds[1][1])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect(1.0)\n",
    "    points = {type_: ax.plot([], [], \"o\", ms=2, color=color)[0] for type_, color in TYPE_TO_COLOR.items()}\n",
    "    return ax, position, points\n",
    "\n",
    "\n",
    "def visualize_pair(particle_type, position_pred, position_gt, metadata):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    plot_info = [\n",
    "        visualize_prepare(axes[0], particle_type, position_gt, metadata),\n",
    "        visualize_prepare(axes[1], particle_type, position_pred, metadata),\n",
    "    ]\n",
    "    axes[0].set_title(\"Ground truth\")\n",
    "    axes[1].set_title(\"Prediction\")\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    def update(step_i):\n",
    "        outputs = []\n",
    "\n",
    "\n",
    "        for _, position, points in plot_info:\n",
    "\n",
    "\n",
    "            for type_, line in points.items():\n",
    "                mask = particle_type == type_\n",
    "\n",
    "\n",
    "                line.set_data(position[step_i, mask, 0], position[step_i, mask, 1])\n",
    "\n",
    "\n",
    "            outputs.append(line)\n",
    "        return outputs\n",
    "\n",
    "    return animation.FuncAnimation(fig, update, frames=np.arange(0, position_gt.size(0)), interval=10, blit=True)\n",
    "\n",
    "anim = visualize_pair(rollout_data[\"particle_type\"], rollout_out, rollout_data[\"position\"], rollout_dataset.metadata)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3t753E3BzEC"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "• Hope this Colab is helpful for you to understand how to apply GNN in a real-world application like simulating complex physics! \n",
    "\n",
    "• If you're interested in technical details, read [medium post](https://) or see [original paper](https://arxiv.org/abs/2002.09405) by DeepMind. \n",
    "\n",
    "• Thanks for spending your time with us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################\n",
    "# ## IMAGE STITCHER ## \n",
    "# ####################\n",
    "# import os\n",
    "# from PIL import Image\n",
    "\n",
    "# def stitch_images_bfdh(image_folder, output_path):\n",
    "#     # Load all PNG images from the folder\n",
    "#     images = []\n",
    "#     for filename in os.listdir(image_folder):\n",
    "#         if filename.endswith('.png'):\n",
    "#             img_path = os.path.join(image_folder, filename)\n",
    "#             img = Image.open(img_path)\n",
    "#             images.append(img)\n",
    "\n",
    "#     # Sort images by width in descending order for better packing\n",
    "#     images.sort(key=lambda img: img.width, reverse=True)\n",
    "\n",
    "#     # Initialize variables for row-based packing\n",
    "#     rows = []\n",
    "#     current_row = []\n",
    "#     current_width = 0\n",
    "#     max_height_in_row = 0\n",
    "#     max_canvas_width = 0\n",
    "\n",
    "#     max_width = max(img.width for img in images)  # Maximum width of any image\n",
    "#     max_total_height = 0\n",
    "\n",
    "#     # Group images into rows based on width (bin-packing approach)\n",
    "#     for img in images:\n",
    "#         if current_width + img.width <= max_width:\n",
    "#             # Add image to the current row\n",
    "#             current_row.append(img)\n",
    "#             current_width += img.width\n",
    "#             max_height_in_row = max(max_height_in_row, img.height)\n",
    "#         else:\n",
    "#             # Move to the next row\n",
    "#             rows.append((current_row, current_width, max_height_in_row))\n",
    "#             max_canvas_width = max(max_canvas_width, current_width)\n",
    "#             max_total_height += max_height_in_row\n",
    "            \n",
    "#             # Reset for the new row\n",
    "#             current_row = [img]\n",
    "#             current_width = img.width\n",
    "#             max_height_in_row = img.height\n",
    "\n",
    "#     # Add the last row\n",
    "#     if current_row:\n",
    "#         rows.append((current_row, current_width, max_height_in_row))\n",
    "#         max_canvas_width = max(max_canvas_width, current_width)\n",
    "#         max_total_height += max_height_in_row\n",
    "\n",
    "#     # Create a new blank canvas large enough to hold all rows\n",
    "#     stitched_image = Image.new('RGBA', (max_canvas_width, max_total_height))\n",
    "\n",
    "#     # Variable to keep track of current y-position (vertical stacking)\n",
    "#     y_offset = 0\n",
    "\n",
    "#     # Stitch images row by row\n",
    "#     for row, row_width, row_height in rows:\n",
    "#         x_offset = 0\n",
    "#         for img in row:\n",
    "#             # Paste each image into its row\n",
    "#             stitched_image.paste(img, (x_offset, y_offset))\n",
    "#             x_offset += img.width  # Move to the right for the next image\n",
    "#         y_offset += row_height  # Move down for the next row\n",
    "\n",
    "#     # Save the stitched image\n",
    "#     stitched_image.save(output_path)\n",
    "#     print(f\"Stitched image saved as {output_path}\")\n",
    "    \n",
    "\n",
    "# # Run stitch_images_bfdh for each item in the folders_created list\n",
    "# for folder in folders_created:\n",
    "#     image_folder = os.path.expanduser(os.path.join('~/Desktop/GNN/outputpng/', folder))\n",
    "#     output_path = os.path.expanduser(os.path.join('~/Desktop/GNN/outputpng/', folder, f\"{folder}.png\"))\n",
    "#     stitch_images_bfdh(image_folder, output_path)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def stitch_images_bfdh(image_folder, output_path, spacing=0):\n",
    "    # Load all PNG images from the folder\n",
    "    images = []\n",
    "    for filename in os.listdir(image_folder):\n",
    "        if filename.endswith('.png'):\n",
    "            img_path = os.path.join(image_folder, filename)\n",
    "            img = Image.open(img_path)\n",
    "            images.append(img)\n",
    "\n",
    "    # Sort images by width in descending order for better packing\n",
    "    images.sort(key=lambda img: img.width, reverse=True)\n",
    "\n",
    "    # Initialize variables for row-based packing\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    current_width = 0\n",
    "    max_height_in_row = 0\n",
    "    max_canvas_width = 0\n",
    "\n",
    "    max_width = max(img.width for img in images)  # Maximum width of any image\n",
    "    max_total_height = 0\n",
    "\n",
    "    # Group images into rows based on width (bin-packing approach)\n",
    "    for img in images:\n",
    "        if current_width + img.width + (len(current_row) * spacing) <= max_width:\n",
    "            # Add image to the current row\n",
    "            current_row.append(img)\n",
    "            current_width += img.width\n",
    "            max_height_in_row = max(max_height_in_row, img.height)\n",
    "        else:\n",
    "            # Move to the next row\n",
    "            rows.append((current_row, current_width, max_height_in_row))\n",
    "            max_canvas_width = max(max_canvas_width, current_width + (len(current_row) - 1) * spacing)\n",
    "            max_total_height += max_height_in_row + spacing\n",
    "\n",
    "            # Reset for the new row\n",
    "            current_row = [img]\n",
    "            current_width = img.width\n",
    "            max_height_in_row = img.height\n",
    "\n",
    "    # Add the last row\n",
    "    if current_row:\n",
    "        rows.append((current_row, current_width, max_height_in_row))\n",
    "        max_canvas_width = max(max_canvas_width, current_width + (len(current_row) - 1) * spacing)\n",
    "        max_total_height += max_height_in_row  # Don't add spacing after the last row\n",
    "\n",
    "    # Create a new blank canvas large enough to hold all rows\n",
    "    stitched_image = Image.new('RGBA', (max_canvas_width, max_total_height))\n",
    "\n",
    "    # Variable to keep track of current y-position (vertical stacking)\n",
    "    y_offset = 0\n",
    "\n",
    "    # Stitch images row by row\n",
    "    for row, row_width, row_height in rows:\n",
    "        x_offset = 0\n",
    "        for img in row:\n",
    "            # Paste each image into its row with spacing\n",
    "            stitched_image.paste(img, (x_offset, y_offset))\n",
    "            x_offset += img.width + spacing  # Move to the right for the next image\n",
    "        y_offset += row_height + spacing  # Move down for the next row with spacing\n",
    "\n",
    "    # Save the stitched image\n",
    "    stitched_image.save(output_path)\n",
    "    print(f\"Stitched image saved as {output_path}\")\n",
    "\n",
    "# Run stitch_images_bfdh for each item in the folders_created list\n",
    "for folder in folders_created:\n",
    "    image_folder = os.path.expanduser(os.path.join('~/Desktop/GNN/outputpng/', folder))\n",
    "    output_path = os.path.expanduser(os.path.join('~/Desktop/GNN/outputpng/', folder, f\"{folder}.png\"))\n",
    "    stitch_images_bfdh(image_folder, output_path, spacing=10)\n",
    "\n",
    "# outputfile = r\"C:\\Users\\Admin\\Desktop\\GNNoutputpng\\OneStepDataset\\outputpng.png\"\n",
    "# # if outputfile exists then delete it\n",
    "# if os.path.exists(outputfile):\n",
    "#     os.remove(outputfile)  # Remove the file\n",
    "#     print(f\"File '{outputfile}' has been removed.\")\n",
    "# else:\n",
    "#     print(f\"File '{outputfile}' does not exist.\")\n",
    "\n",
    "# image_folder = r'C:\\Users\\Admin\\Desktop\\GNNoutputpng\\OneStepDataset'\n",
    "# stitch_images_bfdh(image_folder, outputfile, spacing=10)  # Example with 10px spacing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
